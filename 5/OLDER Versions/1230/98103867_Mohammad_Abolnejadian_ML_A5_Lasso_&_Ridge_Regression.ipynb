{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07eb33c"
      },
      "source": [
        "<div>\n",
        "<img src='http://www-scf.usc.edu/~ghasemig/images/sharif.png' alt=\"SUT logo\" width=220 height=220 align=left class=\"saturate\">\n",
        "\n",
        "<br>\n",
        "<font face=\"Times New Roman\">\n",
        "<div dir=ltr align=center> \n",
        "<!-- <font color=0F5298 size=7> -->\n",
        "<font color=0F5298 size=6>\n",
        "    Introduction to Machine Learning <br> <br>\n",
        "<!-- <font color=2565AE size=5> -->\n",
        "<font size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Spring 2023 <br> <br>\n",
        "<font color=606060 size=5>\n",
        "    Homework 5: Practical - Lasso & Ridge Regression <br> <br>\n",
        "<font color=686880 size=4>\n",
        "    TAs: Alireza Dehghanpour - Arman Malekzadeh - Ali Salesi\n",
        "    \n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5a79508"
      },
      "source": [
        "### Full Name : Mohammad Abolnejadian\n",
        "### Student Number : 98103867\n",
        "### Colab Link:\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sy39_vXU_PO"
      },
      "source": [
        "The Global Health Observatory (GHO) data repository under World Health Organization (WHO) keeps track of the health status as well as many other related factors for all countries. The datasets are made available to public for the purpose of health data analysis. The dataset related to life expectancy, health factors for 193 countries has been collected from the same WHO data repository website and its corresponding economic data was collected from United Nation website. Among all categories of health-related factors only those critical factors were chosen which are more representative.\n",
        "\n",
        "In this assignment you have to predict **life expectancy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-30T08:33:27.082868Z",
          "iopub.status.busy": "2023-04-30T08:33:27.082403Z",
          "iopub.status.idle": "2023-04-30T08:33:27.089987Z",
          "shell.execute_reply": "2023-04-30T08:33:27.088672Z",
          "shell.execute_reply.started": "2023-04-30T08:33:27.082829Z"
        },
        "id": "bBUrdXrVU_PO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sEpKmvvU_PP"
      },
      "source": [
        "# Data Exploration (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJCBJo5XjWUq",
        "outputId": "08ba6431-343a-418b-9b68-666fd4680fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-11 18:25:45--  https://www.dropbox.com/s/2kz21qjt40pjy43/train.csv?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/2kz21qjt40pjy43/train.csv [following]\n",
            "--2023-05-11 18:25:45--  https://www.dropbox.com/s/dl/2kz21qjt40pjy43/train.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucfeb1430fe328d088d8f119a47e.dl.dropboxusercontent.com/cd/0/get/B72mQQZ0sm_2A4IaFzSShgtbV5_PSn_7ucYHwvM1XgjHVySztUaNrdAnLb1nQDqRgaR7qHZX66GfZFFutd_b8ZOECMvlwQQ4nDRtGAbcrQf2mKGJFzJgmCwCrkpcGvX0zTX0QR31vsmgh5hGBjbvRC2ucnjguyNX0StGd8I-AnKBf0fX724Pie2Okr0nYnmnZY0/file?dl=1# [following]\n",
            "--2023-05-11 18:25:46--  https://ucfeb1430fe328d088d8f119a47e.dl.dropboxusercontent.com/cd/0/get/B72mQQZ0sm_2A4IaFzSShgtbV5_PSn_7ucYHwvM1XgjHVySztUaNrdAnLb1nQDqRgaR7qHZX66GfZFFutd_b8ZOECMvlwQQ4nDRtGAbcrQf2mKGJFzJgmCwCrkpcGvX0zTX0QR31vsmgh5hGBjbvRC2ucnjguyNX0StGd8I-AnKBf0fX724Pie2Okr0nYnmnZY0/file?dl=1\n",
            "Resolving ucfeb1430fe328d088d8f119a47e.dl.dropboxusercontent.com (ucfeb1430fe328d088d8f119a47e.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucfeb1430fe328d088d8f119a47e.dl.dropboxusercontent.com (ucfeb1430fe328d088d8f119a47e.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 308420 (301K) [application/binary]\n",
            "Saving to: ‘/content/dataset.csv’\n",
            "\n",
            "/content/dataset.cs 100%[===================>] 301.19K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-11 18:25:46 (3.04 MB/s) - ‘/content/dataset.csv’ saved [308420/308420]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O /content/dataset.csv \"https://www.dropbox.com/s/2kz21qjt40pjy43/train.csv?dl=1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU9i17XgU_PP"
      },
      "source": [
        "Load the dataset as a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:33:30.197642Z",
          "iopub.status.busy": "2023-04-30T08:33:30.197254Z",
          "iopub.status.idle": "2023-04-30T08:33:30.238114Z",
          "shell.execute_reply": "2023-04-30T08:33:30.236875Z",
          "shell.execute_reply.started": "2023-04-30T08:33:30.197608Z"
        },
        "id": "GARdmLGkU_PQ",
        "outputId": "d676b328-716d-4b1b-dd61-e9c0908b09f1",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Country  Year      Status  Life expectancy  \\\n",
              "0                   Finland  2013  Developing             87.0   \n",
              "1                     Japan  2015   Developed             83.7   \n",
              "2      Syrian Arab Republic  2014  Developing             64.4   \n",
              "3                    Latvia  2010   Developed             72.8   \n",
              "4     Sao Tome and Principe  2013  Developing             67.1   \n",
              "...                     ...   ...         ...              ...   \n",
              "2492                Tunisia  2000  Developing             72.9   \n",
              "2493                Myanmar  2001  Developing             62.5   \n",
              "2494            Netherlands  2008   Developed             83.0   \n",
              "2495            South Sudan  2013  Developing             56.4   \n",
              "2496              Guatemala  2000  Developing             67.7   \n",
              "\n",
              "      Adult Mortality  infant deaths  Alcohol  percentage expenditure  \\\n",
              "0                79.0              0     8.97             6115.496624   \n",
              "1                55.0              2      NaN                0.000000   \n",
              "2               294.0              7     0.01                0.000000   \n",
              "3                18.0              0     9.80             1109.969508   \n",
              "4               192.0              0     0.01              200.660099   \n",
              "...               ...            ...      ...                     ...   \n",
              "2492            112.0              4     1.21              264.784220   \n",
              "2493            239.0             72     0.38                1.917164   \n",
              "2494             68.0              1     9.62            10873.405540   \n",
              "2495            345.0             26      NaN               47.444530   \n",
              "2496            221.0             17     2.63              238.736981   \n",
              "\n",
              "      Hepatitis B  Measles   ...  Polio  Total expenditure  Diphtheria   \\\n",
              "0             NaN         2  ...   98.0               9.55         98.0   \n",
              "1             NaN        35  ...   99.0                NaN         96.0   \n",
              "2            47.0       594  ...   52.0               3.25         43.0   \n",
              "3            91.0         0  ...   92.0               6.55         92.0   \n",
              "4            97.0         0  ...   97.0               9.76         97.0   \n",
              "...           ...       ...  ...    ...                ...          ...   \n",
              "2492         94.0        47  ...   97.0               5.40         97.0   \n",
              "2493          NaN      2519  ...   77.0               1.80         73.0   \n",
              "2494          NaN       109  ...   97.0               9.57         97.0   \n",
              "2495          NaN       525  ...    5.0               2.62         45.0   \n",
              "2496          NaN         0  ...    8.0               5.25         81.0   \n",
              "\n",
              "       HIV/AIDS           GDP  Population   thinness  1-19 years  \\\n",
              "0           0.1  49638.771300   5438972.0                    0.9   \n",
              "1           0.1  34474.137360    127141.0                    2.1   \n",
              "2           0.1           NaN     19239.0                    6.3   \n",
              "3           0.1  11326.219470    297555.0                    2.2   \n",
              "4           0.2   1619.532678     18745.0                    5.7   \n",
              "...         ...           ...         ...                    ...   \n",
              "2492        0.1   2213.914880   9699197.0                    6.6   \n",
              "2493        0.4    138.924927  46627994.0                   13.3   \n",
              "2494        0.1  56928.824800  16445593.0                    1.0   \n",
              "2495        3.6   1186.113250   1117749.0                    NaN   \n",
              "2496        0.2   1655.596261   1165743.0                    1.6   \n",
              "\n",
              "       thinness 5-9 years  Income composition of resources  Schooling  \n",
              "0                     0.8                            0.887       17.0  \n",
              "1                     1.8                            0.902       15.3  \n",
              "2                     6.1                            0.575        9.0  \n",
              "3                     2.3                            0.815       16.0  \n",
              "4                     5.5                            0.559       11.0  \n",
              "...                   ...                              ...        ...  \n",
              "2492                  6.5                            0.646       12.8  \n",
              "2493                 13.7                            0.427        7.6  \n",
              "2494                  0.9                            0.905       16.8  \n",
              "2495                  NaN                            0.417        4.9  \n",
              "2496                  1.6                            0.539        8.2  \n",
              "\n",
              "[2497 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a0a9c991-17f5-47c5-9f8b-a553062e9964\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>Year</th>\n",
              "      <th>Status</th>\n",
              "      <th>Life expectancy</th>\n",
              "      <th>Adult Mortality</th>\n",
              "      <th>infant deaths</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>percentage expenditure</th>\n",
              "      <th>Hepatitis B</th>\n",
              "      <th>Measles</th>\n",
              "      <th>...</th>\n",
              "      <th>Polio</th>\n",
              "      <th>Total expenditure</th>\n",
              "      <th>Diphtheria</th>\n",
              "      <th>HIV/AIDS</th>\n",
              "      <th>GDP</th>\n",
              "      <th>Population</th>\n",
              "      <th>thinness  1-19 years</th>\n",
              "      <th>thinness 5-9 years</th>\n",
              "      <th>Income composition of resources</th>\n",
              "      <th>Schooling</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Finland</td>\n",
              "      <td>2013</td>\n",
              "      <td>Developing</td>\n",
              "      <td>87.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.97</td>\n",
              "      <td>6115.496624</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>98.0</td>\n",
              "      <td>9.55</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>49638.771300</td>\n",
              "      <td>5438972.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.887</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Japan</td>\n",
              "      <td>2015</td>\n",
              "      <td>Developed</td>\n",
              "      <td>83.7</td>\n",
              "      <td>55.0</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35</td>\n",
              "      <td>...</td>\n",
              "      <td>99.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>34474.137360</td>\n",
              "      <td>127141.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.902</td>\n",
              "      <td>15.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Syrian Arab Republic</td>\n",
              "      <td>2014</td>\n",
              "      <td>Developing</td>\n",
              "      <td>64.4</td>\n",
              "      <td>294.0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>47.0</td>\n",
              "      <td>594</td>\n",
              "      <td>...</td>\n",
              "      <td>52.0</td>\n",
              "      <td>3.25</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19239.0</td>\n",
              "      <td>6.3</td>\n",
              "      <td>6.1</td>\n",
              "      <td>0.575</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Latvia</td>\n",
              "      <td>2010</td>\n",
              "      <td>Developed</td>\n",
              "      <td>72.8</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.80</td>\n",
              "      <td>1109.969508</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>92.0</td>\n",
              "      <td>6.55</td>\n",
              "      <td>92.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>11326.219470</td>\n",
              "      <td>297555.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.815</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sao Tome and Principe</td>\n",
              "      <td>2013</td>\n",
              "      <td>Developing</td>\n",
              "      <td>67.1</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>200.660099</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>97.0</td>\n",
              "      <td>9.76</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1619.532678</td>\n",
              "      <td>18745.0</td>\n",
              "      <td>5.7</td>\n",
              "      <td>5.5</td>\n",
              "      <td>0.559</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2492</th>\n",
              "      <td>Tunisia</td>\n",
              "      <td>2000</td>\n",
              "      <td>Developing</td>\n",
              "      <td>72.9</td>\n",
              "      <td>112.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.21</td>\n",
              "      <td>264.784220</td>\n",
              "      <td>94.0</td>\n",
              "      <td>47</td>\n",
              "      <td>...</td>\n",
              "      <td>97.0</td>\n",
              "      <td>5.40</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2213.914880</td>\n",
              "      <td>9699197.0</td>\n",
              "      <td>6.6</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0.646</td>\n",
              "      <td>12.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2493</th>\n",
              "      <td>Myanmar</td>\n",
              "      <td>2001</td>\n",
              "      <td>Developing</td>\n",
              "      <td>62.5</td>\n",
              "      <td>239.0</td>\n",
              "      <td>72</td>\n",
              "      <td>0.38</td>\n",
              "      <td>1.917164</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2519</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>138.924927</td>\n",
              "      <td>46627994.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>13.7</td>\n",
              "      <td>0.427</td>\n",
              "      <td>7.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2494</th>\n",
              "      <td>Netherlands</td>\n",
              "      <td>2008</td>\n",
              "      <td>Developed</td>\n",
              "      <td>83.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1</td>\n",
              "      <td>9.62</td>\n",
              "      <td>10873.405540</td>\n",
              "      <td>NaN</td>\n",
              "      <td>109</td>\n",
              "      <td>...</td>\n",
              "      <td>97.0</td>\n",
              "      <td>9.57</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>56928.824800</td>\n",
              "      <td>16445593.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.905</td>\n",
              "      <td>16.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2495</th>\n",
              "      <td>South Sudan</td>\n",
              "      <td>2013</td>\n",
              "      <td>Developing</td>\n",
              "      <td>56.4</td>\n",
              "      <td>345.0</td>\n",
              "      <td>26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>47.444530</td>\n",
              "      <td>NaN</td>\n",
              "      <td>525</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.62</td>\n",
              "      <td>45.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1186.113250</td>\n",
              "      <td>1117749.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.417</td>\n",
              "      <td>4.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>Guatemala</td>\n",
              "      <td>2000</td>\n",
              "      <td>Developing</td>\n",
              "      <td>67.7</td>\n",
              "      <td>221.0</td>\n",
              "      <td>17</td>\n",
              "      <td>2.63</td>\n",
              "      <td>238.736981</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.25</td>\n",
              "      <td>81.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1655.596261</td>\n",
              "      <td>1165743.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.539</td>\n",
              "      <td>8.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2497 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0a9c991-17f5-47c5-9f8b-a553062e9964')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a0a9c991-17f5-47c5-9f8b-a553062e9964 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a0a9c991-17f5-47c5-9f8b-a553062e9964');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df = pd.read_csv(\"dataset.csv\")\n",
        "df = df.rename(columns={\"Life expectancy \": \"Life expectancy\"})\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBsBXJ5dU_PR"
      },
      "source": [
        "Plot \"year\" against \"average life expectancy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClHK3z7vGUWs"
      },
      "source": [
        "**It should have been plot \"average life expectency\" against \"year\".** [Read More](https://english.stackexchange.com/questions/107800/is-x-plotted-against-y-or-is-y-plotted-against-x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:33:33.478151Z",
          "iopub.status.busy": "2023-04-30T08:33:33.477341Z",
          "iopub.status.idle": "2023-04-30T08:33:33.711698Z",
          "shell.execute_reply": "2023-04-30T08:33:33.710510Z",
          "shell.execute_reply.started": "2023-04-30T08:33:33.478109Z"
        },
        "id": "-DfKHpd8U_PR",
        "outputId": "98dbd4eb-9503-4c6f-fd90-402365df3d98",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: title={'center': 'Life expectancy for all countries through years'}, xlabel='Year'>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "df.plot(x=\"Year\", y=\"Life expectancy\", style='x', title='Life expectancy for all countries through years')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "BFTOiLo6Irlj",
        "outputId": "22042e8d-5088-4031-b4b1-90ec545efee1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: title={'center': 'Life expectancy through years'}, xlabel='Year'>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "df.groupby(\"Year\", as_index=False)[\"Life expectancy\"].mean().plot(x=\"Year\", y=\"Life expectancy\", title='Life expectancy through years')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_euMb0n0U_PR"
      },
      "source": [
        "Compare the average life expectancy of \"Developed\" and \"Developing\" countries for each year "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:33:37.167866Z",
          "iopub.status.busy": "2023-04-30T08:33:37.167421Z",
          "iopub.status.idle": "2023-04-30T08:33:37.181164Z",
          "shell.execute_reply": "2023-04-30T08:33:37.179864Z",
          "shell.execute_reply.started": "2023-04-30T08:33:37.167824Z"
        },
        "id": "FHvcfXklU_PS",
        "outputId": "4c72b57b-1ebc-4316-fbce-2175aa4c0d9e",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Year      Status  Life expectancy\n",
              "0   2000   Developed        76.891667\n",
              "1   2000  Developing        65.004959\n",
              "2   2001   Developed        77.114286\n",
              "3   2001  Developing        65.048120\n",
              "4   2002   Developed        77.710000\n",
              "5   2002  Developing        64.393496\n",
              "6   2003   Developed        78.257692\n",
              "7   2003  Developing        65.617213\n",
              "8   2004   Developed        78.452000\n",
              "9   2004  Developing        65.500000\n",
              "10  2005   Developed        78.792593\n",
              "11  2005  Developing        65.898450\n",
              "12  2006   Developed        79.346667\n",
              "13  2006  Developing        67.167939\n",
              "14  2007   Developed        79.252381\n",
              "15  2007  Developing        66.935878\n",
              "16  2008   Developed        78.796552\n",
              "17  2008  Developing        67.365185\n",
              "18  2009   Developed        79.581481\n",
              "19  2009  Developing        67.777206\n",
              "20  2010   Developed        80.167857\n",
              "21  2010  Developing        67.404762\n",
              "22  2011   Developed        79.821739\n",
              "23  2011  Developing        68.182443\n",
              "24  2012   Developed        80.512000\n",
              "25  2012  Developing        68.823485\n",
              "26  2013   Developed        80.376923\n",
              "27  2013  Developing        69.049606\n",
              "28  2014   Developed        81.292593\n",
              "29  2014  Developing        69.442063\n",
              "30  2015   Developed        80.175000\n",
              "31  2015  Developing        69.937984"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4680c137-5fad-4545-a3e6-7531d1928af5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Status</th>\n",
              "      <th>Life expectancy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000</td>\n",
              "      <td>Developed</td>\n",
              "      <td>76.891667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000</td>\n",
              "      <td>Developing</td>\n",
              "      <td>65.004959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>Developed</td>\n",
              "      <td>77.114286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>Developing</td>\n",
              "      <td>65.048120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2002</td>\n",
              "      <td>Developed</td>\n",
              "      <td>77.710000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2002</td>\n",
              "      <td>Developing</td>\n",
              "      <td>64.393496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2003</td>\n",
              "      <td>Developed</td>\n",
              "      <td>78.257692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2003</td>\n",
              "      <td>Developing</td>\n",
              "      <td>65.617213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2004</td>\n",
              "      <td>Developed</td>\n",
              "      <td>78.452000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2004</td>\n",
              "      <td>Developing</td>\n",
              "      <td>65.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2005</td>\n",
              "      <td>Developed</td>\n",
              "      <td>78.792593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2005</td>\n",
              "      <td>Developing</td>\n",
              "      <td>65.898450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2006</td>\n",
              "      <td>Developed</td>\n",
              "      <td>79.346667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2006</td>\n",
              "      <td>Developing</td>\n",
              "      <td>67.167939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2007</td>\n",
              "      <td>Developed</td>\n",
              "      <td>79.252381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2007</td>\n",
              "      <td>Developing</td>\n",
              "      <td>66.935878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2008</td>\n",
              "      <td>Developed</td>\n",
              "      <td>78.796552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2008</td>\n",
              "      <td>Developing</td>\n",
              "      <td>67.365185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2009</td>\n",
              "      <td>Developed</td>\n",
              "      <td>79.581481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2009</td>\n",
              "      <td>Developing</td>\n",
              "      <td>67.777206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2010</td>\n",
              "      <td>Developed</td>\n",
              "      <td>80.167857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2010</td>\n",
              "      <td>Developing</td>\n",
              "      <td>67.404762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2011</td>\n",
              "      <td>Developed</td>\n",
              "      <td>79.821739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2011</td>\n",
              "      <td>Developing</td>\n",
              "      <td>68.182443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2012</td>\n",
              "      <td>Developed</td>\n",
              "      <td>80.512000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2012</td>\n",
              "      <td>Developing</td>\n",
              "      <td>68.823485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2013</td>\n",
              "      <td>Developed</td>\n",
              "      <td>80.376923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2013</td>\n",
              "      <td>Developing</td>\n",
              "      <td>69.049606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2014</td>\n",
              "      <td>Developed</td>\n",
              "      <td>81.292593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2014</td>\n",
              "      <td>Developing</td>\n",
              "      <td>69.442063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>2015</td>\n",
              "      <td>Developed</td>\n",
              "      <td>80.175000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2015</td>\n",
              "      <td>Developing</td>\n",
              "      <td>69.937984</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4680c137-5fad-4545-a3e6-7531d1928af5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4680c137-5fad-4545-a3e6-7531d1928af5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4680c137-5fad-4545-a3e6-7531d1928af5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "average_life_expectancy_based_on_status = df.groupby([\"Year\", \"Status\"], as_index=False)[\"Life expectancy\"].mean()\n",
        "average_life_expectancy_based_on_status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "64Wlt2RSPYjw",
        "outputId": "c7a25ccb-ccd3-470e-dff8-cfb5b1c15f6d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "developing_df = average_life_expectancy_based_on_status.loc[average_life_expectancy_based_on_status['Status'] == \"Developing\"]\n",
        "developed_df = average_life_expectancy_based_on_status.loc[average_life_expectancy_based_on_status['Status'] == \"Developed\"]\n",
        "plt.scatter(x = developing_df.loc[:, \"Year\"], y = developing_df.loc[:, \"Life expectancy\"])\n",
        "plt.scatter(x = developed_df.loc[:, \"Year\"], y = developed_df.loc[:, \"Life expectancy\"])\n",
        "status = ['Developing', 'Developed']\n",
        "plt.legend(labels=status)\n",
        "plt.show()\n",
        "\n",
        "# average_life_expectancy_based_on_status.plot(x=\"Year\", y=\"Life expectancy\", style = 'x',title='Life expectancy through years')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3PokphQU_PS"
      },
      "source": [
        "Get the correlation between \"Life expectancy\" and \"GDP\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:33:39.947838Z",
          "iopub.status.busy": "2023-04-30T08:33:39.947445Z",
          "iopub.status.idle": "2023-04-30T08:33:39.956485Z",
          "shell.execute_reply": "2023-04-30T08:33:39.955285Z",
          "shell.execute_reply.started": "2023-04-30T08:33:39.947799Z"
        },
        "id": "mOfNroPqU_PS",
        "outputId": "9ef8756e-e541-4f5c-91cb-d9c3a65fe310",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4652553879500644"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "correlation_lifeexpectancy_gdp = df['Life expectancy'].corr(df['GDP'])\n",
        "correlation_lifeexpectancy_gdp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL-6KStPU_PS"
      },
      "source": [
        "Find out the feature having the strongest correlation with the \"Life expectancy\"<br>\n",
        "Note: Do not display all of the correlation values. Your code should only print the result (the name of a column)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:33:44.958938Z",
          "iopub.status.busy": "2023-04-30T08:33:44.957730Z",
          "iopub.status.idle": "2023-04-30T08:33:44.977069Z",
          "shell.execute_reply": "2023-04-30T08:33:44.975781Z",
          "shell.execute_reply.started": "2023-04-30T08:33:44.958881Z"
        },
        "id": "sHqKM8nKU_PS",
        "outputId": "6bc50832-584c-4173-8108-8832f5936e29",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schooling: 0.7503972851868812\n"
          ]
        }
      ],
      "source": [
        "correlation = df.drop(columns = [\"Country\", \"Status\"]).corr()['Life expectancy']\n",
        "correlation = correlation.drop('Life expectancy').abs()\n",
        "max_correlation = correlation.max()\n",
        "max_correlation_column = correlation.idxmax()\n",
        "print(str(max_correlation_column) + \": \" + str(max_correlation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbgj4XbWU_PS"
      },
      "source": [
        "# Preprocessing (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeJjivW3U_PS"
      },
      "source": [
        "Remove the rows containing NaN (or null) values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:33:49.740394Z",
          "iopub.status.busy": "2023-04-30T08:33:49.739974Z",
          "iopub.status.idle": "2023-04-30T08:33:49.749989Z",
          "shell.execute_reply": "2023-04-30T08:33:49.748700Z",
          "shell.execute_reply.started": "2023-04-30T08:33:49.740354Z"
        },
        "id": "ZbBXGM9eU_PT",
        "outputId": "9dc03f26-eac7-40ec-9ada-c9ba6e2fe61c",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Country  Year      Status  Life expectancy  \\\n",
              "3                    Latvia  2010   Developed             72.8   \n",
              "4     Sao Tome and Principe  2013  Developing             67.1   \n",
              "7                  Colombia  2005  Developing             73.1   \n",
              "11                 Botswana  2012  Developing             63.4   \n",
              "12                   Mexico  2014  Developing             76.6   \n",
              "...                     ...   ...         ...              ...   \n",
              "2481              Mauritius  2009  Developing             72.8   \n",
              "2485                   Mali  2009  Developing             56.0   \n",
              "2488           Turkmenistan  2008  Developing             64.5   \n",
              "2490                 Angola  2007  Developing             48.2   \n",
              "2492                Tunisia  2000  Developing             72.9   \n",
              "\n",
              "      Adult Mortality  infant deaths  Alcohol  percentage expenditure  \\\n",
              "3                18.0              0     9.80             1109.969508   \n",
              "4               192.0              0     0.01              200.660099   \n",
              "7               144.0             15     4.38              531.980818   \n",
              "11                3.0              2     0.01               12.834474   \n",
              "12              122.0             31     5.26              168.173753   \n",
              "...               ...            ...      ...                     ...   \n",
              "2481            166.0              0     2.83              624.236183   \n",
              "2485            276.0             55     0.59               84.634389   \n",
              "2488            235.0              6     2.40               34.239794   \n",
              "2490            375.0             87     6.35              184.821345   \n",
              "2492            112.0              4     1.21              264.784220   \n",
              "\n",
              "      Hepatitis B  Measles   ...  Polio  Total expenditure  Diphtheria   \\\n",
              "3            91.0         0  ...   92.0               6.55         92.0   \n",
              "4            97.0         0  ...   97.0               9.76         97.0   \n",
              "7            93.0         0  ...   93.0               5.82         93.0   \n",
              "11           95.0         7  ...   96.0               6.27         95.0   \n",
              "12           84.0         3  ...   87.0               6.30         87.0   \n",
              "...           ...       ...  ...    ...                ...          ...   \n",
              "2481         99.0        15  ...   99.0               4.97         99.0   \n",
              "2485         71.0      2939  ...   77.0               6.85         73.0   \n",
              "2488         96.0         0  ...   96.0               1.93         96.0   \n",
              "2490         73.0      1014  ...   75.0               3.38         73.0   \n",
              "2492         94.0        47  ...   97.0               5.40         97.0   \n",
              "\n",
              "       HIV/AIDS           GDP  Population   thinness  1-19 years  \\\n",
              "3           0.1  11326.219470    297555.0                    2.2   \n",
              "4           0.2   1619.532678     18745.0                    5.7   \n",
              "7           0.1   3386.256000  43285634.0                    2.4   \n",
              "11          4.4    729.231453    289315.0                    7.3   \n",
              "12          0.1   1452.277660   1242216.0                    1.6   \n",
              "...         ...           ...         ...                    ...   \n",
              "2481        0.1   7318.126410   1247429.0                    7.3   \n",
              "2485        1.6    697.153124   1466597.0                    9.0   \n",
              "2488        0.1    394.467675   4935762.0                    3.2   \n",
              "2490        2.6   2878.837144   2997687.0                    9.6   \n",
              "2492        0.1   2213.914880   9699197.0                    6.6   \n",
              "\n",
              "       thinness 5-9 years  Income composition of resources  Schooling  \n",
              "3                     2.3                            0.815       16.0  \n",
              "4                     5.5                            0.559       11.0  \n",
              "7                     2.1                            0.658       11.1  \n",
              "11                    7.0                            0.687       12.5  \n",
              "12                    1.5                            0.754       13.1  \n",
              "...                   ...                              ...        ...  \n",
              "2481                  7.3                            0.734       13.8  \n",
              "2485                  8.8                            0.385        7.1  \n",
              "2488                  3.3                            0.000       10.5  \n",
              "2490                  9.6                            0.454        7.7  \n",
              "2492                  6.5                            0.646       12.8  \n",
              "\n",
              "[1415 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94ee50cb-28cb-46fd-b14c-fbde2e6aff7e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>Year</th>\n",
              "      <th>Status</th>\n",
              "      <th>Life expectancy</th>\n",
              "      <th>Adult Mortality</th>\n",
              "      <th>infant deaths</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>percentage expenditure</th>\n",
              "      <th>Hepatitis B</th>\n",
              "      <th>Measles</th>\n",
              "      <th>...</th>\n",
              "      <th>Polio</th>\n",
              "      <th>Total expenditure</th>\n",
              "      <th>Diphtheria</th>\n",
              "      <th>HIV/AIDS</th>\n",
              "      <th>GDP</th>\n",
              "      <th>Population</th>\n",
              "      <th>thinness  1-19 years</th>\n",
              "      <th>thinness 5-9 years</th>\n",
              "      <th>Income composition of resources</th>\n",
              "      <th>Schooling</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Latvia</td>\n",
              "      <td>2010</td>\n",
              "      <td>Developed</td>\n",
              "      <td>72.8</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.80</td>\n",
              "      <td>1109.969508</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>92.0</td>\n",
              "      <td>6.55</td>\n",
              "      <td>92.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>11326.219470</td>\n",
              "      <td>297555.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.815</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sao Tome and Principe</td>\n",
              "      <td>2013</td>\n",
              "      <td>Developing</td>\n",
              "      <td>67.1</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>200.660099</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>97.0</td>\n",
              "      <td>9.76</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1619.532678</td>\n",
              "      <td>18745.0</td>\n",
              "      <td>5.7</td>\n",
              "      <td>5.5</td>\n",
              "      <td>0.559</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Colombia</td>\n",
              "      <td>2005</td>\n",
              "      <td>Developing</td>\n",
              "      <td>73.1</td>\n",
              "      <td>144.0</td>\n",
              "      <td>15</td>\n",
              "      <td>4.38</td>\n",
              "      <td>531.980818</td>\n",
              "      <td>93.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>93.0</td>\n",
              "      <td>5.82</td>\n",
              "      <td>93.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3386.256000</td>\n",
              "      <td>43285634.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>2.1</td>\n",
              "      <td>0.658</td>\n",
              "      <td>11.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Botswana</td>\n",
              "      <td>2012</td>\n",
              "      <td>Developing</td>\n",
              "      <td>63.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.01</td>\n",
              "      <td>12.834474</td>\n",
              "      <td>95.0</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>96.0</td>\n",
              "      <td>6.27</td>\n",
              "      <td>95.0</td>\n",
              "      <td>4.4</td>\n",
              "      <td>729.231453</td>\n",
              "      <td>289315.0</td>\n",
              "      <td>7.3</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.687</td>\n",
              "      <td>12.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Mexico</td>\n",
              "      <td>2014</td>\n",
              "      <td>Developing</td>\n",
              "      <td>76.6</td>\n",
              "      <td>122.0</td>\n",
              "      <td>31</td>\n",
              "      <td>5.26</td>\n",
              "      <td>168.173753</td>\n",
              "      <td>84.0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>87.0</td>\n",
              "      <td>6.30</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1452.277660</td>\n",
              "      <td>1242216.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.754</td>\n",
              "      <td>13.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2481</th>\n",
              "      <td>Mauritius</td>\n",
              "      <td>2009</td>\n",
              "      <td>Developing</td>\n",
              "      <td>72.8</td>\n",
              "      <td>166.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.83</td>\n",
              "      <td>624.236183</td>\n",
              "      <td>99.0</td>\n",
              "      <td>15</td>\n",
              "      <td>...</td>\n",
              "      <td>99.0</td>\n",
              "      <td>4.97</td>\n",
              "      <td>99.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>7318.126410</td>\n",
              "      <td>1247429.0</td>\n",
              "      <td>7.3</td>\n",
              "      <td>7.3</td>\n",
              "      <td>0.734</td>\n",
              "      <td>13.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2485</th>\n",
              "      <td>Mali</td>\n",
              "      <td>2009</td>\n",
              "      <td>Developing</td>\n",
              "      <td>56.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>55</td>\n",
              "      <td>0.59</td>\n",
              "      <td>84.634389</td>\n",
              "      <td>71.0</td>\n",
              "      <td>2939</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>6.85</td>\n",
              "      <td>73.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>697.153124</td>\n",
              "      <td>1466597.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.8</td>\n",
              "      <td>0.385</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2488</th>\n",
              "      <td>Turkmenistan</td>\n",
              "      <td>2008</td>\n",
              "      <td>Developing</td>\n",
              "      <td>64.5</td>\n",
              "      <td>235.0</td>\n",
              "      <td>6</td>\n",
              "      <td>2.40</td>\n",
              "      <td>34.239794</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>96.0</td>\n",
              "      <td>1.93</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>394.467675</td>\n",
              "      <td>4935762.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>3.3</td>\n",
              "      <td>0.000</td>\n",
              "      <td>10.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2490</th>\n",
              "      <td>Angola</td>\n",
              "      <td>2007</td>\n",
              "      <td>Developing</td>\n",
              "      <td>48.2</td>\n",
              "      <td>375.0</td>\n",
              "      <td>87</td>\n",
              "      <td>6.35</td>\n",
              "      <td>184.821345</td>\n",
              "      <td>73.0</td>\n",
              "      <td>1014</td>\n",
              "      <td>...</td>\n",
              "      <td>75.0</td>\n",
              "      <td>3.38</td>\n",
              "      <td>73.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>2878.837144</td>\n",
              "      <td>2997687.0</td>\n",
              "      <td>9.6</td>\n",
              "      <td>9.6</td>\n",
              "      <td>0.454</td>\n",
              "      <td>7.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2492</th>\n",
              "      <td>Tunisia</td>\n",
              "      <td>2000</td>\n",
              "      <td>Developing</td>\n",
              "      <td>72.9</td>\n",
              "      <td>112.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.21</td>\n",
              "      <td>264.784220</td>\n",
              "      <td>94.0</td>\n",
              "      <td>47</td>\n",
              "      <td>...</td>\n",
              "      <td>97.0</td>\n",
              "      <td>5.40</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2213.914880</td>\n",
              "      <td>9699197.0</td>\n",
              "      <td>6.6</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0.646</td>\n",
              "      <td>12.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1415 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94ee50cb-28cb-46fd-b14c-fbde2e6aff7e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-94ee50cb-28cb-46fd-b14c-fbde2e6aff7e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-94ee50cb-28cb-46fd-b14c-fbde2e6aff7e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df = df.dropna()\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tbAjlb2U_PT"
      },
      "source": [
        "Convert categorical columns into their one-hot encoded versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:33:50.336817Z",
          "iopub.status.busy": "2023-04-30T08:33:50.335572Z",
          "iopub.status.idle": "2023-04-30T08:33:50.349409Z",
          "shell.execute_reply": "2023-04-30T08:33:50.348213Z",
          "shell.execute_reply.started": "2023-04-30T08:33:50.336755Z"
        },
        "id": "shlrP4pTU_PT",
        "outputId": "28621ce0-d08a-4f26-dfe7-d6027b6be834",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Year  Life expectancy  Adult Mortality  infant deaths  Alcohol  \\\n",
              "3     2010             72.8             18.0              0     9.80   \n",
              "4     2013             67.1            192.0              0     0.01   \n",
              "7     2005             73.1            144.0             15     4.38   \n",
              "11    2012             63.4              3.0              2     0.01   \n",
              "12    2014             76.6            122.0             31     5.26   \n",
              "...    ...              ...              ...            ...      ...   \n",
              "2481  2009             72.8            166.0              0     2.83   \n",
              "2485  2009             56.0            276.0             55     0.59   \n",
              "2488  2008             64.5            235.0              6     2.40   \n",
              "2490  2007             48.2            375.0             87     6.35   \n",
              "2492  2000             72.9            112.0              4     1.21   \n",
              "\n",
              "      percentage expenditure  Hepatitis B  Measles    BMI   \\\n",
              "3                1109.969508         91.0         0   58.9   \n",
              "4                 200.660099         97.0         0   29.3   \n",
              "7                 531.980818         93.0         0    5.5   \n",
              "11                 12.834474         95.0         7   36.2   \n",
              "12                168.173753         84.0         3   62.8   \n",
              "...                      ...          ...       ...    ...   \n",
              "2481              624.236183         99.0        15    3.2   \n",
              "2485               84.634389         71.0      2939    2.2   \n",
              "2488               34.239794         96.0         0   42.7   \n",
              "2490              184.821345         73.0      1014   18.8   \n",
              "2492              264.784220         94.0        47   48.1   \n",
              "\n",
              "      under-five deaths   ...  Country_Turkmenistan  Country_Uganda  \\\n",
              "3                      0  ...                     0               0   \n",
              "4                      0  ...                     0               0   \n",
              "7                     18  ...                     0               0   \n",
              "11                     3  ...                     0               0   \n",
              "12                    36  ...                     0               0   \n",
              "...                  ...  ...                   ...             ...   \n",
              "2481                   0  ...                     0               0   \n",
              "2485                  93  ...                     0               0   \n",
              "2488                   8  ...                     1               0   \n",
              "2490                 138  ...                     0               0   \n",
              "2492                   5  ...                     0               0   \n",
              "\n",
              "      Country_Ukraine  Country_Uruguay  Country_Uzbekistan  Country_Vanuatu  \\\n",
              "3                   0                0                   0                0   \n",
              "4                   0                0                   0                0   \n",
              "7                   0                0                   0                0   \n",
              "11                  0                0                   0                0   \n",
              "12                  0                0                   0                0   \n",
              "...               ...              ...                 ...              ...   \n",
              "2481                0                0                   0                0   \n",
              "2485                0                0                   0                0   \n",
              "2488                0                0                   0                0   \n",
              "2490                0                0                   0                0   \n",
              "2492                0                0                   0                0   \n",
              "\n",
              "      Country_Zambia  Country_Zimbabwe  Status_Developed  Status_Developing  \n",
              "3                  0                 0                 1                  0  \n",
              "4                  0                 0                 0                  1  \n",
              "7                  0                 0                 0                  1  \n",
              "11                 0                 0                 0                  1  \n",
              "12                 0                 0                 0                  1  \n",
              "...              ...               ...               ...                ...  \n",
              "2481               0                 0                 0                  1  \n",
              "2485               0                 0                 0                  1  \n",
              "2488               0                 0                 0                  1  \n",
              "2490               0                 0                 0                  1  \n",
              "2492               0                 0                 0                  1  \n",
              "\n",
              "[1415 rows x 155 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a439d5e7-724b-477e-91d2-4a2e2e50a95b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Life expectancy</th>\n",
              "      <th>Adult Mortality</th>\n",
              "      <th>infant deaths</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>percentage expenditure</th>\n",
              "      <th>Hepatitis B</th>\n",
              "      <th>Measles</th>\n",
              "      <th>BMI</th>\n",
              "      <th>under-five deaths</th>\n",
              "      <th>...</th>\n",
              "      <th>Country_Turkmenistan</th>\n",
              "      <th>Country_Uganda</th>\n",
              "      <th>Country_Ukraine</th>\n",
              "      <th>Country_Uruguay</th>\n",
              "      <th>Country_Uzbekistan</th>\n",
              "      <th>Country_Vanuatu</th>\n",
              "      <th>Country_Zambia</th>\n",
              "      <th>Country_Zimbabwe</th>\n",
              "      <th>Status_Developed</th>\n",
              "      <th>Status_Developing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010</td>\n",
              "      <td>72.8</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.80</td>\n",
              "      <td>1109.969508</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0</td>\n",
              "      <td>58.9</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2013</td>\n",
              "      <td>67.1</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>200.660099</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0</td>\n",
              "      <td>29.3</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2005</td>\n",
              "      <td>73.1</td>\n",
              "      <td>144.0</td>\n",
              "      <td>15</td>\n",
              "      <td>4.38</td>\n",
              "      <td>531.980818</td>\n",
              "      <td>93.0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.5</td>\n",
              "      <td>18</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2012</td>\n",
              "      <td>63.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.01</td>\n",
              "      <td>12.834474</td>\n",
              "      <td>95.0</td>\n",
              "      <td>7</td>\n",
              "      <td>36.2</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2014</td>\n",
              "      <td>76.6</td>\n",
              "      <td>122.0</td>\n",
              "      <td>31</td>\n",
              "      <td>5.26</td>\n",
              "      <td>168.173753</td>\n",
              "      <td>84.0</td>\n",
              "      <td>3</td>\n",
              "      <td>62.8</td>\n",
              "      <td>36</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2481</th>\n",
              "      <td>2009</td>\n",
              "      <td>72.8</td>\n",
              "      <td>166.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.83</td>\n",
              "      <td>624.236183</td>\n",
              "      <td>99.0</td>\n",
              "      <td>15</td>\n",
              "      <td>3.2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2485</th>\n",
              "      <td>2009</td>\n",
              "      <td>56.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>55</td>\n",
              "      <td>0.59</td>\n",
              "      <td>84.634389</td>\n",
              "      <td>71.0</td>\n",
              "      <td>2939</td>\n",
              "      <td>2.2</td>\n",
              "      <td>93</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2488</th>\n",
              "      <td>2008</td>\n",
              "      <td>64.5</td>\n",
              "      <td>235.0</td>\n",
              "      <td>6</td>\n",
              "      <td>2.40</td>\n",
              "      <td>34.239794</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0</td>\n",
              "      <td>42.7</td>\n",
              "      <td>8</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2490</th>\n",
              "      <td>2007</td>\n",
              "      <td>48.2</td>\n",
              "      <td>375.0</td>\n",
              "      <td>87</td>\n",
              "      <td>6.35</td>\n",
              "      <td>184.821345</td>\n",
              "      <td>73.0</td>\n",
              "      <td>1014</td>\n",
              "      <td>18.8</td>\n",
              "      <td>138</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2492</th>\n",
              "      <td>2000</td>\n",
              "      <td>72.9</td>\n",
              "      <td>112.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.21</td>\n",
              "      <td>264.784220</td>\n",
              "      <td>94.0</td>\n",
              "      <td>47</td>\n",
              "      <td>48.1</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1415 rows × 155 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a439d5e7-724b-477e-91d2-4a2e2e50a95b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a439d5e7-724b-477e-91d2-4a2e2e50a95b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a439d5e7-724b-477e-91d2-4a2e2e50a95b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df = pd.get_dummies(df, columns=['Country', 'Status'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6gaeeHxU_PT"
      },
      "source": [
        "Convert the dataframe into two numpy arrays (called `x` and `y`).<br>\n",
        "To make the first array, remove the \"Life expectancy\" column, and convert the remaining dataframe to a numpy array.<br>\n",
        "Then, use the removed column to make another numpy array (`y`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:33:57.364230Z",
          "iopub.status.busy": "2023-04-30T08:33:57.363830Z",
          "iopub.status.idle": "2023-04-30T08:33:57.373246Z",
          "shell.execute_reply": "2023-04-30T08:33:57.372021Z",
          "shell.execute_reply.started": "2023-04-30T08:33:57.364195Z"
        },
        "id": "dbN1zZ-dU_PT",
        "outputId": "10b1ec0b-e24b-4db7-e3a9-74ef6ac13bc2",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[2.010e+03, 1.800e+01, 0.000e+00, ..., 0.000e+00, 1.000e+00,\n",
              "         0.000e+00],\n",
              "        [2.013e+03, 1.920e+02, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
              "         1.000e+00],\n",
              "        [2.005e+03, 1.440e+02, 1.500e+01, ..., 0.000e+00, 0.000e+00,\n",
              "         1.000e+00],\n",
              "        ...,\n",
              "        [2.008e+03, 2.350e+02, 6.000e+00, ..., 0.000e+00, 0.000e+00,\n",
              "         1.000e+00],\n",
              "        [2.007e+03, 3.750e+02, 8.700e+01, ..., 0.000e+00, 0.000e+00,\n",
              "         1.000e+00],\n",
              "        [2.000e+03, 1.120e+02, 4.000e+00, ..., 0.000e+00, 0.000e+00,\n",
              "         1.000e+00]]),\n",
              " array([72.8, 67.1, 73.1, ..., 64.5, 48.2, 72.9]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "x_df = df.drop(\"Life expectancy\", axis = 1)\n",
        "x = x_df.to_numpy()\n",
        "y = df.loc[:, \"Life expectancy\"].values\n",
        "x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0OE0dA1U_PT"
      },
      "source": [
        "Normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:34:01.195165Z",
          "iopub.status.busy": "2023-04-30T08:34:01.194507Z",
          "iopub.status.idle": "2023-04-30T08:34:01.204086Z",
          "shell.execute_reply": "2023-04-30T08:34:01.202988Z",
          "shell.execute_reply.started": "2023-04-30T08:34:01.195123Z"
        },
        "id": "SBYXkQCbU_PT",
        "outputId": "4aea94fb-9670-4bac-d919-7c4d31791118",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.74996354e-03, 6.04474347e-05, 0.00000000e+00, ...,\n",
              "        0.00000000e+00, 3.35819082e-06, 0.00000000e+00],\n",
              "       [1.06367098e-01, 1.01452970e-02, 0.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 5.28400884e-05],\n",
              "       [4.63202177e-05, 3.32673883e-06, 3.46535295e-07, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 2.31023530e-08],\n",
              "       ...,\n",
              "       [4.06826712e-04, 4.76116918e-05, 1.21561766e-06, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 2.02602944e-07],\n",
              "       [6.69515692e-04, 1.25096355e-04, 2.90223544e-05, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 3.33590280e-07],\n",
              "       [2.06202627e-04, 1.15473471e-05, 4.12405255e-07, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 1.03101314e-07]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "x = preprocessing.normalize(x)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GwHCHMlU_PU"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IksGhWU6U_PU"
      },
      "source": [
        "Split the data to training (80%) and testing (20%) parts (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:34:04.739581Z",
          "iopub.status.busy": "2023-04-30T08:34:04.739033Z",
          "iopub.status.idle": "2023-04-30T08:34:04.748932Z",
          "shell.execute_reply": "2023-04-30T08:34:04.747440Z",
          "shell.execute_reply.started": "2023-04-30T08:34:04.739529Z"
        },
        "id": "YAdRX8VVU_PU",
        "outputId": "5699f08d-327e-4e2a-c981-e1d683eee665",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[2.56443445e-01, 2.33943920e-02, 1.27838208e-03, ...,\n",
              "         0.00000000e+00, 0.00000000e+00, 1.27838208e-04],\n",
              "        [1.09595114e-04, 6.94193491e-06, 4.91948143e-07, ...,\n",
              "         0.00000000e+00, 0.00000000e+00, 5.46609048e-08],\n",
              "        [2.97773224e-03, 2.29511933e-04, 1.48072215e-05, ...,\n",
              "         0.00000000e+00, 0.00000000e+00, 1.48072215e-06],\n",
              "        ...,\n",
              "        [3.11723605e-05, 2.18097475e-07, 6.38714034e-07, ...,\n",
              "         0.00000000e+00, 0.00000000e+00, 1.55783911e-08],\n",
              "        [2.25263815e-02, 2.41795907e-03, 0.00000000e+00, ...,\n",
              "         0.00000000e+00, 0.00000000e+00, 1.12463213e-05],\n",
              "        [1.67705768e-03, 1.00322493e-05, 2.50806233e-06, ...,\n",
              "         0.00000000e+00, 0.00000000e+00, 8.36020777e-07]]),\n",
              " array([65.9, 73.5, 74.1, ..., 78. , 64.7, 74.4]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2)\n",
        "X_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMBCL_FAU_PU"
      },
      "source": [
        "# Common Regression Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVm0vp2_U_PU"
      },
      "source": [
        "This is a regression class and you do not need to change its code. You have to fully understand it and then create new classes for rigde and lasso regression which inherit from this class.\n",
        "For initialization you have to specify learning rate, number of iteration, and a regularization object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-30T08:34:15.970344Z",
          "iopub.status.busy": "2023-04-30T08:34:15.969955Z",
          "iopub.status.idle": "2023-04-30T08:34:15.982914Z",
          "shell.execute_reply": "2023-04-30T08:34:15.981717Z",
          "shell.execute_reply.started": "2023-04-30T08:34:15.970307Z"
        },
        "id": "G-YfrGzAU_PU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Regression:\n",
        "    def __init__(self, learning_rate, iteration, regularization):\n",
        "        \n",
        "        self.N = None # number of samples\n",
        "        self.n = None # number of features\n",
        "        self.w = None # initial weights\n",
        "        self.regularization = regularization # will be the l1/l2 regularization class according to the regression model\n",
        "        self.lr = learning_rate\n",
        "        self.it = iteration\n",
        "\n",
        "    def loss_function(self, y, y_pred):\n",
        "        return (1 / (2*self.N)) * np.sum(np.square(y_pred - y)) + self.regularization(self.w)\n",
        "    \n",
        "    def hypothesis(self, weights, X):\n",
        "        return np.dot(X, weights)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        # Target value should be in the shape of (n, 1) not (n, ).\n",
        "\n",
        "        # Insert constant ones for bias weights.\n",
        "        X = np.insert(X, 0, 1, axis=1)\n",
        "\n",
        "        self.N = X.shape[0]\n",
        "        self.n = X.shape[1]\n",
        "        self.w = np.zeros((self.n , 1))\n",
        "\n",
        "        for it in range(1, self.it+1):\n",
        "            y_pred = self.hypothesis(self.w, X)\n",
        "            cost = self.loss_function(y, y_pred)\n",
        "            dw = (1/self.N) * np.dot(X.T, (y_pred - y)) + self.regularization.derivation(self.w)\n",
        "            self.w = self.w - self.lr * dw\n",
        "\n",
        "            if it % 10 == 0:\n",
        "                print(\"The loss function for the iteration {}----->{} :)\".format(it, cost))\n",
        "    \n",
        "    def predict(self, test_X):\n",
        "        # Insert constant ones for bias weights.\n",
        "        test_X = np.insert(test_X, 0, 1, axis=1)\n",
        "        y_pred = self.hypothesis(self.w, test_X)\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE6pCA4MU_PU"
      },
      "source": [
        "# Regularization Classes (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "101Ics8WU_PU"
      },
      "source": [
        "You have to implement l2/l1 regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-30T08:34:19.277915Z",
          "iopub.status.busy": "2023-04-30T08:34:19.277529Z",
          "iopub.status.idle": "2023-04-30T08:34:19.286725Z",
          "shell.execute_reply": "2023-04-30T08:34:19.285489Z",
          "shell.execute_reply.started": "2023-04-30T08:34:19.277881Z"
        },
        "id": "KkJ1I97gU_PU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class l1_regularization:\n",
        "    \"\"\"Regularization used for Lasso Regression\"\"\"\n",
        "    def __init__(self, lamda):\n",
        "        self.lamda = lamda\n",
        "\n",
        "    def __call__(self, weights):\n",
        "        \"This will be returned when we call this class.\"\n",
        "        return self.lamda * np.sum(np.abs(weights))\n",
        "        \n",
        "    \n",
        "    def derivation(self, weights):\n",
        "        \"Derivation of the regulariozation function.\"\n",
        "        return self.lamda * np.sign(weights)\n",
        "        \n",
        "\n",
        "\n",
        "class l2_regularization:\n",
        "    \"\"\"Regularization used for Ridge Regression\"\"\"\n",
        "    def __init__(self, lamda):\n",
        "        self.lamda = lamda\n",
        "\n",
        "    def __call__(self, weights):\n",
        "        \"This will be retuned when we call this class.\"\n",
        "        return self.lamda * np.sum(np.square(weights))\n",
        "    \n",
        "    def derivation(self, weights):\n",
        "        \"Derivation of the regulariozation function.\"\n",
        "        # return 2 * self.lamda * np.sum(weights)\n",
        "        return 2 * self.lamda * weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYUC1u_FU_PU"
      },
      "source": [
        "# Lasso Regression from scratch (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxjF4g7rU_PU"
      },
      "source": [
        "Train a lasso regression model using your own code and the following class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-30T08:34:23.922713Z",
          "iopub.status.busy": "2023-04-30T08:34:23.921661Z",
          "iopub.status.idle": "2023-04-30T08:34:23.928316Z",
          "shell.execute_reply": "2023-04-30T08:34:23.927223Z",
          "shell.execute_reply.started": "2023-04-30T08:34:23.922672Z"
        },
        "id": "Zl180mzhU_PU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class LassoRegression(Regression):\n",
        "    def __init__(self, lamda, learning_rate, iteration):\n",
        "        self.regularization = l1_regularization(lamda)\n",
        "        super(LassoRegression, self).__init__(learning_rate, iteration, self.regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:34:43.634909Z",
          "iopub.status.busy": "2023-04-30T08:34:43.634507Z",
          "iopub.status.idle": "2023-04-30T08:34:43.669074Z",
          "shell.execute_reply": "2023-04-30T08:34:43.667346Z",
          "shell.execute_reply.started": "2023-04-30T08:34:43.634873Z"
        },
        "id": "Vm0z4ECLU_PV",
        "outputId": "399b9daf-e9aa-4351-9813-1fe5cff29ac3",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss function for the iteration 10----->1729.1724224751358 :)\n",
            "The loss function for the iteration 20----->1185.1427844246878 :)\n",
            "The loss function for the iteration 30----->817.6992440659516 :)\n",
            "The loss function for the iteration 40----->569.510673614206 :)\n",
            "The loss function for the iteration 50----->401.8593900461504 :)\n",
            "The loss function for the iteration 60----->288.5980136075604 :)\n",
            "The loss function for the iteration 70----->212.0683241840306 :)\n",
            "The loss function for the iteration 80----->160.34511729744509 :)\n",
            "The loss function for the iteration 90----->125.37488738875452 :)\n",
            "The loss function for the iteration 100----->101.7188288952608 :)\n",
            "The loss function for the iteration 110----->85.7039236336268 :)\n",
            "The loss function for the iteration 120----->74.84965408609176 :)\n",
            "The loss function for the iteration 130----->67.48084168049179 :)\n",
            "The loss function for the iteration 140----->62.46617461830971 :)\n",
            "The loss function for the iteration 150----->59.041643994357244 :)\n",
            "The loss function for the iteration 160----->56.69127975815445 :)\n",
            "The loss function for the iteration 170----->55.06663058729138 :)\n",
            "The loss function for the iteration 180----->53.93236851720053 :)\n",
            "The loss function for the iteration 190----->53.12957407695444 :)\n",
            "The loss function for the iteration 200----->52.55091320760711 :)\n",
            "The loss function for the iteration 210----->52.12390267594 :)\n",
            "The loss function for the iteration 220----->51.799587301488316 :)\n",
            "The loss function for the iteration 230----->51.544903824475696 :)\n",
            "The loss function for the iteration 240----->51.33752426994425 :)\n",
            "The loss function for the iteration 250----->51.162363161319995 :)\n",
            "The loss function for the iteration 260----->51.00923341706988 :)\n",
            "The loss function for the iteration 270----->50.87125344295938 :)\n",
            "The loss function for the iteration 280----->50.74377135349955 :)\n",
            "The loss function for the iteration 290----->50.62364737479617 :)\n",
            "The loss function for the iteration 300----->50.508752963825636 :)\n",
            "The loss function for the iteration 310----->50.397652703590765 :)\n",
            "The loss function for the iteration 320----->50.28937969372124 :)\n",
            "The loss function for the iteration 330----->50.18326908328708 :)\n",
            "The loss function for the iteration 340----->50.07887582103101 :)\n",
            "The loss function for the iteration 350----->49.97589792850607 :)\n",
            "The loss function for the iteration 360----->49.87412538728435 :)\n",
            "The loss function for the iteration 370----->49.773417504910206 :)\n",
            "The loss function for the iteration 380----->49.67367700993482 :)\n",
            "The loss function for the iteration 390----->49.57483708274171 :)\n",
            "The loss function for the iteration 400----->49.47684760425417 :)\n",
            "The loss function for the iteration 410----->49.379674206225594 :)\n",
            "The loss function for the iteration 420----->49.283295376840975 :)\n",
            "The loss function for the iteration 430----->49.18768898474502 :)\n",
            "The loss function for the iteration 440----->49.09283911440503 :)\n",
            "The loss function for the iteration 450----->48.99873899525662 :)\n",
            "The loss function for the iteration 460----->48.9053773667346 :)\n",
            "The loss function for the iteration 470----->48.81274228184304 :)\n",
            "The loss function for the iteration 480----->48.720830868250594 :)\n",
            "The loss function for the iteration 490----->48.62963454904289 :)\n",
            "The loss function for the iteration 500----->48.53914475944237 :)\n",
            "The loss function for the iteration 510----->48.44935878015526 :)\n",
            "The loss function for the iteration 520----->48.36026834681741 :)\n",
            "The loss function for the iteration 530----->48.271867949099494 :)\n",
            "The loss function for the iteration 540----->48.184155343525454 :)\n",
            "The loss function for the iteration 550----->48.09712120206501 :)\n",
            "The loss function for the iteration 560----->48.01075868733381 :)\n",
            "The loss function for the iteration 570----->47.92506776164439 :)\n",
            "The loss function for the iteration 580----->47.840039778436726 :)\n",
            "The loss function for the iteration 590----->47.7556699236977 :)\n",
            "The loss function for the iteration 600----->47.67195614535001 :)\n",
            "The loss function for the iteration 610----->47.58888904971887 :)\n",
            "The loss function for the iteration 620----->47.506464347862725 :)\n",
            "The loss function for the iteration 630----->47.42467898593416 :)\n",
            "The loss function for the iteration 640----->47.343525809024996 :)\n",
            "The loss function for the iteration 650----->47.26300019825144 :)\n",
            "The loss function for the iteration 660----->47.18309816853491 :)\n",
            "The loss function for the iteration 670----->47.10381503381717 :)\n",
            "The loss function for the iteration 680----->47.02514503690212 :)\n",
            "The loss function for the iteration 690----->46.947083496544906 :)\n",
            "The loss function for the iteration 700----->46.86962977136529 :)\n",
            "The loss function for the iteration 710----->46.79277378828024 :)\n",
            "The loss function for the iteration 720----->46.7165113792347 :)\n",
            "The loss function for the iteration 730----->46.640841840756316 :)\n",
            "The loss function for the iteration 740----->46.565755051958554 :)\n",
            "The loss function for the iteration 750----->46.49124863477946 :)\n",
            "The loss function for the iteration 760----->46.41732031894165 :)\n",
            "The loss function for the iteration 770----->46.3439635865089 :)\n",
            "The loss function for the iteration 780----->46.271173557244055 :)\n",
            "The loss function for the iteration 790----->46.19894646879243 :)\n",
            "The loss function for the iteration 800----->46.127278640518725 :)\n",
            "The loss function for the iteration 810----->46.05616267364188 :)\n",
            "The loss function for the iteration 820----->45.98559685359621 :)\n",
            "The loss function for the iteration 830----->45.91557809239807 :)\n",
            "The loss function for the iteration 840----->45.84609863459415 :)\n",
            "The loss function for the iteration 850----->45.77715790374207 :)\n",
            "The loss function for the iteration 860----->45.7087477112627 :)\n",
            "The loss function for the iteration 870----->45.640867707891424 :)\n",
            "The loss function for the iteration 880----->45.573511314268444 :)\n",
            "The loss function for the iteration 890----->45.50667303919216 :)\n",
            "The loss function for the iteration 900----->45.440354642742356 :)\n",
            "The loss function for the iteration 910----->45.3745479878524 :)\n",
            "The loss function for the iteration 920----->45.30924874468106 :)\n",
            "The loss function for the iteration 930----->45.244453586175354 :)\n",
            "The loss function for the iteration 940----->45.18015939047456 :)\n",
            "The loss function for the iteration 950----->45.116359436126494 :)\n",
            "The loss function for the iteration 960----->45.053054580853185 :)\n",
            "The loss function for the iteration 970----->44.99023846114743 :)\n",
            "The loss function for the iteration 980----->44.92790781110978 :)\n",
            "The loss function for the iteration 990----->44.86605601404777 :)\n",
            "The loss function for the iteration 1000----->44.80468090443373 :)\n",
            "The loss function for the iteration 1010----->44.74378146967816 :)\n",
            "The loss function for the iteration 1020----->44.68334994003571 :)\n",
            "The loss function for the iteration 1030----->44.623386121274336 :)\n",
            "The loss function for the iteration 1040----->44.56388414854853 :)\n",
            "The loss function for the iteration 1050----->44.50484077208898 :)\n",
            "The loss function for the iteration 1060----->44.4462514303542 :)\n",
            "The loss function for the iteration 1070----->44.38811407606612 :)\n",
            "The loss function for the iteration 1080----->44.330427018053136 :)\n",
            "The loss function for the iteration 1090----->44.2731853946103 :)\n",
            "The loss function for the iteration 1100----->44.21638214520886 :)\n",
            "The loss function for the iteration 1110----->44.16001879252447 :)\n",
            "The loss function for the iteration 1120----->44.104087724859845 :)\n",
            "The loss function for the iteration 1130----->44.048590422650854 :)\n",
            "The loss function for the iteration 1140----->43.99351878729231 :)\n",
            "The loss function for the iteration 1150----->43.93887194813887 :)\n",
            "The loss function for the iteration 1160----->43.88464652919045 :)\n",
            "The loss function for the iteration 1170----->43.83083625878532 :)\n",
            "The loss function for the iteration 1180----->43.77744225315694 :)\n",
            "The loss function for the iteration 1190----->43.72445884480011 :)\n",
            "The loss function for the iteration 1200----->43.671885366519675 :)\n",
            "The loss function for the iteration 1210----->43.61971479528052 :)\n",
            "The loss function for the iteration 1220----->43.56794527345936 :)\n",
            "The loss function for the iteration 1230----->43.516573170077 :)\n",
            "The loss function for the iteration 1240----->43.46559847838477 :)\n",
            "The loss function for the iteration 1250----->43.41501742168476 :)\n",
            "The loss function for the iteration 1260----->43.36482153444218 :)\n",
            "The loss function for the iteration 1270----->43.31501468813623 :)\n",
            "The loss function for the iteration 1280----->43.26558795027695 :)\n",
            "The loss function for the iteration 1290----->43.216545277870445 :)\n",
            "The loss function for the iteration 1300----->43.167876304292285 :)\n",
            "The loss function for the iteration 1310----->43.11958616988914 :)\n",
            "The loss function for the iteration 1320----->43.07166314248859 :)\n",
            "The loss function for the iteration 1330----->43.02411053340107 :)\n",
            "The loss function for the iteration 1340----->42.97692014468153 :)\n",
            "The loss function for the iteration 1350----->42.93009631956065 :)\n",
            "The loss function for the iteration 1360----->42.88363133432979 :)\n",
            "The loss function for the iteration 1370----->42.83752327495513 :)\n",
            "The loss function for the iteration 1380----->42.79176740489677 :)\n",
            "The loss function for the iteration 1390----->42.746363192971295 :)\n",
            "The loss function for the iteration 1400----->42.70131035472675 :)\n",
            "The loss function for the iteration 1410----->42.65660185823037 :)\n",
            "The loss function for the iteration 1420----->42.61223695332946 :)\n",
            "The loss function for the iteration 1430----->42.568209645987174 :)\n",
            "The loss function for the iteration 1440----->42.52452519512099 :)\n",
            "The loss function for the iteration 1450----->42.48117341952232 :)\n",
            "The loss function for the iteration 1460----->42.43815756639821 :)\n",
            "The loss function for the iteration 1470----->42.395465112644274 :)\n",
            "The loss function for the iteration 1480----->42.35310390079195 :)\n",
            "The loss function for the iteration 1490----->42.311065737864766 :)\n",
            "The loss function for the iteration 1500----->42.269354376546524 :)\n",
            "The loss function for the iteration 1510----->42.22795874365301 :)\n",
            "The loss function for the iteration 1520----->42.18688150951017 :)\n",
            "The loss function for the iteration 1530----->42.146118249568275 :)\n",
            "The loss function for the iteration 1540----->42.105669224359445 :)\n",
            "The loss function for the iteration 1550----->42.06553033367703 :)\n",
            "The loss function for the iteration 1560----->42.02569856612048 :)\n",
            "The loss function for the iteration 1570----->41.98617012908966 :)\n",
            "The loss function for the iteration 1580----->41.94694607835646 :)\n",
            "The loss function for the iteration 1590----->41.90802134885422 :)\n",
            "The loss function for the iteration 1600----->41.86939658811577 :)\n",
            "The loss function for the iteration 1610----->41.8310652989192 :)\n",
            "The loss function for the iteration 1620----->41.79302920891198 :)\n",
            "The loss function for the iteration 1630----->41.75528364729814 :)\n",
            "The loss function for the iteration 1640----->41.71782696366026 :)\n",
            "The loss function for the iteration 1650----->41.68065852366423 :)\n",
            "The loss function for the iteration 1660----->41.64377157428563 :)\n",
            "The loss function for the iteration 1670----->41.607169234569774 :)\n",
            "The loss function for the iteration 1680----->41.57084248649342 :)\n",
            "The loss function for the iteration 1690----->41.53479826606015 :)\n",
            "The loss function for the iteration 1700----->41.49902617855055 :)\n",
            "The loss function for the iteration 1710----->41.463530071173246 :)\n",
            "The loss function for the iteration 1720----->41.42830306486486 :)\n",
            "The loss function for the iteration 1730----->41.393346781272754 :)\n",
            "The loss function for the iteration 1740----->41.358657900086435 :)\n",
            "The loss function for the iteration 1750----->41.324232824573144 :)\n",
            "The loss function for the iteration 1760----->41.29007041183556 :)\n",
            "The loss function for the iteration 1770----->41.25616837526957 :)\n",
            "The loss function for the iteration 1780----->41.22252647917011 :)\n",
            "The loss function for the iteration 1790----->41.189141488547385 :)\n",
            "The loss function for the iteration 1800----->41.15601347173739 :)\n",
            "The loss function for the iteration 1810----->41.12313369387639 :)\n",
            "The loss function for the iteration 1820----->41.09050676654875 :)\n",
            "The loss function for the iteration 1830----->41.05812822360343 :)\n",
            "The loss function for the iteration 1840----->41.02599851730129 :)\n",
            "The loss function for the iteration 1850----->40.994112940535 :)\n",
            "The loss function for the iteration 1860----->40.96246621976682 :)\n",
            "The loss function for the iteration 1870----->40.931067241683905 :)\n",
            "The loss function for the iteration 1880----->40.89990389891968 :)\n",
            "The loss function for the iteration 1890----->40.86897884264946 :)\n",
            "The loss function for the iteration 1900----->40.83828948878904 :)\n",
            "The loss function for the iteration 1910----->40.807832497331916 :)\n",
            "The loss function for the iteration 1920----->40.77760766391314 :)\n",
            "The loss function for the iteration 1930----->40.747611463847555 :)\n",
            "The loss function for the iteration 1940----->40.71784786685864 :)\n",
            "The loss function for the iteration 1950----->40.68830845586469 :)\n",
            "The loss function for the iteration 1960----->40.65899397838819 :)\n",
            "The loss function for the iteration 1970----->40.62990137117939 :)\n",
            "The loss function for the iteration 1980----->40.60102995923247 :)\n",
            "The loss function for the iteration 1990----->40.57238117283168 :)\n",
            "The loss function for the iteration 2000----->40.54394603776918 :)\n",
            "The loss function for the iteration 2010----->40.515730162948095 :)\n",
            "The loss function for the iteration 2020----->40.48772698841384 :)\n",
            "The loss function for the iteration 2030----->40.45993800483106 :)\n",
            "The loss function for the iteration 2040----->40.43236203324621 :)\n",
            "The loss function for the iteration 2050----->40.404988401296436 :)\n",
            "The loss function for the iteration 2060----->40.37782982943493 :)\n",
            "The loss function for the iteration 2070----->40.35087525289637 :)\n",
            "The loss function for the iteration 2080----->40.32412245802556 :)\n",
            "The loss function for the iteration 2090----->40.29757773322458 :)\n",
            "The loss function for the iteration 2100----->40.27122921633479 :)\n",
            "The loss function for the iteration 2110----->40.245084372940646 :)\n",
            "The loss function for the iteration 2120----->40.21913674866883 :)\n",
            "The loss function for the iteration 2130----->40.1933844678782 :)\n",
            "The loss function for the iteration 2140----->40.16782824182738 :)\n",
            "The loss function for the iteration 2150----->40.14246499449365 :)\n",
            "The loss function for the iteration 2160----->40.11729335673391 :)\n",
            "The loss function for the iteration 2170----->40.09231683624818 :)\n",
            "The loss function for the iteration 2180----->40.06752428001924 :)\n",
            "The loss function for the iteration 2190----->40.042921722726106 :)\n",
            "The loss function for the iteration 2200----->40.0185033302553 :)\n",
            "The loss function for the iteration 2210----->39.99427117189548 :)\n",
            "The loss function for the iteration 2220----->39.970224501602075 :)\n",
            "The loss function for the iteration 2230----->39.94635478697051 :)\n",
            "The loss function for the iteration 2240----->39.92267024735032 :)\n",
            "The loss function for the iteration 2250----->39.89916446400647 :)\n",
            "The loss function for the iteration 2260----->39.875831674943946 :)\n",
            "The loss function for the iteration 2270----->39.852679076132155 :)\n",
            "The loss function for the iteration 2280----->39.82970053537716 :)\n",
            "The loss function for the iteration 2290----->39.80689473995191 :)\n",
            "The loss function for the iteration 2300----->39.78426401338617 :)\n",
            "The loss function for the iteration 2310----->39.76180274546235 :)\n",
            "The loss function for the iteration 2320----->39.73950936809415 :)\n",
            "The loss function for the iteration 2330----->39.71738477398472 :)\n",
            "The loss function for the iteration 2340----->39.695427865229334 :)\n",
            "The loss function for the iteration 2350----->39.67363603258016 :)\n",
            "The loss function for the iteration 2360----->39.65200999230422 :)\n",
            "The loss function for the iteration 2370----->39.630545111087706 :)\n",
            "The loss function for the iteration 2380----->39.60924174832084 :)\n",
            "The loss function for the iteration 2390----->39.58810134639626 :)\n",
            "The loss function for the iteration 2400----->39.56711573354022 :)\n",
            "The loss function for the iteration 2410----->39.54629095686211 :)\n",
            "The loss function for the iteration 2420----->39.52562350290082 :)\n",
            "The loss function for the iteration 2430----->39.505110950292334 :)\n",
            "The loss function for the iteration 2440----->39.484753463909584 :)\n",
            "The loss function for the iteration 2450----->39.46454709868079 :)\n",
            "The loss function for the iteration 2460----->39.44449257825938 :)\n",
            "The loss function for the iteration 2470----->39.42459112655803 :)\n",
            "The loss function for the iteration 2480----->39.4048387779153 :)\n",
            "The loss function for the iteration 2490----->39.385232063730776 :)\n",
            "The loss function for the iteration 2500----->39.36577640011419 :)\n",
            "The loss function for the iteration 2510----->39.346465167304885 :)\n",
            "The loss function for the iteration 2520----->39.32729764505117 :)\n",
            "The loss function for the iteration 2530----->39.30827551557095 :)\n",
            "The loss function for the iteration 2540----->39.28939646768915 :)\n",
            "The loss function for the iteration 2550----->39.27065716369743 :)\n",
            "The loss function for the iteration 2560----->39.252060119542605 :)\n",
            "The loss function for the iteration 2570----->39.233601617234775 :)\n",
            "The loss function for the iteration 2580----->39.215280501197505 :)\n",
            "The loss function for the iteration 2590----->39.197098938909136 :)\n",
            "The loss function for the iteration 2600----->39.17905332014442 :)\n",
            "The loss function for the iteration 2610----->39.16114130627059 :)\n",
            "The loss function for the iteration 2620----->39.14336216821664 :)\n",
            "The loss function for the iteration 2630----->39.12571761267798 :)\n",
            "The loss function for the iteration 2640----->39.1082050474414 :)\n",
            "The loss function for the iteration 2650----->39.090825847486244 :)\n",
            "The loss function for the iteration 2660----->39.073574065588836 :)\n",
            "The loss function for the iteration 2670----->39.05644982641714 :)\n",
            "The loss function for the iteration 2680----->39.03945448385996 :)\n",
            "The loss function for the iteration 2690----->39.02258673034224 :)\n",
            "The loss function for the iteration 2700----->39.005842648801206 :)\n",
            "The loss function for the iteration 2710----->38.98922591014978 :)\n",
            "The loss function for the iteration 2720----->38.97273480982224 :)\n",
            "The loss function for the iteration 2730----->38.95636448808364 :)\n",
            "The loss function for the iteration 2740----->38.940114833858466 :)\n",
            "The loss function for the iteration 2750----->38.92398785149448 :)\n",
            "The loss function for the iteration 2760----->38.90798132036785 :)\n",
            "The loss function for the iteration 2770----->38.892092515079945 :)\n",
            "The loss function for the iteration 2780----->38.876322511774625 :)\n",
            "The loss function for the iteration 2790----->38.860670158399195 :)\n",
            "The loss function for the iteration 2800----->38.845134644229056 :)\n",
            "The loss function for the iteration 2810----->38.829713582126466 :)\n",
            "The loss function for the iteration 2820----->38.814409633766225 :)\n",
            "The loss function for the iteration 2830----->38.79921808080692 :)\n",
            "The loss function for the iteration 2840----->38.78413835241177 :)\n",
            "The loss function for the iteration 2850----->38.76916923880953 :)\n",
            "The loss function for the iteration 2860----->38.75431543277085 :)\n",
            "The loss function for the iteration 2870----->38.73956926451729 :)\n",
            "The loss function for the iteration 2880----->38.724932222609866 :)\n",
            "The loss function for the iteration 2890----->38.710405373312206 :)\n",
            "The loss function for the iteration 2900----->38.69598683870536 :)\n",
            "The loss function for the iteration 2910----->38.68167128884616 :)\n",
            "The loss function for the iteration 2920----->38.667463898897296 :)\n",
            "The loss function for the iteration 2930----->38.653364042662105 :)\n",
            "The loss function for the iteration 2940----->38.63936498382144 :)\n",
            "The loss function for the iteration 2950----->38.625470132173724 :)\n",
            "The loss function for the iteration 2960----->38.61167891502301 :)\n",
            "The loss function for the iteration 2970----->38.597990608296236 :)\n",
            "The loss function for the iteration 2980----->38.58440178386116 :)\n",
            "The loss function for the iteration 2990----->38.57091505596912 :)\n",
            "The loss function for the iteration 3000----->38.55752629598082 :)\n",
            "The loss function for the iteration 3010----->38.54423572629517 :)\n",
            "The loss function for the iteration 3020----->38.53104435996896 :)\n",
            "The loss function for the iteration 3030----->38.5179492649047 :)\n",
            "The loss function for the iteration 3040----->38.504953652145765 :)\n",
            "The loss function for the iteration 3050----->38.492051355236306 :)\n",
            "The loss function for the iteration 3060----->38.47924298435512 :)\n",
            "The loss function for the iteration 3070----->38.46653203297713 :)\n",
            "The loss function for the iteration 3080----->38.453914995204315 :)\n",
            "The loss function for the iteration 3090----->38.441388377913356 :)\n",
            "The loss function for the iteration 3100----->38.42895247739202 :)\n",
            "The loss function for the iteration 3110----->38.41661103811376 :)\n",
            "The loss function for the iteration 3120----->38.40435811622669 :)\n",
            "The loss function for the iteration 3130----->38.392195758384375 :)\n",
            "The loss function for the iteration 3140----->38.38012407914448 :)\n",
            "The loss function for the iteration 3150----->38.368138919578264 :)\n",
            "The loss function for the iteration 3160----->38.35624844097932 :)\n",
            "The loss function for the iteration 3170----->38.344450801806204 :)\n",
            "The loss function for the iteration 3180----->38.33274058738376 :)\n",
            "The loss function for the iteration 3190----->38.32111606180237 :)\n",
            "The loss function for the iteration 3200----->38.309576335821475 :)\n",
            "The loss function for the iteration 3210----->38.29812006518783 :)\n",
            "The loss function for the iteration 3220----->38.286750040625584 :)\n",
            "The loss function for the iteration 3230----->38.27546347402154 :)\n",
            "The loss function for the iteration 3240----->38.26425958697709 :)\n",
            "The loss function for the iteration 3250----->38.253136284386436 :)\n",
            "The loss function for the iteration 3260----->38.24209639058509 :)\n",
            "The loss function for the iteration 3270----->38.231134994247405 :)\n",
            "The loss function for the iteration 3280----->38.22025603892269 :)\n",
            "The loss function for the iteration 3290----->38.20945517486597 :)\n",
            "The loss function for the iteration 3300----->38.19873330905495 :)\n",
            "The loss function for the iteration 3310----->38.18809110102807 :)\n",
            "The loss function for the iteration 3320----->38.17752380209836 :)\n",
            "The loss function for the iteration 3330----->38.167034764290065 :)\n",
            "The loss function for the iteration 3340----->38.156624876824 :)\n",
            "The loss function for the iteration 3350----->38.14628839036883 :)\n",
            "The loss function for the iteration 3360----->38.13602843507422 :)\n",
            "The loss function for the iteration 3370----->38.12584273737423 :)\n",
            "The loss function for the iteration 3380----->38.115728976515 :)\n",
            "The loss function for the iteration 3390----->38.105692736978014 :)\n",
            "The loss function for the iteration 3400----->38.09572851674314 :)\n",
            "The loss function for the iteration 3410----->38.08583359334838 :)\n",
            "The loss function for the iteration 3420----->38.07601308252698 :)\n",
            "The loss function for the iteration 3430----->38.066263160467884 :)\n",
            "The loss function for the iteration 3440----->38.05658467356857 :)\n",
            "The loss function for the iteration 3450----->38.0469785797271 :)\n",
            "The loss function for the iteration 3460----->38.03743898739635 :)\n",
            "The loss function for the iteration 3470----->38.02796823992016 :)\n",
            "The loss function for the iteration 3480----->38.0185686900691 :)\n",
            "The loss function for the iteration 3490----->38.00923567025772 :)\n",
            "The loss function for the iteration 3500----->37.99997057164837 :)\n",
            "The loss function for the iteration 3510----->37.99077189462933 :)\n",
            "The loss function for the iteration 3520----->37.981638132919976 :)\n",
            "The loss function for the iteration 3530----->37.97257454653199 :)\n",
            "The loss function for the iteration 3540----->37.96357564423671 :)\n",
            "The loss function for the iteration 3550----->37.95463710175399 :)\n",
            "The loss function for the iteration 3560----->37.94576682170997 :)\n",
            "The loss function for the iteration 3570----->37.936962442308335 :)\n",
            "The loss function for the iteration 3580----->37.92821851682841 :)\n",
            "The loss function for the iteration 3590----->37.91953741080086 :)\n",
            "The loss function for the iteration 3600----->37.91091907301592 :)\n",
            "The loss function for the iteration 3610----->37.90236293924543 :)\n",
            "The loss function for the iteration 3620----->37.89386994532389 :)\n",
            "The loss function for the iteration 3630----->37.88543675805657 :)\n",
            "The loss function for the iteration 3640----->37.877061879358 :)\n",
            "The loss function for the iteration 3650----->37.86874906481914 :)\n",
            "The loss function for the iteration 3660----->37.86049771986749 :)\n",
            "The loss function for the iteration 3670----->37.85230462393563 :)\n",
            "The loss function for the iteration 3680----->37.84416913530592 :)\n",
            "The loss function for the iteration 3690----->37.83609182769252 :)\n",
            "The loss function for the iteration 3700----->37.82807144767657 :)\n",
            "The loss function for the iteration 3710----->37.82011065058734 :)\n",
            "The loss function for the iteration 3720----->37.81220881220022 :)\n",
            "The loss function for the iteration 3730----->37.804359767604566 :)\n",
            "The loss function for the iteration 3740----->37.796564841391294 :)\n",
            "The loss function for the iteration 3750----->37.78882789497766 :)\n",
            "The loss function for the iteration 3760----->37.78114791896267 :)\n",
            "The loss function for the iteration 3770----->37.773521411674814 :)\n",
            "The loss function for the iteration 3780----->37.76594868520013 :)\n",
            "The loss function for the iteration 3790----->37.758429797562435 :)\n",
            "The loss function for the iteration 3800----->37.75096387614172 :)\n",
            "The loss function for the iteration 3810----->37.74355295589303 :)\n",
            "The loss function for the iteration 3820----->37.736194103796194 :)\n",
            "The loss function for the iteration 3830----->37.72888719416043 :)\n",
            "The loss function for the iteration 3840----->37.72163287970408 :)\n",
            "The loss function for the iteration 3850----->37.71442897931596 :)\n",
            "The loss function for the iteration 3860----->37.707275834052794 :)\n",
            "The loss function for the iteration 3870----->37.70017247807434 :)\n",
            "The loss function for the iteration 3880----->37.69312333483753 :)\n",
            "The loss function for the iteration 3890----->37.68612133858023 :)\n",
            "The loss function for the iteration 3900----->37.6791668171813 :)\n",
            "The loss function for the iteration 3910----->37.672263941550035 :)\n",
            "The loss function for the iteration 3920----->37.665410300063876 :)\n",
            "The loss function for the iteration 3930----->37.65860421737553 :)\n",
            "The loss function for the iteration 3940----->37.65184623143342 :)\n",
            "The loss function for the iteration 3950----->37.645135211259586 :)\n",
            "The loss function for the iteration 3960----->37.63847244404635 :)\n",
            "The loss function for the iteration 3970----->37.63185605086941 :)\n",
            "The loss function for the iteration 3980----->37.625286890037245 :)\n",
            "The loss function for the iteration 3990----->37.61876435919074 :)\n",
            "The loss function for the iteration 4000----->37.61228474484179 :)\n",
            "The loss function for the iteration 4010----->37.60585092769064 :)\n",
            "The loss function for the iteration 4020----->37.59946517207752 :)\n",
            "The loss function for the iteration 4030----->37.59312363161721 :)\n",
            "The loss function for the iteration 4040----->37.586824625883374 :)\n",
            "The loss function for the iteration 4050----->37.58056968607434 :)\n",
            "The loss function for the iteration 4060----->37.57435859292138 :)\n",
            "The loss function for the iteration 4070----->37.56819258039459 :)\n",
            "The loss function for the iteration 4080----->37.56206833996551 :)\n",
            "The loss function for the iteration 4090----->37.55598775545801 :)\n",
            "The loss function for the iteration 4100----->37.54994991838359 :)\n",
            "The loss function for the iteration 4110----->37.54395332973245 :)\n",
            "The loss function for the iteration 4120----->37.537996468047794 :)\n",
            "The loss function for the iteration 4130----->37.53208201510098 :)\n",
            "The loss function for the iteration 4140----->37.52621115972718 :)\n",
            "The loss function for the iteration 4150----->37.520381983551886 :)\n",
            "The loss function for the iteration 4160----->37.51458825026188 :)\n",
            "The loss function for the iteration 4170----->37.508837014596 :)\n",
            "The loss function for the iteration 4180----->37.50312430383292 :)\n",
            "The loss function for the iteration 4190----->37.497453020357106 :)\n",
            "The loss function for the iteration 4200----->37.491820817091856 :)\n",
            "The loss function for the iteration 4210----->37.486227130331564 :)\n",
            "The loss function for the iteration 4220----->37.480672805090556 :)\n",
            "The loss function for the iteration 4230----->37.47515549761694 :)\n",
            "The loss function for the iteration 4240----->37.469676673945955 :)\n",
            "The loss function for the iteration 4250----->37.46423580296159 :)\n",
            "The loss function for the iteration 4260----->37.4588325771203 :)\n",
            "The loss function for the iteration 4270----->37.45346611683364 :)\n",
            "The loss function for the iteration 4280----->37.44813748355075 :)\n",
            "The loss function for the iteration 4290----->37.44284449165359 :)\n",
            "The loss function for the iteration 4300----->37.437588039951095 :)\n",
            "The loss function for the iteration 4310----->37.432366414334304 :)\n",
            "The loss function for the iteration 4320----->37.42718159490941 :)\n",
            "The loss function for the iteration 4330----->37.42203435033703 :)\n",
            "The loss function for the iteration 4340----->37.41691885975161 :)\n",
            "The loss function for the iteration 4350----->37.411838894742964 :)\n",
            "The loss function for the iteration 4360----->37.406794192937575 :)\n",
            "The loss function for the iteration 4370----->37.40178427522833 :)\n",
            "The loss function for the iteration 4380----->37.39680873982195 :)\n",
            "The loss function for the iteration 4390----->37.39186621150294 :)\n",
            "The loss function for the iteration 4400----->37.38696310827472 :)\n",
            "The loss function for the iteration 4410----->37.38209471494294 :)\n",
            "The loss function for the iteration 4420----->37.377255434065994 :)\n",
            "The loss function for the iteration 4430----->37.37245149919298 :)\n",
            "The loss function for the iteration 4440----->37.36768018959542 :)\n",
            "The loss function for the iteration 4450----->37.36294271860899 :)\n",
            "The loss function for the iteration 4460----->37.3582362349061 :)\n",
            "The loss function for the iteration 4470----->37.353559329411254 :)\n",
            "The loss function for the iteration 4480----->37.34891613167902 :)\n",
            "The loss function for the iteration 4490----->37.34430280831997 :)\n",
            "The loss function for the iteration 4500----->37.33972117410697 :)\n",
            "The loss function for the iteration 4510----->37.33517190807471 :)\n",
            "The loss function for the iteration 4520----->37.33065365942492 :)\n",
            "The loss function for the iteration 4530----->37.326164694646344 :)\n",
            "The loss function for the iteration 4540----->37.32170499779578 :)\n",
            "The loss function for the iteration 4550----->37.3172764064195 :)\n",
            "The loss function for the iteration 4560----->37.31287590488835 :)\n",
            "The loss function for the iteration 4570----->37.30850580817895 :)\n",
            "The loss function for the iteration 4580----->37.30416574231304 :)\n",
            "The loss function for the iteration 4590----->37.299855173664014 :)\n",
            "The loss function for the iteration 4600----->37.29557545227534 :)\n",
            "The loss function for the iteration 4610----->37.29132051881718 :)\n",
            "The loss function for the iteration 4620----->37.28709348076979 :)\n",
            "The loss function for the iteration 4630----->37.282896664115 :)\n",
            "The loss function for the iteration 4640----->37.278727118503866 :)\n",
            "The loss function for the iteration 4650----->37.2745866658794 :)\n",
            "The loss function for the iteration 4660----->37.270472563272854 :)\n",
            "The loss function for the iteration 4670----->37.26638664314858 :)\n",
            "The loss function for the iteration 4680----->37.26232657145101 :)\n",
            "The loss function for the iteration 4690----->37.25829454656546 :)\n",
            "The loss function for the iteration 4700----->37.2542892349034 :)\n",
            "The loss function for the iteration 4710----->37.25030746565101 :)\n",
            "The loss function for the iteration 4720----->37.24635318429298 :)\n",
            "The loss function for the iteration 4730----->37.24243065902924 :)\n",
            "The loss function for the iteration 4740----->37.238536284763896 :)\n",
            "The loss function for the iteration 4750----->37.2346656012265 :)\n",
            "The loss function for the iteration 4760----->37.23081926016766 :)\n",
            "The loss function for the iteration 4770----->37.22700175590718 :)\n",
            "The loss function for the iteration 4780----->37.2232077351206 :)\n",
            "The loss function for the iteration 4790----->37.21943740653295 :)\n",
            "The loss function for the iteration 4800----->37.21569494108148 :)\n",
            "The loss function for the iteration 4810----->37.211975276454474 :)\n",
            "The loss function for the iteration 4820----->37.20827890788043 :)\n",
            "The loss function for the iteration 4830----->37.20460734942817 :)\n",
            "The loss function for the iteration 4840----->37.20096356661503 :)\n",
            "The loss function for the iteration 4850----->37.19734614558025 :)\n",
            "The loss function for the iteration 4860----->37.19375029437987 :)\n",
            "The loss function for the iteration 4870----->37.19017910252875 :)\n",
            "The loss function for the iteration 4880----->37.186633347237944 :)\n",
            "The loss function for the iteration 4890----->37.18310955443091 :)\n",
            "The loss function for the iteration 4900----->37.179607761990184 :)\n",
            "The loss function for the iteration 4910----->37.176128518674375 :)\n",
            "The loss function for the iteration 4920----->37.172674455518056 :)\n",
            "The loss function for the iteration 4930----->37.169238905464894 :)\n",
            "The loss function for the iteration 4940----->37.16582741959721 :)\n",
            "The loss function for the iteration 4950----->37.1624382397547 :)\n",
            "The loss function for the iteration 4960----->37.15907396455834 :)\n",
            "The loss function for the iteration 4970----->37.15572852621693 :)\n",
            "The loss function for the iteration 4980----->37.15240300225176 :)\n",
            "The loss function for the iteration 4990----->37.14910039678798 :)\n",
            "The loss function for the iteration 5000----->37.145819888475955 :)\n",
            "The loss function for the iteration 5010----->37.1425601861471 :)\n",
            "The loss function for the iteration 5020----->37.13932117300202 :)\n",
            "The loss function for the iteration 5030----->37.13610210465484 :)\n",
            "The loss function for the iteration 5040----->37.1329062092844 :)\n",
            "The loss function for the iteration 5050----->37.12972894885485 :)\n",
            "The loss function for the iteration 5060----->37.126573528819314 :)\n",
            "The loss function for the iteration 5070----->37.123435730258045 :)\n",
            "The loss function for the iteration 5080----->37.12031744404252 :)\n",
            "The loss function for the iteration 5090----->37.11722102072941 :)\n",
            "The loss function for the iteration 5100----->37.11414333925013 :)\n",
            "The loss function for the iteration 5110----->37.11108740730712 :)\n",
            "The loss function for the iteration 5120----->37.108049267163565 :)\n",
            "The loss function for the iteration 5130----->37.105031877309806 :)\n",
            "The loss function for the iteration 5140----->37.102032032774964 :)\n",
            "The loss function for the iteration 5150----->37.0990502494105 :)\n",
            "The loss function for the iteration 5160----->37.09608826087812 :)\n",
            "The loss function for the iteration 5170----->37.093144638901855 :)\n",
            "The loss function for the iteration 5180----->37.09021946200384 :)\n",
            "The loss function for the iteration 5190----->37.087311757608056 :)\n",
            "The loss function for the iteration 5200----->37.08442510490085 :)\n",
            "The loss function for the iteration 5210----->37.081555364090924 :)\n",
            "The loss function for the iteration 5220----->37.078702414846504 :)\n",
            "The loss function for the iteration 5230----->37.07586899535088 :)\n",
            "The loss function for the iteration 5240----->37.07305312236443 :)\n",
            "The loss function for the iteration 5250----->37.07025512134728 :)\n",
            "The loss function for the iteration 5260----->37.067472191250914 :)\n",
            "The loss function for the iteration 5270----->37.064708999093334 :)\n",
            "The loss function for the iteration 5280----->37.06196265887559 :)\n",
            "The loss function for the iteration 5290----->37.059230855257404 :)\n",
            "The loss function for the iteration 5300----->37.05651721579342 :)\n",
            "The loss function for the iteration 5310----->37.053821828780514 :)\n",
            "The loss function for the iteration 5320----->37.051144120078405 :)\n",
            "The loss function for the iteration 5330----->37.04847950094732 :)\n",
            "The loss function for the iteration 5340----->37.045832606025066 :)\n",
            "The loss function for the iteration 5350----->37.04320301618375 :)\n",
            "The loss function for the iteration 5360----->37.0405890752277 :)\n",
            "The loss function for the iteration 5370----->37.03799094772274 :)\n",
            "The loss function for the iteration 5380----->37.0354089494576 :)\n",
            "The loss function for the iteration 5390----->37.03284454698307 :)\n",
            "The loss function for the iteration 5400----->37.03029241715475 :)\n",
            "The loss function for the iteration 5410----->37.0277567564236 :)\n",
            "The loss function for the iteration 5420----->37.025237685686825 :)\n",
            "The loss function for the iteration 5430----->37.022733149853245 :)\n",
            "The loss function for the iteration 5440----->37.020243889447016 :)\n",
            "The loss function for the iteration 5450----->37.01776944676908 :)\n",
            "The loss function for the iteration 5460----->37.015312957064125 :)\n",
            "The loss function for the iteration 5470----->37.01286858026142 :)\n",
            "The loss function for the iteration 5480----->37.010438357997224 :)\n",
            "The loss function for the iteration 5490----->37.008024001727684 :)\n",
            "The loss function for the iteration 5500----->37.00562373861225 :)\n",
            "The loss function for the iteration 5510----->37.0032381809965 :)\n",
            "The loss function for the iteration 5520----->37.00086499189929 :)\n",
            "The loss function for the iteration 5530----->36.99850873810532 :)\n",
            "The loss function for the iteration 5540----->36.996164965974295 :)\n",
            "The loss function for the iteration 5550----->36.99383461958598 :)\n",
            "The loss function for the iteration 5560----->36.99151925800513 :)\n",
            "The loss function for the iteration 5570----->36.9892188200212 :)\n",
            "The loss function for the iteration 5580----->36.9869320467285 :)\n",
            "The loss function for the iteration 5590----->36.98465618361363 :)\n",
            "The loss function for the iteration 5600----->36.98239643377085 :)\n",
            "The loss function for the iteration 5610----->36.9801494620931 :)\n",
            "The loss function for the iteration 5620----->36.97791576158106 :)\n",
            "The loss function for the iteration 5630----->36.97569415631207 :)\n",
            "The loss function for the iteration 5640----->36.97348560540904 :)\n",
            "The loss function for the iteration 5650----->36.97129064464975 :)\n",
            "The loss function for the iteration 5660----->36.96910799307133 :)\n",
            "The loss function for the iteration 5670----->36.96694050686423 :)\n",
            "The loss function for the iteration 5680----->36.964784182166305 :)\n",
            "The loss function for the iteration 5690----->36.962640546190755 :)\n",
            "The loss function for the iteration 5700----->36.96050852239354 :)\n",
            "The loss function for the iteration 5710----->36.95838883705958 :)\n",
            "The loss function for the iteration 5720----->36.95628179187394 :)\n",
            "The loss function for the iteration 5730----->36.954186740736176 :)\n",
            "The loss function for the iteration 5740----->36.952106673066595 :)\n",
            "The loss function for the iteration 5750----->36.950035848833714 :)\n",
            "The loss function for the iteration 5760----->36.94797690728384 :)\n",
            "The loss function for the iteration 5770----->36.94592952147794 :)\n",
            "The loss function for the iteration 5780----->36.943895137929694 :)\n",
            "The loss function for the iteration 5790----->36.94187136037893 :)\n",
            "The loss function for the iteration 5800----->36.93985892836139 :)\n",
            "The loss function for the iteration 5810----->36.937860116921065 :)\n",
            "The loss function for the iteration 5820----->36.935870964725126 :)\n",
            "The loss function for the iteration 5830----->36.93389396925218 :)\n",
            "The loss function for the iteration 5840----->36.93192771705288 :)\n",
            "The loss function for the iteration 5850----->36.929973460440664 :)\n",
            "The loss function for the iteration 5860----->36.92802868432084 :)\n",
            "The loss function for the iteration 5870----->36.92609538725701 :)\n",
            "The loss function for the iteration 5880----->36.92417442157949 :)\n",
            "The loss function for the iteration 5890----->36.9222616706711 :)\n",
            "The loss function for the iteration 5900----->36.920362182582735 :)\n",
            "The loss function for the iteration 5910----->36.91847252139082 :)\n",
            "The loss function for the iteration 5920----->36.91659286816224 :)\n",
            "The loss function for the iteration 5930----->36.91472328502964 :)\n",
            "The loss function for the iteration 5940----->36.912866365818026 :)\n",
            "The loss function for the iteration 5950----->36.91101768319175 :)\n",
            "The loss function for the iteration 5960----->36.90917849159804 :)\n",
            "The loss function for the iteration 5970----->36.907352229668774 :)\n",
            "The loss function for the iteration 5980----->36.905534918016464 :)\n",
            "The loss function for the iteration 5990----->36.903728947800374 :)\n",
            "The loss function for the iteration 6000----->36.90193120025393 :)\n",
            "The loss function for the iteration 6010----->36.90014536468332 :)\n",
            "The loss function for the iteration 6020----->36.898368470788874 :)\n",
            "The loss function for the iteration 6030----->36.8965996180071 :)\n",
            "The loss function for the iteration 6040----->36.89484136868462 :)\n",
            "The loss function for the iteration 6050----->36.8930931931967 :)\n",
            "The loss function for the iteration 6060----->36.89135376840823 :)\n",
            "The loss function for the iteration 6070----->36.889623626672105 :)\n",
            "The loss function for the iteration 6080----->36.88790374449776 :)\n",
            "The loss function for the iteration 6090----->36.88619243109767 :)\n",
            "The loss function for the iteration 6100----->36.88449294802376 :)\n",
            "The loss function for the iteration 6110----->36.882798970383156 :)\n",
            "The loss function for the iteration 6120----->36.88111619007436 :)\n",
            "The loss function for the iteration 6130----->36.87944338210154 :)\n",
            "The loss function for the iteration 6140----->36.877777300563956 :)\n",
            "The loss function for the iteration 6150----->36.87612174188969 :)\n",
            "The loss function for the iteration 6160----->36.874472956628445 :)\n",
            "The loss function for the iteration 6170----->36.87283710105062 :)\n",
            "The loss function for the iteration 6180----->36.87120560271896 :)\n",
            "The loss function for the iteration 6190----->36.86958450197102 :)\n",
            "The loss function for the iteration 6200----->36.86797243394014 :)\n",
            "The loss function for the iteration 6210----->36.866368323088075 :)\n",
            "The loss function for the iteration 6220----->36.864771135331104 :)\n",
            "The loss function for the iteration 6230----->36.86318458408966 :)\n",
            "The loss function for the iteration 6240----->36.861606932599635 :)\n",
            "The loss function for the iteration 6250----->36.8600332264802 :)\n",
            "The loss function for the iteration 6260----->36.85847396184869 :)\n",
            "The loss function for the iteration 6270----->36.856918037431896 :)\n",
            "The loss function for the iteration 6280----->36.85537385995556 :)\n",
            "The loss function for the iteration 6290----->36.85383447299494 :)\n",
            "The loss function for the iteration 6300----->36.852304515712625 :)\n",
            "The loss function for the iteration 6310----->36.85078504372679 :)\n",
            "The loss function for the iteration 6320----->36.84926779832627 :)\n",
            "The loss function for the iteration 6330----->36.847761490396344 :)\n",
            "The loss function for the iteration 6340----->36.84626276447014 :)\n",
            "The loss function for the iteration 6350----->36.8447719455196 :)\n",
            "The loss function for the iteration 6360----->36.84328619641035 :)\n",
            "The loss function for the iteration 6370----->36.84181135890508 :)\n",
            "The loss function for the iteration 6380----->36.840344238975945 :)\n",
            "The loss function for the iteration 6390----->36.838882212277454 :)\n",
            "The loss function for the iteration 6400----->36.837428445099675 :)\n",
            "The loss function for the iteration 6410----->36.83598456772179 :)\n",
            "The loss function for the iteration 6420----->36.83454459143919 :)\n",
            "The loss function for the iteration 6430----->36.83311197375965 :)\n",
            "The loss function for the iteration 6440----->36.83168799911724 :)\n",
            "The loss function for the iteration 6450----->36.83027095031988 :)\n",
            "The loss function for the iteration 6460----->36.82886000183373 :)\n",
            "The loss function for the iteration 6470----->36.827454798748484 :)\n",
            "The loss function for the iteration 6480----->36.82606046213669 :)\n",
            "The loss function for the iteration 6490----->36.82467070447099 :)\n",
            "The loss function for the iteration 6500----->36.82328749892448 :)\n",
            "The loss function for the iteration 6510----->36.82191312429425 :)\n",
            "The loss function for the iteration 6520----->36.820545178658314 :)\n",
            "The loss function for the iteration 6530----->36.81918338620113 :)\n",
            "The loss function for the iteration 6540----->36.81782710296273 :)\n",
            "The loss function for the iteration 6550----->36.81648047452432 :)\n",
            "The loss function for the iteration 6560----->36.81513739099411 :)\n",
            "The loss function for the iteration 6570----->36.81380158587983 :)\n",
            "The loss function for the iteration 6580----->36.812471718582 :)\n",
            "The loss function for the iteration 6590----->36.8111513044416 :)\n",
            "The loss function for the iteration 6600----->36.809832009853885 :)\n",
            "The loss function for the iteration 6610----->36.808523394316424 :)\n",
            "The loss function for the iteration 6620----->36.80721889534127 :)\n",
            "The loss function for the iteration 6630----->36.80592388399467 :)\n",
            "The loss function for the iteration 6640----->36.80462907634702 :)\n",
            "The loss function for the iteration 6650----->36.80334722307298 :)\n",
            "The loss function for the iteration 6660----->36.80206789561101 :)\n",
            "The loss function for the iteration 6670----->36.80079518598918 :)\n",
            "The loss function for the iteration 6680----->36.79952750478915 :)\n",
            "The loss function for the iteration 6690----->36.798268359126666 :)\n",
            "The loss function for the iteration 6700----->36.79701345102427 :)\n",
            "The loss function for the iteration 6710----->36.79576358983458 :)\n",
            "The loss function for the iteration 6720----->36.794521311764 :)\n",
            "The loss function for the iteration 6730----->36.79328597565286 :)\n",
            "The loss function for the iteration 6740----->36.79205257387403 :)\n",
            "The loss function for the iteration 6750----->36.790828122624625 :)\n",
            "The loss function for the iteration 6760----->36.78960766565966 :)\n",
            "The loss function for the iteration 6770----->36.78839434164813 :)\n",
            "The loss function for the iteration 6780----->36.78718345847416 :)\n",
            "The loss function for the iteration 6790----->36.785981674057176 :)\n",
            "The loss function for the iteration 6800----->36.784784083645235 :)\n",
            "The loss function for the iteration 6810----->36.78359055270679 :)\n",
            "The loss function for the iteration 6820----->36.78240277295952 :)\n",
            "The loss function for the iteration 6830----->36.7812237044037 :)\n",
            "The loss function for the iteration 6840----->36.780046151889984 :)\n",
            "The loss function for the iteration 6850----->36.778875798713614 :)\n",
            "The loss function for the iteration 6860----->36.77771171651288 :)\n",
            "The loss function for the iteration 6870----->36.77655188018043 :)\n",
            "The loss function for the iteration 6880----->36.77539871283137 :)\n",
            "The loss function for the iteration 6890----->36.77424703485113 :)\n",
            "The loss function for the iteration 6900----->36.77310678857948 :)\n",
            "The loss function for the iteration 6910----->36.771965543582155 :)\n",
            "The loss function for the iteration 6920----->36.77083081329225 :)\n",
            "The loss function for the iteration 6930----->36.76970257701497 :)\n",
            "The loss function for the iteration 6940----->36.768578367712756 :)\n",
            "The loss function for the iteration 6950----->36.76745768029278 :)\n",
            "The loss function for the iteration 6960----->36.76634214435014 :)\n",
            "The loss function for the iteration 6970----->36.76523428875428 :)\n",
            "The loss function for the iteration 6980----->36.76412798453688 :)\n",
            "The loss function for the iteration 6990----->36.763027919402454 :)\n",
            "The loss function for the iteration 7000----->36.761934672911934 :)\n",
            "The loss function for the iteration 7010----->36.760844155254965 :)\n",
            "The loss function for the iteration 7020----->36.75975845081533 :)\n",
            "The loss function for the iteration 7030----->36.75867846616977 :)\n",
            "The loss function for the iteration 7040----->36.757601867741315 :)\n",
            "The loss function for the iteration 7050----->36.756531967122825 :)\n",
            "The loss function for the iteration 7060----->36.75546308708521 :)\n",
            "The loss function for the iteration 7070----->36.75440238842312 :)\n",
            "The loss function for the iteration 7080----->36.75334591338585 :)\n",
            "The loss function for the iteration 7090----->36.752291191829784 :)\n",
            "The loss function for the iteration 7100----->36.75124296008698 :)\n",
            "The loss function for the iteration 7110----->36.75019957684195 :)\n",
            "The loss function for the iteration 7120----->36.74915871927835 :)\n",
            "The loss function for the iteration 7130----->36.74812126813697 :)\n",
            "The loss function for the iteration 7140----->36.74709176280995 :)\n",
            "The loss function for the iteration 7150----->36.746063562634454 :)\n",
            "The loss function for the iteration 7160----->36.745039232649276 :)\n",
            "The loss function for the iteration 7170----->36.74402097613443 :)\n",
            "The loss function for the iteration 7180----->36.743008857926654 :)\n",
            "The loss function for the iteration 7190----->36.74199670388016 :)\n",
            "The loss function for the iteration 7200----->36.740991955970436 :)\n",
            "The loss function for the iteration 7210----->36.739991310062585 :)\n",
            "The loss function for the iteration 7220----->36.73899345413242 :)\n",
            "The loss function for the iteration 7230----->36.73799887518005 :)\n",
            "The loss function for the iteration 7240----->36.73700962781229 :)\n",
            "The loss function for the iteration 7250----->36.73602578313307 :)\n",
            "The loss function for the iteration 7260----->36.73504330953988 :)\n",
            "The loss function for the iteration 7270----->36.73406501597829 :)\n",
            "The loss function for the iteration 7280----->36.73309282468464 :)\n",
            "The loss function for the iteration 7290----->36.73212409139118 :)\n",
            "The loss function for the iteration 7300----->36.731153989983284 :)\n",
            "The loss function for the iteration 7310----->36.73019392506382 :)\n",
            "The loss function for the iteration 7320----->36.72923637320288 :)\n",
            "The loss function for the iteration 7330----->36.72827914969701 :)\n",
            "The loss function for the iteration 7340----->36.727328210478085 :)\n",
            "The loss function for the iteration 7350----->36.72638447582434 :)\n",
            "The loss function for the iteration 7360----->36.72543916705086 :)\n",
            "The loss function for the iteration 7370----->36.724499281587164 :)\n",
            "The loss function for the iteration 7380----->36.72356564030805 :)\n",
            "The loss function for the iteration 7390----->36.72263230184092 :)\n",
            "The loss function for the iteration 7400----->36.72170286033916 :)\n",
            "The loss function for the iteration 7410----->36.720779124357726 :)\n",
            "The loss function for the iteration 7420----->36.719858968823345 :)\n",
            "The loss function for the iteration 7430----->36.718939124425724 :)\n",
            "The loss function for the iteration 7440----->36.7180271206109 :)\n",
            "The loss function for the iteration 7450----->36.717116786000815 :)\n",
            "The loss function for the iteration 7460----->36.71620778436419 :)\n",
            "The loss function for the iteration 7470----->36.71530215060811 :)\n",
            "The loss function for the iteration 7480----->36.714403511631204 :)\n",
            "The loss function for the iteration 7490----->36.713505755692275 :)\n",
            "The loss function for the iteration 7500----->36.71261035240173 :)\n",
            "The loss function for the iteration 7510----->36.71172035831259 :)\n",
            "The loss function for the iteration 7520----->36.71083464426405 :)\n",
            "The loss function for the iteration 7530----->36.70994927821326 :)\n",
            "The loss function for the iteration 7540----->36.70906664450639 :)\n",
            "The loss function for the iteration 7550----->36.70818982864639 :)\n",
            "The loss function for the iteration 7560----->36.70731596200765 :)\n",
            "The loss function for the iteration 7570----->36.70644512779783 :)\n",
            "The loss function for the iteration 7580----->36.70557713739401 :)\n",
            "The loss function for the iteration 7590----->36.70471381322897 :)\n",
            "The loss function for the iteration 7600----->36.70385121473508 :)\n",
            "The loss function for the iteration 7610----->36.70299397331684 :)\n",
            "The loss function for the iteration 7620----->36.70213847385147 :)\n",
            "The loss function for the iteration 7630----->36.701286594638006 :)\n",
            "The loss function for the iteration 7640----->36.70043713759459 :)\n",
            "The loss function for the iteration 7650----->36.69959350002799 :)\n",
            "The loss function for the iteration 7660----->36.698750318718865 :)\n",
            "The loss function for the iteration 7670----->36.69790959864698 :)\n",
            "The loss function for the iteration 7680----->36.69707292101715 :)\n",
            "The loss function for the iteration 7690----->36.69623895955848 :)\n",
            "The loss function for the iteration 7700----->36.69540695781394 :)\n",
            "The loss function for the iteration 7710----->36.6945792718646 :)\n",
            "The loss function for the iteration 7720----->36.69375645553588 :)\n",
            "The loss function for the iteration 7730----->36.69293202377355 :)\n",
            "The loss function for the iteration 7740----->36.69211058349121 :)\n",
            "The loss function for the iteration 7750----->36.69129383237182 :)\n",
            "The loss function for the iteration 7760----->36.690482286551024 :)\n",
            "The loss function for the iteration 7770----->36.689669261062704 :)\n",
            "The loss function for the iteration 7780----->36.688860576529095 :)\n",
            "The loss function for the iteration 7790----->36.68805502011135 :)\n",
            "The loss function for the iteration 7800----->36.68725310204443 :)\n",
            "The loss function for the iteration 7810----->36.686451959660985 :)\n",
            "The loss function for the iteration 7820----->36.68565514000383 :)\n",
            "The loss function for the iteration 7830----->36.684860128486285 :)\n",
            "The loss function for the iteration 7840----->36.68406899395692 :)\n",
            "The loss function for the iteration 7850----->36.68327951268599 :)\n",
            "The loss function for the iteration 7860----->36.68249266523771 :)\n",
            "The loss function for the iteration 7870----->36.68170782250014 :)\n",
            "The loss function for the iteration 7880----->36.68092680918902 :)\n",
            "The loss function for the iteration 7890----->36.68015002993177 :)\n",
            "The loss function for the iteration 7900----->36.67937344282472 :)\n",
            "The loss function for the iteration 7910----->36.67859908356511 :)\n",
            "The loss function for the iteration 7920----->36.67782736732482 :)\n",
            "The loss function for the iteration 7930----->36.67705956823785 :)\n",
            "The loss function for the iteration 7940----->36.676292670230026 :)\n",
            "The loss function for the iteration 7950----->36.675530236738254 :)\n",
            "The loss function for the iteration 7960----->36.6747684394057 :)\n",
            "The loss function for the iteration 7970----->36.6740098338282 :)\n",
            "The loss function for the iteration 7980----->36.673252027184354 :)\n",
            "The loss function for the iteration 7990----->36.672498750501575 :)\n",
            "The loss function for the iteration 8000----->36.671747015842975 :)\n",
            "The loss function for the iteration 8010----->36.67099924836425 :)\n",
            "The loss function for the iteration 8020----->36.67025228313984 :)\n",
            "The loss function for the iteration 8030----->36.66950688164211 :)\n",
            "The loss function for the iteration 8040----->36.668763926509484 :)\n",
            "The loss function for the iteration 8050----->36.66802437562248 :)\n",
            "The loss function for the iteration 8060----->36.66728826517408 :)\n",
            "The loss function for the iteration 8070----->36.666553572694696 :)\n",
            "The loss function for the iteration 8080----->36.66582097966511 :)\n",
            "The loss function for the iteration 8090----->36.66508898674311 :)\n",
            "The loss function for the iteration 8100----->36.66436049108103 :)\n",
            "The loss function for the iteration 8110----->36.66363481973215 :)\n",
            "The loss function for the iteration 8120----->36.6629122959659 :)\n",
            "The loss function for the iteration 8130----->36.662189662566796 :)\n",
            "The loss function for the iteration 8140----->36.66147064006787 :)\n",
            "The loss function for the iteration 8150----->36.66075207485596 :)\n",
            "The loss function for the iteration 8160----->36.660038198570206 :)\n",
            "The loss function for the iteration 8170----->36.65932542887159 :)\n",
            "The loss function for the iteration 8180----->36.658616114569774 :)\n",
            "The loss function for the iteration 8190----->36.657905067607075 :)\n",
            "The loss function for the iteration 8200----->36.65719759347761 :)\n",
            "The loss function for the iteration 8210----->36.656493022943664 :)\n",
            "The loss function for the iteration 8220----->36.655791992225886 :)\n",
            "The loss function for the iteration 8230----->36.6550916981273 :)\n",
            "The loss function for the iteration 8240----->36.6543944591338 :)\n",
            "The loss function for the iteration 8250----->36.653697317754116 :)\n",
            "The loss function for the iteration 8260----->36.65300122525552 :)\n",
            "The loss function for the iteration 8270----->36.652309144373326 :)\n",
            "The loss function for the iteration 8280----->36.65162020210228 :)\n",
            "The loss function for the iteration 8290----->36.65093250301767 :)\n",
            "The loss function for the iteration 8300----->36.650245116710146 :)\n",
            "The loss function for the iteration 8310----->36.64956191915683 :)\n",
            "The loss function for the iteration 8320----->36.64887876436291 :)\n",
            "The loss function for the iteration 8330----->36.648198575093 :)\n",
            "The loss function for the iteration 8340----->36.64752051352744 :)\n",
            "The loss function for the iteration 8350----->36.64684537140937 :)\n",
            "The loss function for the iteration 8360----->36.646168522566946 :)\n",
            "The loss function for the iteration 8370----->36.64549660678263 :)\n",
            "The loss function for the iteration 8380----->36.64482612501721 :)\n",
            "The loss function for the iteration 8390----->36.64415789114392 :)\n",
            "The loss function for the iteration 8400----->36.64348922195774 :)\n",
            "The loss function for the iteration 8410----->36.642825687047406 :)\n",
            "The loss function for the iteration 8420----->36.642161555038456 :)\n",
            "The loss function for the iteration 8430----->36.64149982245815 :)\n",
            "The loss function for the iteration 8440----->36.640840676341604 :)\n",
            "The loss function for the iteration 8450----->36.640183666103745 :)\n",
            "The loss function for the iteration 8460----->36.63952562378541 :)\n",
            "The loss function for the iteration 8470----->36.63887119887538 :)\n",
            "The loss function for the iteration 8480----->36.638219677170156 :)\n",
            "The loss function for the iteration 8490----->36.63756800998832 :)\n",
            "The loss function for the iteration 8500----->36.636917559633375 :)\n",
            "The loss function for the iteration 8510----->36.63627077477326 :)\n",
            "The loss function for the iteration 8520----->36.63562552807097 :)\n",
            "The loss function for the iteration 8530----->36.6349809198557 :)\n",
            "The loss function for the iteration 8540----->36.63434056590426 :)\n",
            "The loss function for the iteration 8550----->36.63370006995385 :)\n",
            "The loss function for the iteration 8560----->36.63305950519291 :)\n",
            "The loss function for the iteration 8570----->36.632420999004054 :)\n",
            "The loss function for the iteration 8580----->36.63178673190246 :)\n",
            "The loss function for the iteration 8590----->36.63115117340589 :)\n",
            "The loss function for the iteration 8600----->36.630518764772305 :)\n",
            "The loss function for the iteration 8610----->36.629888081685735 :)\n",
            "The loss function for the iteration 8620----->36.629260519625646 :)\n",
            "The loss function for the iteration 8630----->36.62863179342128 :)\n",
            "The loss function for the iteration 8640----->36.628007059427716 :)\n",
            "The loss function for the iteration 8650----->36.627381589805125 :)\n",
            "The loss function for the iteration 8660----->36.6267580241298 :)\n",
            "The loss function for the iteration 8670----->36.62613615567036 :)\n",
            "The loss function for the iteration 8680----->36.6255186340709 :)\n",
            "The loss function for the iteration 8690----->36.62489984351433 :)\n",
            "The loss function for the iteration 8700----->36.624283596965384 :)\n",
            "The loss function for the iteration 8710----->36.62366945424048 :)\n",
            "The loss function for the iteration 8720----->36.62305688107732 :)\n",
            "The loss function for the iteration 8730----->36.62244443467684 :)\n",
            "The loss function for the iteration 8740----->36.621833778540946 :)\n",
            "The loss function for the iteration 8750----->36.62122396800821 :)\n",
            "The loss function for the iteration 8760----->36.62061610234076 :)\n",
            "The loss function for the iteration 8770----->36.62001040591274 :)\n",
            "The loss function for the iteration 8780----->36.61940617347867 :)\n",
            "The loss function for the iteration 8790----->36.6188026379962 :)\n",
            "The loss function for the iteration 8800----->36.618200831367446 :)\n",
            "The loss function for the iteration 8810----->36.6176018191735 :)\n",
            "The loss function for the iteration 8820----->36.6170032129598 :)\n",
            "The loss function for the iteration 8830----->36.6164061492986 :)\n",
            "The loss function for the iteration 8840----->36.61580946758154 :)\n",
            "The loss function for the iteration 8850----->36.615215810583486 :)\n",
            "The loss function for the iteration 8860----->36.614623125745425 :)\n",
            "The loss function for the iteration 8870----->36.61403249434895 :)\n",
            "The loss function for the iteration 8880----->36.61344105046444 :)\n",
            "The loss function for the iteration 8890----->36.612853013204536 :)\n",
            "The loss function for the iteration 8900----->36.61226548297124 :)\n",
            "The loss function for the iteration 8910----->36.6116802281279 :)\n",
            "The loss function for the iteration 8920----->36.61109393240225 :)\n",
            "The loss function for the iteration 8930----->36.61051152063704 :)\n",
            "The loss function for the iteration 8940----->36.609928326711334 :)\n",
            "The loss function for the iteration 8950----->36.60934839788267 :)\n",
            "The loss function for the iteration 8960----->36.6087692543256 :)\n",
            "The loss function for the iteration 8970----->36.608191576301785 :)\n",
            "The loss function for the iteration 8980----->36.60761326601999 :)\n",
            "The loss function for the iteration 8990----->36.607039446197106 :)\n",
            "The loss function for the iteration 9000----->36.60646554682969 :)\n",
            "The loss function for the iteration 9010----->36.60589155223318 :)\n",
            "The loss function for the iteration 9020----->36.60531889486645 :)\n",
            "The loss function for the iteration 9030----->36.60474961425957 :)\n",
            "The loss function for the iteration 9040----->36.60417950034538 :)\n",
            "The loss function for the iteration 9050----->36.6036108530662 :)\n",
            "The loss function for the iteration 9060----->36.60304433003899 :)\n",
            "The loss function for the iteration 9070----->36.60247836231956 :)\n",
            "The loss function for the iteration 9080----->36.601913475906635 :)\n",
            "The loss function for the iteration 9090----->36.60135186947134 :)\n",
            "The loss function for the iteration 9100----->36.600790437257636 :)\n",
            "The loss function for the iteration 9110----->36.600228267219194 :)\n",
            "The loss function for the iteration 9120----->36.59967009612216 :)\n",
            "The loss function for the iteration 9130----->36.599113462101634 :)\n",
            "The loss function for the iteration 9140----->36.5985555113571 :)\n",
            "The loss function for the iteration 9150----->36.597998410540214 :)\n",
            "The loss function for the iteration 9160----->36.59744548042334 :)\n",
            "The loss function for the iteration 9170----->36.59689137904487 :)\n",
            "The loss function for the iteration 9180----->36.596338122302605 :)\n",
            "The loss function for the iteration 9190----->36.595786730610385 :)\n",
            "The loss function for the iteration 9200----->36.59523755484766 :)\n",
            "The loss function for the iteration 9210----->36.594687112888266 :)\n",
            "The loss function for the iteration 9220----->36.594141052170556 :)\n",
            "The loss function for the iteration 9230----->36.59359425773623 :)\n",
            "The loss function for the iteration 9240----->36.59304862274041 :)\n",
            "The loss function for the iteration 9250----->36.592503982519595 :)\n",
            "The loss function for the iteration 9260----->36.59196351241568 :)\n",
            "The loss function for the iteration 9270----->36.591419816916485 :)\n",
            "The loss function for the iteration 9280----->36.590878989747445 :)\n",
            "The loss function for the iteration 9290----->36.59033996506832 :)\n",
            "The loss function for the iteration 9300----->36.58980285939546 :)\n",
            "The loss function for the iteration 9310----->36.58926343467141 :)\n",
            "The loss function for the iteration 9320----->36.588727671827485 :)\n",
            "The loss function for the iteration 9330----->36.58819223980273 :)\n",
            "The loss function for the iteration 9340----->36.58765849847305 :)\n",
            "The loss function for the iteration 9350----->36.58712445922273 :)\n",
            "The loss function for the iteration 9360----->36.58659288723873 :)\n",
            "The loss function for the iteration 9370----->36.58606103486914 :)\n",
            "The loss function for the iteration 9380----->36.58553175160362 :)\n",
            "The loss function for the iteration 9390----->36.585002807171605 :)\n",
            "The loss function for the iteration 9400----->36.58447470776376 :)\n",
            "The loss function for the iteration 9410----->36.5839473447107 :)\n",
            "The loss function for the iteration 9420----->36.58342218647995 :)\n",
            "The loss function for the iteration 9430----->36.58289686766575 :)\n",
            "The loss function for the iteration 9440----->36.58237268589157 :)\n",
            "The loss function for the iteration 9450----->36.581849241531415 :)\n",
            "The loss function for the iteration 9460----->36.581327564328014 :)\n",
            "The loss function for the iteration 9470----->36.58080702162809 :)\n",
            "The loss function for the iteration 9480----->36.58028651676087 :)\n",
            "The loss function for the iteration 9490----->36.57976663050131 :)\n",
            "The loss function for the iteration 9500----->36.57924882908627 :)\n",
            "The loss function for the iteration 9510----->36.5787324717953 :)\n",
            "The loss function for the iteration 9520----->36.57821481663218 :)\n",
            "The loss function for the iteration 9530----->36.57769896799518 :)\n",
            "The loss function for the iteration 9540----->36.57718564999803 :)\n",
            "The loss function for the iteration 9550----->36.576672488940815 :)\n",
            "The loss function for the iteration 9560----->36.57615844440118 :)\n",
            "The loss function for the iteration 9570----->36.57564753502169 :)\n",
            "The loss function for the iteration 9580----->36.57513712880979 :)\n",
            "The loss function for the iteration 9590----->36.57462685334973 :)\n",
            "The loss function for the iteration 9600----->36.574117569890944 :)\n",
            "The loss function for the iteration 9610----->36.57361035360479 :)\n",
            "The loss function for the iteration 9620----->36.573102212941386 :)\n",
            "The loss function for the iteration 9630----->36.572597398816555 :)\n",
            "The loss function for the iteration 9640----->36.57209271223182 :)\n",
            "The loss function for the iteration 9650----->36.57158811680818 :)\n",
            "The loss function for the iteration 9660----->36.57108425960498 :)\n",
            "The loss function for the iteration 9670----->36.570584517638544 :)\n",
            "The loss function for the iteration 9680----->36.57008204047591 :)\n",
            "The loss function for the iteration 9690----->36.5695807618229 :)\n",
            "The loss function for the iteration 9700----->36.56908202191572 :)\n",
            "The loss function for the iteration 9710----->36.5685854202949 :)\n",
            "The loss function for the iteration 9720----->36.56808512182005 :)\n",
            "The loss function for the iteration 9730----->36.56758886763037 :)\n",
            "The loss function for the iteration 9740----->36.5670934462855 :)\n",
            "The loss function for the iteration 9750----->36.566598802519835 :)\n",
            "The loss function for the iteration 9760----->36.566103041400645 :)\n",
            "The loss function for the iteration 9770----->36.56561123236763 :)\n",
            "The loss function for the iteration 9780----->36.56511784804931 :)\n",
            "The loss function for the iteration 9790----->36.56462690357129 :)\n",
            "The loss function for the iteration 9800----->36.56413565116321 :)\n",
            "The loss function for the iteration 9810----->36.56364602576869 :)\n",
            "The loss function for the iteration 9820----->36.563155718606616 :)\n",
            "The loss function for the iteration 9830----->36.56266880696599 :)\n",
            "The loss function for the iteration 9840----->36.562180206059935 :)\n",
            "The loss function for the iteration 9850----->36.56169269272719 :)\n",
            "The loss function for the iteration 9860----->36.561206435294174 :)\n",
            "The loss function for the iteration 9870----->36.560722784164795 :)\n",
            "The loss function for the iteration 9880----->36.56023700206273 :)\n",
            "The loss function for the iteration 9890----->36.55975345158307 :)\n",
            "The loss function for the iteration 9900----->36.55927079935362 :)\n",
            "The loss function for the iteration 9910----->36.558789507983136 :)\n",
            "The loss function for the iteration 9920----->36.55830730630704 :)\n",
            "The loss function for the iteration 9930----->36.55782691159323 :)\n",
            "The loss function for the iteration 9940----->36.55734720430548 :)\n",
            "The loss function for the iteration 9950----->36.556869133647396 :)\n",
            "The loss function for the iteration 9960----->36.55639000162784 :)\n",
            "The loss function for the iteration 9970----->36.55591254341335 :)\n",
            "The loss function for the iteration 9980----->36.55543688937423 :)\n",
            "The loss function for the iteration 9990----->36.55496177011535 :)\n",
            "The loss function for the iteration 10000----->36.55448552139856 :)\n"
          ]
        }
      ],
      "source": [
        "lasso_regression_custom = LassoRegression(0.01, 1e-2, 10000)\n",
        "y_train_reshaped = y_train.reshape(-1, 1)\n",
        "lasso_regression_custom.train(X_train, y_train_reshaped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxeSokECU_PW"
      },
      "source": [
        "# Lasso Regression using skicit-learn (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjkaQEKlU_PW"
      },
      "source": [
        "Use `sklearn` to train a Lasso Regression Model. To determine the best regularization coefficients, use grid-search (or other techniques you've learned till now)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:35:17.375634Z",
          "iopub.status.busy": "2023-04-30T08:35:17.374562Z",
          "iopub.status.idle": "2023-04-30T08:35:17.403838Z",
          "shell.execute_reply": "2023-04-30T08:35:17.401840Z",
          "shell.execute_reply.started": "2023-04-30T08:35:17.375589Z"
        },
        "id": "Q1fX_s6aU_PW",
        "outputId": "3615bf86-92a6-4088-e090-7293f4a8e750",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.818e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.705e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.791e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.766e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.682e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.784e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.787e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.769e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.798e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.746e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.778e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.870e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.846e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.763e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.854e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.852e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.840e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.867e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.824e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.965e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.842e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.945e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.925e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.844e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.919e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.911e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.909e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.929e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.894e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.016e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.012e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.913e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.977e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.970e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.972e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.955e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.062e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.950e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.069e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.051e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.970e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.033e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.029e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.038e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.014e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.098e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.121e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.100e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.020e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.086e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.091e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.086e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.093e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.070e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.090e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.158e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.056e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.130e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.152e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.134e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.124e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.117e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.127e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.910e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.142e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.160e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.069e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.198e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.163e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.039e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.136e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.188e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.873e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.087e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.196e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.112e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.494e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.205e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.134e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.154e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.126e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.219e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.595e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.147e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.224e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.139e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.869e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.866e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.750e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.205e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.150e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.232e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.462e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.187e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.235e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.143e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.089e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.548e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.812e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.232e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.159e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.205e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.395e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.187e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.212e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.138e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.083e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.089e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.244e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.158e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.157e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.513e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.157e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.180e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.036e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.889e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.234e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.146e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.095e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.607e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.088e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.079e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.059e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.864e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.021e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.192e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.119e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.013e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.446e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+04, tolerance: 7.470e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.964e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.987e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.709e+04, tolerance: 7.545e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.839e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.121e+04, tolerance: 7.493e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.043e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.847e+04, tolerance: 7.531e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.070e+04, tolerance: 7.466e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.742e+04, tolerance: 7.454e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.835e+04, tolerance: 7.345e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.440e+04, tolerance: 7.459e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.645e+04, tolerance: 7.522e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+04, tolerance: 7.364e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, estimator=Lasso(max_iter=10000),\n",
              "             param_grid={'alpha': array([1.00000000e-08, 1.45082878e-08, 2.10490414e-08, 3.05385551e-08,\n",
              "       4.43062146e-08, 6.42807312e-08, 9.32603347e-08, 1.35304777e-07,\n",
              "       1.96304065e-07, 2.84803587e-07, 4.13201240e-07, 5.99484250e-07,\n",
              "       8.69749003e-07, 1.26185688e-06, 1.83073828e-06, 2.65608778e-06,\n",
              "       3.85352859e-06, 5.59081018e-06, 8....\n",
              "       1.91791026e+04, 2.78255940e+04, 4.03701726e+04, 5.85702082e+04,\n",
              "       8.49753436e+04, 1.23284674e+05, 1.78864953e+05, 2.59502421e+05,\n",
              "       3.76493581e+05, 5.46227722e+05, 7.92482898e+05, 1.14975700e+06,\n",
              "       1.66810054e+06, 2.42012826e+06, 3.51119173e+06, 5.09413801e+06,\n",
              "       7.39072203e+06, 1.07226722e+07, 1.55567614e+07, 2.25701972e+07,\n",
              "       3.27454916e+07, 4.75081016e+07, 6.89261210e+07, 1.00000000e+08])})"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=Lasso(max_iter=10000),\n",
              "             param_grid={&#x27;alpha&#x27;: array([1.00000000e-08, 1.45082878e-08, 2.10490414e-08, 3.05385551e-08,\n",
              "       4.43062146e-08, 6.42807312e-08, 9.32603347e-08, 1.35304777e-07,\n",
              "       1.96304065e-07, 2.84803587e-07, 4.13201240e-07, 5.99484250e-07,\n",
              "       8.69749003e-07, 1.26185688e-06, 1.83073828e-06, 2.65608778e-06,\n",
              "       3.85352859e-06, 5.59081018e-06, 8....\n",
              "       1.91791026e+04, 2.78255940e+04, 4.03701726e+04, 5.85702082e+04,\n",
              "       8.49753436e+04, 1.23284674e+05, 1.78864953e+05, 2.59502421e+05,\n",
              "       3.76493581e+05, 5.46227722e+05, 7.92482898e+05, 1.14975700e+06,\n",
              "       1.66810054e+06, 2.42012826e+06, 3.51119173e+06, 5.09413801e+06,\n",
              "       7.39072203e+06, 1.07226722e+07, 1.55567614e+07, 2.25701972e+07,\n",
              "       3.27454916e+07, 4.75081016e+07, 6.89261210e+07, 1.00000000e+08])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10, estimator=Lasso(max_iter=10000),\n",
              "             param_grid={&#x27;alpha&#x27;: array([1.00000000e-08, 1.45082878e-08, 2.10490414e-08, 3.05385551e-08,\n",
              "       4.43062146e-08, 6.42807312e-08, 9.32603347e-08, 1.35304777e-07,\n",
              "       1.96304065e-07, 2.84803587e-07, 4.13201240e-07, 5.99484250e-07,\n",
              "       8.69749003e-07, 1.26185688e-06, 1.83073828e-06, 2.65608778e-06,\n",
              "       3.85352859e-06, 5.59081018e-06, 8....\n",
              "       1.91791026e+04, 2.78255940e+04, 4.03701726e+04, 5.85702082e+04,\n",
              "       8.49753436e+04, 1.23284674e+05, 1.78864953e+05, 2.59502421e+05,\n",
              "       3.76493581e+05, 5.46227722e+05, 7.92482898e+05, 1.14975700e+06,\n",
              "       1.66810054e+06, 2.42012826e+06, 3.51119173e+06, 5.09413801e+06,\n",
              "       7.39072203e+06, 1.07226722e+07, 1.55567614e+07, 2.25701972e+07,\n",
              "       3.27454916e+07, 4.75081016e+07, 6.89261210e+07, 1.00000000e+08])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso(max_iter=10000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso(max_iter=10000)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "params = {'alpha': (np.logspace(-8, 8, 100))}\n",
        "lasso_regression_sklearn = Lasso(max_iter=10000)\n",
        "lasso_model = GridSearchCV(lasso_regression_sklearn, params, cv = 10)\n",
        "lasso_model.fit(X_train, y_train_reshaped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vHBMK9pU_PW"
      },
      "source": [
        "# Ridge Regression From scratch (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkmTdCqlU_PW"
      },
      "source": [
        "Train a ridge regression model using your own code and the following class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-30T08:35:21.763364Z",
          "iopub.status.busy": "2023-04-30T08:35:21.762634Z",
          "iopub.status.idle": "2023-04-30T08:35:21.769134Z",
          "shell.execute_reply": "2023-04-30T08:35:21.768198Z",
          "shell.execute_reply.started": "2023-04-30T08:35:21.763321Z"
        },
        "id": "ueIbohzQU_PW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class RidgeRegression(Regression):\n",
        "    def __init__(self, lamda, learning_rate, iteration):\n",
        "        self.regularization = l2_regularization(lamda)\n",
        "        super(RidgeRegression, self).__init__(learning_rate, iteration, self.regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:36:39.759679Z",
          "iopub.status.busy": "2023-04-30T08:36:39.759220Z",
          "iopub.status.idle": "2023-04-30T08:36:39.803215Z",
          "shell.execute_reply": "2023-04-30T08:36:39.801482Z",
          "shell.execute_reply.started": "2023-04-30T08:36:39.759636Z"
        },
        "id": "Ows-DOCFU_PW",
        "outputId": "5ec2f68c-4e15-4b4f-e5fc-46d853b6bddb",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss function for the iteration 10----->1729.0773058230477 :)\n",
            "The loss function for the iteration 20----->1185.172057821407 :)\n",
            "The loss function for the iteration 30----->817.9613874870652 :)\n",
            "The loss function for the iteration 40----->570.0303000824639 :)\n",
            "The loss function for the iteration 50----->402.6202937211041 :)\n",
            "The loss function for the iteration 60----->289.5670967270054 :)\n",
            "The loss function for the iteration 70----->213.20828372515766 :)\n",
            "The loss function for the iteration 80----->161.6206757581874 :)\n",
            "The loss function for the iteration 90----->126.75542284842035 :)\n",
            "The loss function for the iteration 100----->103.17907501889032 :)\n",
            "The loss function for the iteration 110----->87.22373117284641 :)\n",
            "The loss function for the iteration 120----->76.4133395927257 :)\n",
            "The loss function for the iteration 130----->69.07641702753938 :)\n",
            "The loss function for the iteration 140----->64.08461129395707 :)\n",
            "The loss function for the iteration 150----->60.6762156521807 :)\n",
            "The loss function for the iteration 160----->58.33703062729429 :)\n",
            "The loss function for the iteration 170----->56.71993872990978 :)\n",
            "The loss function for the iteration 180----->55.59061266121779 :)\n",
            "The loss function for the iteration 190----->54.790865157377716 :)\n",
            "The loss function for the iteration 200----->54.213907978274186 :)\n",
            "The loss function for the iteration 210----->53.78765026930585 :)\n",
            "The loss function for the iteration 220----->53.463423972908345 :)\n",
            "The loss function for the iteration 230----->53.20837281790733 :)\n",
            "The loss function for the iteration 240----->53.0003144394278 :)\n",
            "The loss function for the iteration 250----->52.82427200741729 :)\n",
            "The loss function for the iteration 260----->52.67013287169832 :)\n",
            "The loss function for the iteration 270----->52.531068009478226 :)\n",
            "The loss function for the iteration 280----->52.40246505927549 :)\n",
            "The loss function for the iteration 290----->52.28120805589153 :)\n",
            "The loss function for the iteration 300----->52.16519120898466 :)\n",
            "The loss function for the iteration 310----->52.052990674845375 :)\n",
            "The loss function for the iteration 320----->51.943642982877996 :)\n",
            "The loss function for the iteration 330----->51.836495460284546 :)\n",
            "The loss function for the iteration 340----->51.73110525977218 :)\n",
            "The loss function for the iteration 350----->51.62717119716155 :)\n",
            "The loss function for the iteration 360----->51.5244877376101 :)\n",
            "The loss function for the iteration 370----->51.422913933454375 :)\n",
            "The loss function for the iteration 380----->51.322352455276125 :)\n",
            "The loss function for the iteration 390----->51.22273543648933 :)\n",
            "The loss function for the iteration 400----->51.12401491745604 :)\n",
            "The loss function for the iteration 410----->51.02615639455583 :)\n",
            "The loss function for the iteration 420----->50.92913446528294 :)\n",
            "The loss function for the iteration 430----->50.83292988828643 :)\n",
            "The loss function for the iteration 440----->50.737527598581465 :)\n",
            "The loss function for the iteration 450----->50.64291536755841 :)\n",
            "The loss function for the iteration 460----->50.549082898269425 :)\n",
            "The loss function for the iteration 470----->50.456021214554475 :)\n",
            "The loss function for the iteration 480----->50.36372224852691 :)\n",
            "The loss function for the iteration 490----->50.272178561965006 :)\n",
            "The loss function for the iteration 500----->50.18138315809867 :)\n",
            "The loss function for the iteration 510----->50.091329354419685 :)\n",
            "The loss function for the iteration 520----->50.00201069668712 :)\n",
            "The loss function for the iteration 530----->49.91342090074341 :)\n",
            "The loss function for the iteration 540----->49.8255538131051 :)\n",
            "The loss function for the iteration 550----->49.73840338422876 :)\n",
            "The loss function for the iteration 560----->49.6519636503345 :)\n",
            "The loss function for the iteration 570----->49.56622872100729 :)\n",
            "The loss function for the iteration 580----->49.48119277069998 :)\n",
            "The loss function for the iteration 590----->49.396850032870894 :)\n",
            "The loss function for the iteration 600----->49.31319479590118 :)\n",
            "The loss function for the iteration 610----->49.2302214002146 :)\n",
            "The loss function for the iteration 620----->49.14792423621002 :)\n",
            "The loss function for the iteration 630----->49.06629774274355 :)\n",
            "The loss function for the iteration 640----->48.9853364059827 :)\n",
            "The loss function for the iteration 650----->48.905034758512805 :)\n",
            "The loss function for the iteration 660----->48.82538737861446 :)\n",
            "The loss function for the iteration 670----->48.74638888965775 :)\n",
            "The loss function for the iteration 680----->48.66803395957594 :)\n",
            "The loss function for the iteration 690----->48.590317300394 :)\n",
            "The loss function for the iteration 700----->48.51323366779501 :)\n",
            "The loss function for the iteration 710----->48.43677786071312 :)\n",
            "The loss function for the iteration 720----->48.36094472094535 :)\n",
            "The loss function for the iteration 730----->48.28572913277701 :)\n",
            "The loss function for the iteration 740----->48.21112602261742 :)\n",
            "The loss function for the iteration 750----->48.13713035864319 :)\n",
            "The loss function for the iteration 760----->48.063737150447835 :)\n",
            "The loss function for the iteration 770----->47.99094144869632 :)\n",
            "The loss function for the iteration 780----->47.91873834478401 :)\n",
            "The loss function for the iteration 790----->47.84712297049938 :)\n",
            "The loss function for the iteration 800----->47.776090497690085 :)\n",
            "The loss function for the iteration 810----->47.70563613793249 :)\n",
            "The loss function for the iteration 820----->47.63575514220382 :)\n",
            "The loss function for the iteration 830----->47.566442800557645 :)\n",
            "The loss function for the iteration 840----->47.49769444180185 :)\n",
            "The loss function for the iteration 850----->47.4295054331795 :)\n",
            "The loss function for the iteration 860----->47.361871180052376 :)\n",
            "The loss function for the iteration 870----->47.29478712558716 :)\n",
            "The loss function for the iteration 880----->47.22824875044422 :)\n",
            "The loss function for the iteration 890----->47.16225157246892 :)\n",
            "The loss function for the iteration 900----->47.09679114638561 :)\n",
            "The loss function for the iteration 910----->47.03186306349407 :)\n",
            "The loss function for the iteration 920----->46.967462951368404 :)\n",
            "The loss function for the iteration 930----->46.90358647355851 :)\n",
            "The loss function for the iteration 940----->46.84022932929403 :)\n",
            "The loss function for the iteration 950----->46.77738725319058 :)\n",
            "The loss function for the iteration 960----->46.71505601495859 :)\n",
            "The loss function for the iteration 970----->46.6532314191144 :)\n",
            "The loss function for the iteration 980----->46.5919093046938 :)\n",
            "The loss function for the iteration 990----->46.53108554496798 :)\n",
            "The loss function for the iteration 1000----->46.47075604716163 :)\n",
            "The loss function for the iteration 1010----->46.41091675217362 :)\n",
            "The loss function for the iteration 1020----->46.351563634299744 :)\n",
            "The loss function for the iteration 1030----->46.292692700957936 :)\n",
            "The loss function for the iteration 1040----->46.23429999241553 :)\n",
            "The loss function for the iteration 1050----->46.17638158151905 :)\n",
            "The loss function for the iteration 1060----->46.11893357342597 :)\n",
            "The loss function for the iteration 1070----->46.06195210533881 :)\n",
            "The loss function for the iteration 1080----->46.005433346241354 :)\n",
            "The loss function for the iteration 1090----->45.94937349663715 :)\n",
            "The loss function for the iteration 1100----->45.89376878829001 :)\n",
            "The loss function for the iteration 1110----->45.8386154839668 :)\n",
            "The loss function for the iteration 1120----->45.78390987718219 :)\n",
            "The loss function for the iteration 1130----->45.72964829194565 :)\n",
            "The loss function for the iteration 1140----->45.67582708251046 :)\n",
            "The loss function for the iteration 1150----->45.62244263312474 :)\n",
            "The loss function for the iteration 1160----->45.56949135778463 :)\n",
            "The loss function for the iteration 1170----->45.51696969998944 :)\n",
            "The loss function for the iteration 1180----->45.46487413249883 :)\n",
            "The loss function for the iteration 1190----->45.41320115709195 :)\n",
            "The loss function for the iteration 1200----->45.36194730432867 :)\n",
            "The loss function for the iteration 1210----->45.31110913331265 :)\n",
            "The loss function for the iteration 1220----->45.260683231456376 :)\n",
            "The loss function for the iteration 1230----->45.210666214248285 :)\n",
            "The loss function for the iteration 1240----->45.16105472502158 :)\n",
            "The loss function for the iteration 1250----->45.11184543472513 :)\n",
            "The loss function for the iteration 1260----->45.0630350416961 :)\n",
            "The loss function for the iteration 1270----->45.01462027143465 :)\n",
            "The loss function for the iteration 1280----->44.96659787638026 :)\n",
            "The loss function for the iteration 1290----->44.918964635690116 :)\n",
            "The loss function for the iteration 1300----->44.87171735501909 :)\n",
            "The loss function for the iteration 1310----->44.8248528663018 :)\n",
            "The loss function for the iteration 1320----->44.778368027536196 :)\n",
            "The loss function for the iteration 1330----->44.73225972256909 :)\n",
            "The loss function for the iteration 1340----->44.68652486088346 :)\n",
            "The loss function for the iteration 1350----->44.64116037738739 :)\n",
            "The loss function for the iteration 1360----->44.59616323220485 :)\n",
            "The loss function for the iteration 1370----->44.551530410468146 :)\n",
            "The loss function for the iteration 1380----->44.5072589221121 :)\n",
            "The loss function for the iteration 1390----->44.46334580166994 :)\n",
            "The loss function for the iteration 1400----->44.419788108070776 :)\n",
            "The loss function for the iteration 1410----->44.376582924438914 :)\n",
            "The loss function for the iteration 1420----->44.33372735789465 :)\n",
            "The loss function for the iteration 1430----->44.291218539356805 :)\n",
            "The loss function for the iteration 1440----->44.24905362334686 :)\n",
            "The loss function for the iteration 1450----->44.207229787794674 :)\n",
            "The loss function for the iteration 1460----->44.16574423384586 :)\n",
            "The loss function for the iteration 1470----->44.1245941856707 :)\n",
            "The loss function for the iteration 1480----->44.08377689027464 :)\n",
            "The loss function for the iteration 1490----->44.04328961731035 :)\n",
            "The loss function for the iteration 1500----->44.00312965889136 :)\n",
            "The loss function for the iteration 1510----->43.963294329407155 :)\n",
            "The loss function for the iteration 1520----->43.92378096533989 :)\n",
            "The loss function for the iteration 1530----->43.88458692508252 :)\n",
            "The loss function for the iteration 1540----->43.84570958875849 :)\n",
            "The loss function for the iteration 1550----->43.8071463580429 :)\n",
            "The loss function for the iteration 1560----->43.76889465598508 :)\n",
            "The loss function for the iteration 1570----->43.73095192683279 :)\n",
            "The loss function for the iteration 1580----->43.69331563585757 :)\n",
            "The loss function for the iteration 1590----->43.65598326918192 :)\n",
            "The loss function for the iteration 1600----->43.61895233360752 :)\n",
            "The loss function for the iteration 1610----->43.58222035644512 :)\n",
            "The loss function for the iteration 1620----->43.54578488534576 :)\n",
            "The loss function for the iteration 1630----->43.50964348813335 :)\n",
            "The loss function for the iteration 1640----->43.47379375263856 :)\n",
            "The loss function for the iteration 1650----->43.43823328653432 :)\n",
            "The loss function for the iteration 1660----->43.40295971717239 :)\n",
            "The loss function for the iteration 1670----->43.367970691421476 :)\n",
            "The loss function for the iteration 1680----->43.3332638755065 :)\n",
            "The loss function for the iteration 1690----->43.29883695484946 :)\n",
            "The loss function for the iteration 1700----->43.26468763391129 :)\n",
            "The loss function for the iteration 1710----->43.23081363603521 :)\n",
            "The loss function for the iteration 1720----->43.19721270329135 :)\n",
            "The loss function for the iteration 1730----->43.16388259632263 :)\n",
            "The loss function for the iteration 1740----->43.13082109419187 :)\n",
            "The loss function for the iteration 1750----->43.098025994230206 :)\n",
            "The loss function for the iteration 1760----->43.06549511188671 :)\n",
            "The loss function for the iteration 1770----->43.03322628057937 :)\n",
            "The loss function for the iteration 1780----->43.00121735154712 :)\n",
            "The loss function for the iteration 1790----->42.969466193703184 :)\n",
            "The loss function for the iteration 1800----->42.93797069348963 :)\n",
            "The loss function for the iteration 1810----->42.906728754733145 :)\n",
            "The loss function for the iteration 1820----->42.87573829850189 :)\n",
            "The loss function for the iteration 1830----->42.84499726296362 :)\n",
            "The loss function for the iteration 1840----->42.81450360324502 :)\n",
            "The loss function for the iteration 1850----->42.784255291292055 :)\n",
            "The loss function for the iteration 1860----->42.75425031573155 :)\n",
            "The loss function for the iteration 1870----->42.72448668173396 :)\n",
            "The loss function for the iteration 1880----->42.69496241087717 :)\n",
            "The loss function for the iteration 1890----->42.665675541011524 :)\n",
            "The loss function for the iteration 1900----->42.63662412612581 :)\n",
            "The loss function for the iteration 1910----->42.607806236214515 :)\n",
            "The loss function for the iteration 1920----->42.57921995714606 :)\n",
            "The loss function for the iteration 1930----->42.5508633905322 :)\n",
            "The loss function for the iteration 1940----->42.522734653598384 :)\n",
            "The loss function for the iteration 1950----->42.49483187905536 :)\n",
            "The loss function for the iteration 1960----->42.46715321497166 :)\n",
            "The loss function for the iteration 1970----->42.43969682464717 :)\n",
            "The loss function for the iteration 1980----->42.41246088648792 :)\n",
            "The loss function for the iteration 1990----->42.3854435938816 :)\n",
            "The loss function for the iteration 2000----->42.35864315507434 :)\n",
            "The loss function for the iteration 2010----->42.33205779304847 :)\n",
            "The loss function for the iteration 2020----->42.305685745401156 :)\n",
            "The loss function for the iteration 2030----->42.27952526422416 :)\n",
            "The loss function for the iteration 2040----->42.2535746159846 :)\n",
            "The loss function for the iteration 2050----->42.22783208140651 :)\n",
            "The loss function for the iteration 2060----->42.2022959553537 :)\n",
            "The loss function for the iteration 2070----->42.17696454671319 :)\n",
            "The loss function for the iteration 2080----->42.151836178279936 :)\n",
            "The loss function for the iteration 2090----->42.12690918664234 :)\n",
            "The loss function for the iteration 2100----->42.102181922068674 :)\n",
            "The loss function for the iteration 2110----->42.07765274839462 :)\n",
            "The loss function for the iteration 2120----->42.05332004291147 :)\n",
            "The loss function for the iteration 2130----->42.0291821962555 :)\n",
            "The loss function for the iteration 2140----->42.00523761229808 :)\n",
            "The loss function for the iteration 2150----->41.98148470803677 :)\n",
            "The loss function for the iteration 2160----->41.95792191348732 :)\n",
            "The loss function for the iteration 2170----->41.9345476715765 :)\n",
            "The loss function for the iteration 2180----->41.91136043803588 :)\n",
            "The loss function for the iteration 2190----->41.88835868129641 :)\n",
            "The loss function for the iteration 2200----->41.865540882383904 :)\n",
            "The loss function for the iteration 2210----->41.84290553481548 :)\n",
            "The loss function for the iteration 2220----->41.82045114449662 :)\n",
            "The loss function for the iteration 2230----->41.798176229619344 :)\n",
            "The loss function for the iteration 2240----->41.77607932056099 :)\n",
            "The loss function for the iteration 2250----->41.75415895978406 :)\n",
            "The loss function for the iteration 2260----->41.73241370173661 :)\n",
            "The loss function for the iteration 2270----->41.710842112753745 :)\n",
            "The loss function for the iteration 2280----->41.689442770959715 :)\n",
            "The loss function for the iteration 2290----->41.66821426617093 :)\n",
            "The loss function for the iteration 2300----->41.64715519979972 :)\n",
            "The loss function for the iteration 2310----->41.626264184758924 :)\n",
            "The loss function for the iteration 2320----->41.60553984536725 :)\n",
            "The loss function for the iteration 2330----->41.58498081725536 :)\n",
            "The loss function for the iteration 2340----->41.564585747272815 :)\n",
            "The loss function for the iteration 2350----->41.5443532933958 :)\n",
            "The loss function for the iteration 2360----->41.52428212463542 :)\n",
            "The loss function for the iteration 2370----->41.50437092094703 :)\n",
            "The loss function for the iteration 2380----->41.484618373140115 :)\n",
            "The loss function for the iteration 2390----->41.465023182788926 :)\n",
            "The loss function for the iteration 2400----->41.44558406214395 :)\n",
            "The loss function for the iteration 2410----->41.426299734043994 :)\n",
            "The loss function for the iteration 2420----->41.40716893182913 :)\n",
            "The loss function for the iteration 2430----->41.38819039925421 :)\n",
            "The loss function for the iteration 2440----->41.369362890403146 :)\n",
            "The loss function for the iteration 2450----->41.35068516960396 :)\n",
            "The loss function for the iteration 2460----->41.332156011344466 :)\n",
            "The loss function for the iteration 2470----->41.313774200188625 :)\n",
            "The loss function for the iteration 2480----->41.29553853069369 :)\n",
            "The loss function for the iteration 2490----->41.27744780732788 :)\n",
            "The loss function for the iteration 2500----->41.25950084438891 :)\n",
            "The loss function for the iteration 2510----->41.24169646592303 :)\n",
            "The loss function for the iteration 2520----->41.22403350564484 :)\n",
            "The loss function for the iteration 2530----->41.20651080685773 :)\n",
            "The loss function for the iteration 2540----->41.189127222374914 :)\n",
            "The loss function for the iteration 2550----->41.17188161444125 :)\n",
            "The loss function for the iteration 2560----->41.15477285465556 :)\n",
            "The loss function for the iteration 2570----->41.13779982389371 :)\n",
            "The loss function for the iteration 2580----->41.12096141223222 :)\n",
            "The loss function for the iteration 2590----->41.10425651887254 :)\n",
            "The loss function for the iteration 2600----->41.08768405206601 :)\n",
            "The loss function for the iteration 2610----->41.071242929039364 :)\n",
            "The loss function for the iteration 2620----->41.054932075920846 :)\n",
            "The loss function for the iteration 2630----->41.038750427667 :)\n",
            "The loss function for the iteration 2640----->41.022696927989976 :)\n",
            "The loss function for the iteration 2650----->41.00677052928549 :)\n",
            "The loss function for the iteration 2660----->40.990970192561434 :)\n",
            "The loss function for the iteration 2670----->40.97529488736685 :)\n",
            "The loss function for the iteration 2680----->40.959743591721825 :)\n",
            "The loss function for the iteration 2690----->40.94431529204765 :)\n",
            "The loss function for the iteration 2700----->40.929008983097745 :)\n",
            "The loss function for the iteration 2710----->40.91382366788907 :)\n",
            "The loss function for the iteration 2720----->40.89875835763414 :)\n",
            "The loss function for the iteration 2730----->40.88381207167357 :)\n",
            "The loss function for the iteration 2740----->40.868983837409196 :)\n",
            "The loss function for the iteration 2750----->40.85427269023772 :)\n",
            "The loss function for the iteration 2760----->40.839677673484985 :)\n",
            "The loss function for the iteration 2770----->40.82519783834062 :)\n",
            "The loss function for the iteration 2780----->40.810832243793406 :)\n",
            "The loss function for the iteration 2790----->40.79657995656707 :)\n",
            "The loss function for the iteration 2800----->40.782440051056675 :)\n",
            "The loss function for the iteration 2810----->40.76841160926541 :)\n",
            "The loss function for the iteration 2820----->40.75449372074205 :)\n",
            "The loss function for the iteration 2830----->40.740685482518835 :)\n",
            "The loss function for the iteration 2840----->40.72698599904994 :)\n",
            "The loss function for the iteration 2850----->40.713394382150305 :)\n",
            "The loss function for the iteration 2860----->40.69990975093515 :)\n",
            "The loss function for the iteration 2870----->40.686531231759844 :)\n",
            "The loss function for the iteration 2880----->40.67325795816039 :)\n",
            "The loss function for the iteration 2890----->40.660089070794264 :)\n",
            "The loss function for the iteration 2900----->40.64702371738186 :)\n",
            "The loss function for the iteration 2910----->40.63406105264837 :)\n",
            "The loss function for the iteration 2920----->40.62120023826611 :)\n",
            "The loss function for the iteration 2930----->40.608440442797395 :)\n",
            "The loss function for the iteration 2940----->40.59578084163782 :)\n",
            "The loss function for the iteration 2950----->40.583220616960006 :)\n",
            "The loss function for the iteration 2960----->40.5707589576579 :)\n",
            "The loss function for the iteration 2970----->40.55839505929141 :)\n",
            "The loss function for the iteration 2980----->40.546128124031554 :)\n",
            "The loss function for the iteration 2990----->40.53395736060612 :)\n",
            "The loss function for the iteration 3000----->40.52188198424561 :)\n",
            "The loss function for the iteration 3010----->40.50990121662983 :)\n",
            "The loss function for the iteration 3020----->40.49801428583476 :)\n",
            "The loss function for the iteration 3030----->40.48622042627997 :)\n",
            "The loss function for the iteration 3040----->40.47451887867635 :)\n",
            "The loss function for the iteration 3050----->40.462908889974415 :)\n",
            "The loss function for the iteration 3060----->40.45138971331293 :)\n",
            "The loss function for the iteration 3070----->40.43996060796796 :)\n",
            "The loss function for the iteration 3080----->40.42862083930245 :)\n",
            "The loss function for the iteration 3090----->40.41736967871604 :)\n",
            "The loss function for the iteration 3100----->40.406206403595455 :)\n",
            "The loss function for the iteration 3110----->40.395130297265275 :)\n",
            "The loss function for the iteration 3120----->40.38414064893891 :)\n",
            "The loss function for the iteration 3130----->40.37323675367038 :)\n",
            "The loss function for the iteration 3140----->40.362417912306086 :)\n",
            "The loss function for the iteration 3150----->40.35168343143715 :)\n",
            "The loss function for the iteration 3160----->40.3410326233523 :)\n",
            "The loss function for the iteration 3170----->40.33046480599077 :)\n",
            "The loss function for the iteration 3180----->40.31997930289598 :)\n",
            "The loss function for the iteration 3190----->40.30957544316934 :)\n",
            "The loss function for the iteration 3200----->40.299252561424545 :)\n",
            "The loss function for the iteration 3210----->40.2890099977422 :)\n",
            "The loss function for the iteration 3220----->40.27884709762482 :)\n",
            "The loss function for the iteration 3230----->40.26876321195229 :)\n",
            "The loss function for the iteration 3240----->40.25875769693754 :)\n",
            "The loss function for the iteration 3250----->40.2488299140827 :)\n",
            "The loss function for the iteration 3260----->40.23897923013556 :)\n",
            "The loss function for the iteration 3270----->40.229205017046446 :)\n",
            "The loss function for the iteration 3280----->40.21950665192539 :)\n",
            "The loss function for the iteration 3290----->40.209883516999604 :)\n",
            "The loss function for the iteration 3300----->40.20033499957155 :)\n",
            "The loss function for the iteration 3310----->40.190860491976984 :)\n",
            "The loss function for the iteration 3320----->40.181459391543676 :)\n",
            "The loss function for the iteration 3330----->40.17213110055027 :)\n",
            "The loss function for the iteration 3340----->40.162875026185596 :)\n",
            "The loss function for the iteration 3350----->40.15369058050819 :)\n",
            "The loss function for the iteration 3360----->40.14457718040634 :)\n",
            "The loss function for the iteration 3370----->40.13553424755822 :)\n",
            "The loss function for the iteration 3380----->40.12656120839258 :)\n",
            "The loss function for the iteration 3390----->40.117657494049624 :)\n",
            "The loss function for the iteration 3400----->40.10882254034224 :)\n",
            "The loss function for the iteration 3410----->40.1000557877175 :)\n",
            "The loss function for the iteration 3420----->40.09135668121866 :)\n",
            "The loss function for the iteration 3430----->40.08272467044723 :)\n",
            "The loss function for the iteration 3440----->40.07415920952551 :)\n",
            "The loss function for the iteration 3450----->40.06565975705939 :)\n",
            "The loss function for the iteration 3460----->40.05722577610148 :)\n",
            "The loss function for the iteration 3470----->40.048856734114466 :)\n",
            "The loss function for the iteration 3480----->40.040552102934896 :)\n",
            "The loss function for the iteration 3490----->40.03231135873714 :)\n",
            "The loss function for the iteration 3500----->40.024133981997736 :)\n",
            "The loss function for the iteration 3510----->40.01601945745997 :)\n",
            "The loss function for the iteration 3520----->40.00796727409876 :)\n",
            "The loss function for the iteration 3530----->39.99997692508589 :)\n",
            "The loss function for the iteration 3540----->39.99204790775541 :)\n",
            "The loss function for the iteration 3550----->39.984179723569454 :)\n",
            "The loss function for the iteration 3560----->39.97637187808424 :)\n",
            "The loss function for the iteration 3570----->39.96862388091638 :)\n",
            "The loss function for the iteration 3580----->39.960935245709514 :)\n",
            "The loss function for the iteration 3590----->39.953305490101116 :)\n",
            "The loss function for the iteration 3600----->39.94573413568971 :)\n",
            "The loss function for the iteration 3610----->39.93822070800219 :)\n",
            "The loss function for the iteration 3620----->39.930764736461605 :)\n",
            "The loss function for the iteration 3630----->39.923365754355025 :)\n",
            "The loss function for the iteration 3640----->39.91602329880174 :)\n",
            "The loss function for the iteration 3650----->39.90873691072182 :)\n",
            "The loss function for the iteration 3660----->39.90150613480475 :)\n",
            "The loss function for the iteration 3670----->39.89433051947848 :)\n",
            "The loss function for the iteration 3680----->39.887209616878614 :)\n",
            "The loss function for the iteration 3690----->39.88014298281797 :)\n",
            "The loss function for the iteration 3700----->39.87313017675627 :)\n",
            "The loss function for the iteration 3710----->39.86617076177018 :)\n",
            "The loss function for the iteration 3720----->39.8592643045235 :)\n",
            "The loss function for the iteration 3730----->39.85241037523772 :)\n",
            "The loss function for the iteration 3740----->39.84560854766269 :)\n",
            "The loss function for the iteration 3750----->39.83885839904762 :)\n",
            "The loss function for the iteration 3760----->39.83215951011232 :)\n",
            "The loss function for the iteration 3770----->39.82551146501854 :)\n",
            "The loss function for the iteration 3780----->39.81891385134178 :)\n",
            "The loss function for the iteration 3790----->39.81236626004318 :)\n",
            "The loss function for the iteration 3800----->39.80586828544155 :)\n",
            "The loss function for the iteration 3810----->39.79941952518591 :)\n",
            "The loss function for the iteration 3820----->39.79301958022799 :)\n",
            "The loss function for the iteration 3830----->39.7866680547951 :)\n",
            "The loss function for the iteration 3840----->39.78036455636313 :)\n",
            "The loss function for the iteration 3850----->39.77410869562992 :)\n",
            "The loss function for the iteration 3860----->39.767900086488645 :)\n",
            "The loss function for the iteration 3870----->39.761738346001614 :)\n",
            "The loss function for the iteration 3880----->39.75562309437414 :)\n",
            "The loss function for the iteration 3890----->39.749553954928736 :)\n",
            "The loss function for the iteration 3900----->39.74353055407941 :)\n",
            "The loss function for the iteration 3910----->39.7375525213063 :)\n",
            "The loss function for the iteration 3920----->39.731619489130374 :)\n",
            "The loss function for the iteration 3930----->39.72573109308847 :)\n",
            "The loss function for the iteration 3940----->39.719886971708505 :)\n",
            "The loss function for the iteration 3950----->39.714086766484776 :)\n",
            "The loss function for the iteration 3960----->39.70833012185365 :)\n",
            "The loss function for the iteration 3970----->39.70261668516931 :)\n",
            "The loss function for the iteration 3980----->39.6969461066798 :)\n",
            "The loss function for the iteration 3990----->39.69131803950316 :)\n",
            "The loss function for the iteration 4000----->39.685732139603815 :)\n",
            "The loss function for the iteration 4010----->39.680188065769265 :)\n",
            "The loss function for the iteration 4020----->39.67468547958672 :)\n",
            "The loss function for the iteration 4030----->39.669224045420144 :)\n",
            "The loss function for the iteration 4040----->39.663803430387425 :)\n",
            "The loss function for the iteration 4050----->39.65842330433769 :)\n",
            "The loss function for the iteration 4060----->39.65308333982881 :)\n",
            "The loss function for the iteration 4070----->39.64778321210521 :)\n",
            "The loss function for the iteration 4080----->39.642522599075676 :)\n",
            "The loss function for the iteration 4090----->39.63730118129148 :)\n",
            "The loss function for the iteration 4100----->39.63211864192462 :)\n",
            "The loss function for the iteration 4110----->39.6269746667463 :)\n",
            "The loss function for the iteration 4120----->39.62186894410554 :)\n",
            "The loss function for the iteration 4130----->39.61680116490792 :)\n",
            "The loss function for the iteration 4140----->39.61177102259461 :)\n",
            "The loss function for the iteration 4150----->39.60677821312146 :)\n",
            "The loss function for the iteration 4160----->39.601822434938406 :)\n",
            "The loss function for the iteration 4170----->39.59690338896885 :)\n",
            "The loss function for the iteration 4180----->39.592020778589344 :)\n",
            "The loss function for the iteration 4190----->39.587174309609466 :)\n",
            "The loss function for the iteration 4200----->39.582363690251725 :)\n",
            "The loss function for the iteration 4210----->39.57758863113178 :)\n",
            "The loss function for the iteration 4220----->39.572848845238724 :)\n",
            "The loss function for the iteration 4230----->39.56814404791554 :)\n",
            "The loss function for the iteration 4240----->39.56347395683979 :)\n",
            "The loss function for the iteration 4250----->39.55883829200437 :)\n",
            "The loss function for the iteration 4260----->39.55423677569848 :)\n",
            "The loss function for the iteration 4270----->39.54966913248876 :)\n",
            "The loss function for the iteration 4280----->39.54513508920051 :)\n",
            "The loss function for the iteration 4290----->39.54063437489913 :)\n",
            "The loss function for the iteration 4300----->39.53616672087171 :)\n",
            "The loss function for the iteration 4310----->39.53173186060876 :)\n",
            "The loss function for the iteration 4320----->39.52732952978607 :)\n",
            "The loss function for the iteration 4330----->39.5229594662467 :)\n",
            "The loss function for the iteration 4340----->39.518621409983204 :)\n",
            "The loss function for the iteration 4350----->39.51431510311999 :)\n",
            "The loss function for the iteration 4360----->39.510040289895656 :)\n",
            "The loss function for the iteration 4370----->39.50579671664574 :)\n",
            "The loss function for the iteration 4380----->39.50158413178538 :)\n",
            "The loss function for the iteration 4390----->39.49740228579222 :)\n",
            "The loss function for the iteration 4400----->39.49325093118952 :)\n",
            "The loss function for the iteration 4410----->39.48912982252923 :)\n",
            "The loss function for the iteration 4420----->39.48503871637537 :)\n",
            "The loss function for the iteration 4430----->39.48097737128745 :)\n",
            "The loss function for the iteration 4440----->39.47694554780409 :)\n",
            "The loss function for the iteration 4450----->39.47294300842671 :)\n",
            "The loss function for the iteration 4460----->39.468969517603384 :)\n",
            "The loss function for the iteration 4470----->39.46502484171286 :)\n",
            "The loss function for the iteration 4480----->39.46110874904866 :)\n",
            "The loss function for the iteration 4490----->39.45722100980337 :)\n",
            "The loss function for the iteration 4500----->39.45336139605292 :)\n",
            "The loss function for the iteration 4510----->39.44952968174122 :)\n",
            "The loss function for the iteration 4520----->39.44572564266472 :)\n",
            "The loss function for the iteration 4530----->39.44194905645716 :)\n",
            "The loss function for the iteration 4540----->39.43819970257458 :)\n",
            "The loss function for the iteration 4550----->39.434477362280155 :)\n",
            "The loss function for the iteration 4560----->39.43078181862948 :)\n",
            "The loss function for the iteration 4570----->39.427112856455736 :)\n",
            "The loss function for the iteration 4580----->39.423470262355124 :)\n",
            "The loss function for the iteration 4590----->39.419853824672366 :)\n",
            "The loss function for the iteration 4600----->39.41626333348628 :)\n",
            "The loss function for the iteration 4610----->39.41269858059557 :)\n",
            "The loss function for the iteration 4620----->39.40915935950461 :)\n",
            "The loss function for the iteration 4630----->39.405645465409556 :)\n",
            "The loss function for the iteration 4640----->39.402156695184246 :)\n",
            "The loss function for the iteration 4650----->39.39869284736657 :)\n",
            "The loss function for the iteration 4660----->39.39525372214468 :)\n",
            "The loss function for the iteration 4670----->39.39183912134345 :)\n",
            "The loss function for the iteration 4680----->39.388448848411045 :)\n",
            "The loss function for the iteration 4690----->39.38508270840553 :)\n",
            "The loss function for the iteration 4700----->39.38174050798163 :)\n",
            "The loss function for the iteration 4710----->39.37842205537767 :)\n",
            "The loss function for the iteration 4720----->39.37512716040244 :)\n",
            "The loss function for the iteration 4730----->39.371855634422374 :)\n",
            "The loss function for the iteration 4740----->39.368607290348706 :)\n",
            "The loss function for the iteration 4750----->39.36538194262475 :)\n",
            "The loss function for the iteration 4760----->39.36217940721333 :)\n",
            "The loss function for the iteration 4770----->39.35899950158428 :)\n",
            "The loss function for the iteration 4780----->39.355842044702044 :)\n",
            "The loss function for the iteration 4790----->39.35270685701336 :)\n",
            "The loss function for the iteration 4800----->39.34959376043514 :)\n",
            "The loss function for the iteration 4810----->39.34650257834232 :)\n",
            "The loss function for the iteration 4820----->39.34343313555586 :)\n",
            "The loss function for the iteration 4830----->39.34038525833094 :)\n",
            "The loss function for the iteration 4840----->39.33735877434508 :)\n",
            "The loss function for the iteration 4850----->39.33435351268646 :)\n",
            "The loss function for the iteration 4860----->39.33136930384237 :)\n",
            "The loss function for the iteration 4870----->39.32840597968763 :)\n",
            "The loss function for the iteration 4880----->39.32546337347326 :)\n",
            "The loss function for the iteration 4890----->39.322541319815095 :)\n",
            "The loss function for the iteration 4900----->39.31963965468261 :)\n",
            "The loss function for the iteration 4910----->39.31675821538778 :)\n",
            "The loss function for the iteration 4920----->39.31389684057402 :)\n",
            "The loss function for the iteration 4930----->39.311055370205246 :)\n",
            "The loss function for the iteration 4940----->39.30823364555505 :)\n",
            "The loss function for the iteration 4950----->39.30543150919588 :)\n",
            "The loss function for the iteration 4960----->39.302648804988394 :)\n",
            "The loss function for the iteration 4970----->39.29988537807087 :)\n",
            "The loss function for the iteration 4980----->39.29714107484868 :)\n",
            "The loss function for the iteration 4990----->39.2944157429839 :)\n",
            "The loss function for the iteration 5000----->39.29170923138493 :)\n",
            "The loss function for the iteration 5010----->39.28902139019632 :)\n",
            "The loss function for the iteration 5020----->39.28635207078851 :)\n",
            "The loss function for the iteration 5030----->39.283701125747875 :)\n",
            "The loss function for the iteration 5040----->39.281068408866595 :)\n",
            "The loss function for the iteration 5050----->39.27845377513285 :)\n",
            "The loss function for the iteration 5060----->39.275857080720925 :)\n",
            "The loss function for the iteration 5070----->39.27327818298147 :)\n",
            "The loss function for the iteration 5080----->39.27071694043185 :)\n",
            "The loss function for the iteration 5090----->39.26817321274653 :)\n",
            "The loss function for the iteration 5100----->39.26564686074757 :)\n",
            "The loss function for the iteration 5110----->39.26313774639518 :)\n",
            "The loss function for the iteration 5120----->39.26064573277839 :)\n",
            "The loss function for the iteration 5130----->39.258170684105764 :)\n",
            "The loss function for the iteration 5140----->39.25571246569614 :)\n",
            "The loss function for the iteration 5150----->39.25327094396962 :)\n",
            "The loss function for the iteration 5160----->39.25084598643836 :)\n",
            "The loss function for the iteration 5170----->39.24843746169775 :)\n",
            "The loss function for the iteration 5180----->39.24604523941741 :)\n",
            "The loss function for the iteration 5190----->39.24366919033236 :)\n",
            "The loss function for the iteration 5200----->39.241309186234346 :)\n",
            "The loss function for the iteration 5210----->39.23896509996304 :)\n",
            "The loss function for the iteration 5220----->39.2366368053975 :)\n",
            "The loss function for the iteration 5230----->39.23432417744763 :)\n",
            "The loss function for the iteration 5240----->39.23202709204565 :)\n",
            "The loss function for the iteration 5250----->39.22974542613773 :)\n",
            "The loss function for the iteration 5260----->39.22747905767568 :)\n",
            "The loss function for the iteration 5270----->39.2252278656086 :)\n",
            "The loss function for the iteration 5280----->39.2229917298748 :)\n",
            "The loss function for the iteration 5290----->39.22077053139354 :)\n",
            "The loss function for the iteration 5300----->39.21856415205709 :)\n",
            "The loss function for the iteration 5310----->39.21637247472265 :)\n",
            "The loss function for the iteration 5320----->39.21419538320441 :)\n",
            "The loss function for the iteration 5330----->39.21203276226579 :)\n",
            "The loss function for the iteration 5340----->39.20988449761149 :)\n",
            "The loss function for the iteration 5350----->39.207750475879884 :)\n",
            "The loss function for the iteration 5360----->39.205630584635244 :)\n",
            "The loss function for the iteration 5370----->39.203524712360206 :)\n",
            "The loss function for the iteration 5380----->39.20143274844817 :)\n",
            "The loss function for the iteration 5390----->39.19935458319587 :)\n",
            "The loss function for the iteration 5400----->39.197290107795865 :)\n",
            "The loss function for the iteration 5410----->39.19523921432927 :)\n",
            "The loss function for the iteration 5420----->39.19320179575839 :)\n",
            "The loss function for the iteration 5430----->39.191177745919475 :)\n",
            "The loss function for the iteration 5440----->39.18916695951561 :)\n",
            "The loss function for the iteration 5450----->39.1871693321095 :)\n",
            "The loss function for the iteration 5460----->39.18518476011645 :)\n",
            "The loss function for the iteration 5470----->39.18321314079737 :)\n",
            "The loss function for the iteration 5480----->39.181254372251786 :)\n",
            "The loss function for the iteration 5490----->39.179308353411 :)\n",
            "The loss function for the iteration 5500----->39.17737498403119 :)\n",
            "The loss function for the iteration 5510----->39.17545416468669 :)\n",
            "The loss function for the iteration 5520----->39.173545796763236 :)\n",
            "The loss function for the iteration 5530----->39.17164978245132 :)\n",
            "The loss function for the iteration 5540----->39.16976602473951 :)\n",
            "The loss function for the iteration 5550----->39.167894427408 :)\n",
            "The loss function for the iteration 5560----->39.166034895022015 :)\n",
            "The loss function for the iteration 5570----->39.1641873329254 :)\n",
            "The loss function for the iteration 5580----->39.162351647234225 :)\n",
            "The loss function for the iteration 5590----->39.16052774483041 :)\n",
            "The loss function for the iteration 5600----->39.158715533355455 :)\n",
            "The loss function for the iteration 5610----->39.15691492120421 :)\n",
            "The loss function for the iteration 5620----->39.15512581751863 :)\n",
            "The loss function for the iteration 5630----->39.153348132181726 :)\n",
            "The loss function for the iteration 5640----->39.15158177581138 :)\n",
            "The loss function for the iteration 5650----->39.149826659754375 :)\n",
            "The loss function for the iteration 5660----->39.14808269608035 :)\n",
            "The loss function for the iteration 5670----->39.14634979757594 :)\n",
            "The loss function for the iteration 5680----->39.144627877738785 :)\n",
            "The loss function for the iteration 5690----->39.14291685077178 :)\n",
            "The loss function for the iteration 5700----->39.14121663157723 :)\n",
            "The loss function for the iteration 5710----->39.13952713575111 :)\n",
            "The loss function for the iteration 5720----->39.13784827957735 :)\n",
            "The loss function for the iteration 5730----->39.136179980022234 :)\n",
            "The loss function for the iteration 5740----->39.13452215472875 :)\n",
            "The loss function for the iteration 5750----->39.132874722011024 :)\n",
            "The loss function for the iteration 5760----->39.131237600848834 :)\n",
            "The loss function for the iteration 5770----->39.12961071088217 :)\n",
            "The loss function for the iteration 5780----->39.127993972405704 :)\n",
            "The loss function for the iteration 5790----->39.12638730636353 :)\n",
            "The loss function for the iteration 5800----->39.124790634343775 :)\n",
            "The loss function for the iteration 5810----->39.123203878573264 :)\n",
            "The loss function for the iteration 5820----->39.12162696191238 :)\n",
            "The loss function for the iteration 5830----->39.12005980784974 :)\n",
            "The loss function for the iteration 5840----->39.118502340497166 :)\n",
            "The loss function for the iteration 5850----->39.11695448458442 :)\n",
            "The loss function for the iteration 5860----->39.11541616545423 :)\n",
            "The loss function for the iteration 5870----->39.113887309057255 :)\n",
            "The loss function for the iteration 5880----->39.11236784194702 :)\n",
            "The loss function for the iteration 5890----->39.110857691275015 :)\n",
            "The loss function for the iteration 5900----->39.10935678478576 :)\n",
            "The loss function for the iteration 5910----->39.10786505081201 :)\n",
            "The loss function for the iteration 5920----->39.106382418269774 :)\n",
            "The loss function for the iteration 5930----->39.104908816653676 :)\n",
            "The loss function for the iteration 5940----->39.103444176032106 :)\n",
            "The loss function for the iteration 5950----->39.101988427042556 :)\n",
            "The loss function for the iteration 5960----->39.1005415008869 :)\n",
            "The loss function for the iteration 5970----->39.0991033293268 :)\n",
            "The loss function for the iteration 5980----->39.09767384467914 :)\n",
            "The loss function for the iteration 5990----->39.09625297981135 :)\n",
            "The loss function for the iteration 6000----->39.09484066813698 :)\n",
            "The loss function for the iteration 6010----->39.0934368436112 :)\n",
            "The loss function for the iteration 6020----->39.09204144072636 :)\n",
            "The loss function for the iteration 6030----->39.09065439450752 :)\n",
            "The loss function for the iteration 6040----->39.08927564050811 :)\n",
            "The loss function for the iteration 6050----->39.087905114805665 :)\n",
            "The loss function for the iteration 6060----->39.08654275399737 :)\n",
            "The loss function for the iteration 6070----->39.08518849519591 :)\n",
            "The loss function for the iteration 6080----->39.083842276025216 :)\n",
            "The loss function for the iteration 6090----->39.082504034616214 :)\n",
            "The loss function for the iteration 6100----->39.081173709602744 :)\n",
            "The loss function for the iteration 6110----->39.07985124011735 :)\n",
            "The loss function for the iteration 6120----->39.078536565787225 :)\n",
            "The loss function for the iteration 6130----->39.07722962673015 :)\n",
            "The loss function for the iteration 6140----->39.07593036355048 :)\n",
            "The loss function for the iteration 6150----->39.07463871733511 :)\n",
            "The loss function for the iteration 6160----->39.073354629649515 :)\n",
            "The loss function for the iteration 6170----->39.07207804253385 :)\n",
            "The loss function for the iteration 6180----->39.070808898499074 :)\n",
            "The loss function for the iteration 6190----->39.069547140522985 :)\n",
            "The loss function for the iteration 6200----->39.06829271204652 :)\n",
            "The loss function for the iteration 6210----->39.067045556969816 :)\n",
            "The loss function for the iteration 6220----->39.06580561964857 :)\n",
            "The loss function for the iteration 6230----->39.06457284489021 :)\n",
            "The loss function for the iteration 6240----->39.0633471779502 :)\n",
            "The loss function for the iteration 6250----->39.0621285645284 :)\n",
            "The loss function for the iteration 6260----->39.060916950765396 :)\n",
            "The loss function for the iteration 6270----->39.05971228323884 :)\n",
            "The loss function for the iteration 6280----->39.05851450895993 :)\n",
            "The loss function for the iteration 6290----->39.05732357536984 :)\n",
            "The loss function for the iteration 6300----->39.05613943033613 :)\n",
            "The loss function for the iteration 6310----->39.05496202214931 :)\n",
            "The loss function for the iteration 6320----->39.05379129951937 :)\n",
            "The loss function for the iteration 6330----->39.05262721157224 :)\n",
            "The loss function for the iteration 6340----->39.05146970784654 :)\n",
            "The loss function for the iteration 6350----->39.050318738290024 :)\n",
            "The loss function for the iteration 6360----->39.049174253256346 :)\n",
            "The loss function for the iteration 6370----->39.048036203501674 :)\n",
            "The loss function for the iteration 6380----->39.04690454018136 :)\n",
            "The loss function for the iteration 6390----->39.045779214846746 :)\n",
            "The loss function for the iteration 6400----->39.04466017944184 :)\n",
            "The loss function for the iteration 6410----->39.0435473863001 :)\n",
            "The loss function for the iteration 6420----->39.0424407881413 :)\n",
            "The loss function for the iteration 6430----->39.04134033806826 :)\n",
            "The loss function for the iteration 6440----->39.04024598956382 :)\n",
            "The loss function for the iteration 6450----->39.0391576964876 :)\n",
            "The loss function for the iteration 6460----->39.03807541307301 :)\n",
            "The loss function for the iteration 6470----->39.03699909392411 :)\n",
            "The loss function for the iteration 6480----->39.035928694012625 :)\n",
            "The loss function for the iteration 6490----->39.03486416867488 :)\n",
            "The loss function for the iteration 6500----->39.03380547360883 :)\n",
            "The loss function for the iteration 6510----->39.03275256487109 :)\n",
            "The loss function for the iteration 6520----->39.03170539887403 :)\n",
            "The loss function for the iteration 6530----->39.03066393238275 :)\n",
            "The loss function for the iteration 6540----->39.02962812251233 :)\n",
            "The loss function for the iteration 6550----->39.02859792672483 :)\n",
            "The loss function for the iteration 6560----->39.0275733028265 :)\n",
            "The loss function for the iteration 6570----->39.026554208964995 :)\n",
            "The loss function for the iteration 6580----->39.02554060362649 :)\n",
            "The loss function for the iteration 6590----->39.02453244563294 :)\n",
            "The loss function for the iteration 6600----->39.023529694139384 :)\n",
            "The loss function for the iteration 6610----->39.022532308631085 :)\n",
            "The loss function for the iteration 6620----->39.02154024892096 :)\n",
            "The loss function for the iteration 6630----->39.02055347514674 :)\n",
            "The loss function for the iteration 6640----->39.01957194776851 :)\n",
            "The loss function for the iteration 6650----->39.018595627565844 :)\n",
            "The loss function for the iteration 6660----->39.017624475635344 :)\n",
            "The loss function for the iteration 6670----->39.01665845338795 :)\n",
            "The loss function for the iteration 6680----->39.0156975225464 :)\n",
            "The loss function for the iteration 6690----->39.014741645142664 :)\n",
            "The loss function for the iteration 6700----->39.01379078351543 :)\n",
            "The loss function for the iteration 6710----->39.012844900307556 :)\n",
            "The loss function for the iteration 6720----->39.01190395846355 :)\n",
            "The loss function for the iteration 6730----->39.0109679212272 :)\n",
            "The loss function for the iteration 6740----->39.010036752138994 :)\n",
            "The loss function for the iteration 6750----->39.00911041503378 :)\n",
            "The loss function for the iteration 6760----->39.00818887403831 :)\n",
            "The loss function for the iteration 6770----->39.00727209356882 :)\n",
            "The loss function for the iteration 6780----->39.00636003832875 :)\n",
            "The loss function for the iteration 6790----->39.00545267330627 :)\n",
            "The loss function for the iteration 6800----->39.004549963772035 :)\n",
            "The loss function for the iteration 6810----->39.003651875276816 :)\n",
            "The loss function for the iteration 6820----->39.00275837364925 :)\n",
            "The loss function for the iteration 6830----->39.00186942499351 :)\n",
            "The loss function for the iteration 6840----->39.00098499568708 :)\n",
            "The loss function for the iteration 6850----->39.0001050523785 :)\n",
            "The loss function for the iteration 6860----->38.99922956198516 :)\n",
            "The loss function for the iteration 6870----->38.998358491691086 :)\n",
            "The loss function for the iteration 6880----->38.99749180894473 :)\n",
            "The loss function for the iteration 6890----->38.996629481456836 :)\n",
            "The loss function for the iteration 6900----->38.995771477198296 :)\n",
            "The loss function for the iteration 6910----->38.99491776439795 :)\n",
            "The loss function for the iteration 6920----->38.99406831154059 :)\n",
            "The loss function for the iteration 6930----->38.99322308736473 :)\n",
            "The loss function for the iteration 6940----->38.99238206086061 :)\n",
            "The loss function for the iteration 6950----->38.99154520126814 :)\n",
            "The loss function for the iteration 6960----->38.99071247807479 :)\n",
            "The loss function for the iteration 6970----->38.98988386101359 :)\n",
            "The loss function for the iteration 6980----->38.989059320061145 :)\n",
            "The loss function for the iteration 6990----->38.98823882543562 :)\n",
            "The loss function for the iteration 7000----->38.987422347594745 :)\n",
            "The loss function for the iteration 7010----->38.986609857233866 :)\n",
            "The loss function for the iteration 7020----->38.985801325283965 :)\n",
            "The loss function for the iteration 7030----->38.98499672290981 :)\n",
            "The loss function for the iteration 7040----->38.98419602150798 :)\n",
            "The loss function for the iteration 7050----->38.983399192704915 :)\n",
            "The loss function for the iteration 7060----->38.982606208355186 :)\n",
            "The loss function for the iteration 7070----->38.981817040539426 :)\n",
            "The loss function for the iteration 7080----->38.98103166156268 :)\n",
            "The loss function for the iteration 7090----->38.98025004395239 :)\n",
            "The loss function for the iteration 7100----->38.979472160456694 :)\n",
            "The loss function for the iteration 7110----->38.978697984042576 :)\n",
            "The loss function for the iteration 7120----->38.97792748789405 :)\n",
            "The loss function for the iteration 7130----->38.97716064541041 :)\n",
            "The loss function for the iteration 7140----->38.97639743020447 :)\n",
            "The loss function for the iteration 7150----->38.9756378161008 :)\n",
            "The loss function for the iteration 7160----->38.97488177713399 :)\n",
            "The loss function for the iteration 7170----->38.97412928754696 :)\n",
            "The loss function for the iteration 7180----->38.973380321789165 :)\n",
            "The loss function for the iteration 7190----->38.97263485451507 :)\n",
            "The loss function for the iteration 7200----->38.97189286058227 :)\n",
            "The loss function for the iteration 7210----->38.971154315049986 :)\n",
            "The loss function for the iteration 7220----->38.97041919317732 :)\n",
            "The loss function for the iteration 7230----->38.969687470421675 :)\n",
            "The loss function for the iteration 7240----->38.96895912243709 :)\n",
            "The loss function for the iteration 7250----->38.96823412507264 :)\n",
            "The loss function for the iteration 7260----->38.96751245437086 :)\n",
            "The loss function for the iteration 7270----->38.96679408656615 :)\n",
            "The loss function for the iteration 7280----->38.96607899808317 :)\n",
            "The loss function for the iteration 7290----->38.96536716553534 :)\n",
            "The loss function for the iteration 7300----->38.96465856572324 :)\n",
            "The loss function for the iteration 7310----->38.96395317563314 :)\n",
            "The loss function for the iteration 7320----->38.963250972435425 :)\n",
            "The loss function for the iteration 7330----->38.96255193348307 :)\n",
            "The loss function for the iteration 7340----->38.96185603631023 :)\n",
            "The loss function for the iteration 7350----->38.9611632586307 :)\n",
            "The loss function for the iteration 7360----->38.96047357833644 :)\n",
            "The loss function for the iteration 7370----->38.95978697349611 :)\n",
            "The loss function for the iteration 7380----->38.95910342235369 :)\n",
            "The loss function for the iteration 7390----->38.95842290332694 :)\n",
            "The loss function for the iteration 7400----->38.9577453950061 :)\n",
            "The loss function for the iteration 7410----->38.95707087615234 :)\n",
            "The loss function for the iteration 7420----->38.9563993256965 :)\n",
            "The loss function for the iteration 7430----->38.95573072273759 :)\n",
            "The loss function for the iteration 7440----->38.955065046541485 :)\n",
            "The loss function for the iteration 7450----->38.95440227653948 :)\n",
            "The loss function for the iteration 7460----->38.953742392327094 :)\n",
            "The loss function for the iteration 7470----->38.953085373662525 :)\n",
            "The loss function for the iteration 7480----->38.95243120046546 :)\n",
            "The loss function for the iteration 7490----->38.951779852815704 :)\n",
            "The loss function for the iteration 7500----->38.951131310951865 :)\n",
            "The loss function for the iteration 7510----->38.95048555527006 :)\n",
            "The loss function for the iteration 7520----->38.949842566322666 :)\n",
            "The loss function for the iteration 7530----->38.94920232481697 :)\n",
            "The loss function for the iteration 7540----->38.94856481161395 :)\n",
            "The loss function for the iteration 7550----->38.94793000772702 :)\n",
            "The loss function for the iteration 7560----->38.94729789432076 :)\n",
            "The loss function for the iteration 7570----->38.94666845270968 :)\n",
            "The loss function for the iteration 7580----->38.946041664357 :)\n",
            "The loss function for the iteration 7590----->38.94541751087346 :)\n",
            "The loss function for the iteration 7600----->38.944795974016074 :)\n",
            "The loss function for the iteration 7610----->38.944177035686955 :)\n",
            "The loss function for the iteration 7620----->38.943560677932126 :)\n",
            "The loss function for the iteration 7630----->38.942946882940326 :)\n",
            "The loss function for the iteration 7640----->38.94233563304187 :)\n",
            "The loss function for the iteration 7650----->38.94172691070748 :)\n",
            "The loss function for the iteration 7660----->38.94112069854711 :)\n",
            "The loss function for the iteration 7670----->38.94051697930888 :)\n",
            "The loss function for the iteration 7680----->38.93991573587786 :)\n",
            "The loss function for the iteration 7690----->38.939316951274996 :)\n",
            "The loss function for the iteration 7700----->38.93872060865602 :)\n",
            "The loss function for the iteration 7710----->38.938126691310295 :)\n",
            "The loss function for the iteration 7720----->38.93753518265976 :)\n",
            "The loss function for the iteration 7730----->38.936946066257825 :)\n",
            "The loss function for the iteration 7740----->38.9363593257883 :)\n",
            "The loss function for the iteration 7750----->38.93577494506438 :)\n",
            "The loss function for the iteration 7760----->38.935192908027496 :)\n",
            "The loss function for the iteration 7770----->38.934613198746334 :)\n",
            "The loss function for the iteration 7780----->38.934035801415774 :)\n",
            "The loss function for the iteration 7790----->38.933460700355866 :)\n",
            "The loss function for the iteration 7800----->38.93288788001079 :)\n",
            "The loss function for the iteration 7810----->38.93231732494786 :)\n",
            "The loss function for the iteration 7820----->38.93174901985652 :)\n",
            "The loss function for the iteration 7830----->38.93118294954731 :)\n",
            "The loss function for the iteration 7840----->38.930619098950956 :)\n",
            "The loss function for the iteration 7850----->38.93005745311728 :)\n",
            "The loss function for the iteration 7860----->38.92949799721431 :)\n",
            "The loss function for the iteration 7870----->38.928940716527315 :)\n",
            "The loss function for the iteration 7880----->38.928385596457744 :)\n",
            "The loss function for the iteration 7890----->38.92783262252243 :)\n",
            "The loss function for the iteration 7900----->38.927281780352544 :)\n",
            "The loss function for the iteration 7910----->38.926733055692694 :)\n",
            "The loss function for the iteration 7920----->38.926186434399966 :)\n",
            "The loss function for the iteration 7930----->38.925641902443076 :)\n",
            "The loss function for the iteration 7940----->38.925099445901374 :)\n",
            "The loss function for the iteration 7950----->38.92455905096404 :)\n",
            "The loss function for the iteration 7960----->38.924020703929045 :)\n",
            "The loss function for the iteration 7970----->38.92348439120243 :)\n",
            "The loss function for the iteration 7980----->38.92295009929728 :)\n",
            "The loss function for the iteration 7990----->38.922417814832954 :)\n",
            "The loss function for the iteration 8000----->38.92188752453414 :)\n",
            "The loss function for the iteration 8010----->38.921359215230055 :)\n",
            "The loss function for the iteration 8020----->38.92083287385357 :)\n",
            "The loss function for the iteration 8030----->38.92030848744034 :)\n",
            "The loss function for the iteration 8040----->38.91978604312799 :)\n",
            "The loss function for the iteration 8050----->38.9192655281553 :)\n",
            "The loss function for the iteration 8060----->38.91874692986133 :)\n",
            "The loss function for the iteration 8070----->38.91823023568465 :)\n",
            "The loss function for the iteration 8080----->38.91771543316251 :)\n",
            "The loss function for the iteration 8090----->38.91720250993003 :)\n",
            "The loss function for the iteration 8100----->38.91669145371936 :)\n",
            "The loss function for the iteration 8110----->38.916182252359036 :)\n",
            "The loss function for the iteration 8120----->38.915674893772994 :)\n",
            "The loss function for the iteration 8130----->38.91516936597994 :)\n",
            "The loss function for the iteration 8140----->38.914665657092506 :)\n",
            "The loss function for the iteration 8150----->38.91416375531651 :)\n",
            "The loss function for the iteration 8160----->38.91366364895019 :)\n",
            "The loss function for the iteration 8170----->38.91316532638346 :)\n",
            "The loss function for the iteration 8180----->38.91266877609715 :)\n",
            "The loss function for the iteration 8190----->38.91217398666224 :)\n",
            "The loss function for the iteration 8200----->38.911680946739196 :)\n",
            "The loss function for the iteration 8210----->38.91118964507717 :)\n",
            "The loss function for the iteration 8220----->38.910700070513336 :)\n",
            "The loss function for the iteration 8230----->38.91021221197208 :)\n",
            "The loss function for the iteration 8240----->38.909726058464415 :)\n",
            "The loss function for the iteration 8250----->38.90924159908717 :)\n",
            "The loss function for the iteration 8260----->38.908758823022346 :)\n",
            "The loss function for the iteration 8270----->38.908277719536414 :)\n",
            "The loss function for the iteration 8280----->38.90779827797961 :)\n",
            "The loss function for the iteration 8290----->38.907320487785235 :)\n",
            "The loss function for the iteration 8300----->38.90684433846907 :)\n",
            "The loss function for the iteration 8310----->38.90636981962858 :)\n",
            "The loss function for the iteration 8320----->38.90589692094235 :)\n",
            "The loss function for the iteration 8330----->38.905425632169326 :)\n",
            "The loss function for the iteration 8340----->38.90495594314829 :)\n",
            "The loss function for the iteration 8350----->38.904487843797064 :)\n",
            "The loss function for the iteration 8360----->38.904021324111994 :)\n",
            "The loss function for the iteration 8370----->38.903556374167245 :)\n",
            "The loss function for the iteration 8380----->38.90309298411414 :)\n",
            "The loss function for the iteration 8390----->38.9026311441806 :)\n",
            "The loss function for the iteration 8400----->38.90217084467051 :)\n",
            "The loss function for the iteration 8410----->38.90171207596302 :)\n",
            "The loss function for the iteration 8420----->38.901254828512045 :)\n",
            "The loss function for the iteration 8430----->38.900799092845595 :)\n",
            "The loss function for the iteration 8440----->38.90034485956516 :)\n",
            "The loss function for the iteration 8450----->38.89989211934513 :)\n",
            "The loss function for the iteration 8460----->38.89944086293226 :)\n",
            "The loss function for the iteration 8470----->38.89899108114494 :)\n",
            "The loss function for the iteration 8480----->38.89854276487278 :)\n",
            "The loss function for the iteration 8490----->38.89809590507588 :)\n",
            "The loss function for the iteration 8500----->38.89765049278437 :)\n",
            "The loss function for the iteration 8510----->38.89720651909777 :)\n",
            "The loss function for the iteration 8520----->38.89676397518448 :)\n",
            "The loss function for the iteration 8530----->38.89632285228115 :)\n",
            "The loss function for the iteration 8540----->38.89588314169221 :)\n",
            "The loss function for the iteration 8550----->38.89544483478922 :)\n",
            "The loss function for the iteration 8560----->38.89500792301043 :)\n",
            "The loss function for the iteration 8570----->38.89457239786016 :)\n",
            "The loss function for the iteration 8580----->38.89413825090829 :)\n",
            "The loss function for the iteration 8590----->38.89370547378972 :)\n",
            "The loss function for the iteration 8600----->38.89327405820388 :)\n",
            "The loss function for the iteration 8610----->38.892843995914134 :)\n",
            "The loss function for the iteration 8620----->38.892415278747315 :)\n",
            "The loss function for the iteration 8630----->38.89198789859317 :)\n",
            "The loss function for the iteration 8640----->38.891561847403935 :)\n",
            "The loss function for the iteration 8650----->38.89113711719369 :)\n",
            "The loss function for the iteration 8660----->38.89071370003799 :)\n",
            "The loss function for the iteration 8670----->38.89029158807325 :)\n",
            "The loss function for the iteration 8680----->38.88987077349636 :)\n",
            "The loss function for the iteration 8690----->38.8894512485641 :)\n",
            "The loss function for the iteration 8700----->38.88903300559274 :)\n",
            "The loss function for the iteration 8710----->38.88861603695744 :)\n",
            "The loss function for the iteration 8720----->38.888200335091916 :)\n",
            "The loss function for the iteration 8730----->38.887785892487855 :)\n",
            "The loss function for the iteration 8740----->38.88737270169449 :)\n",
            "The loss function for the iteration 8750----->38.88696075531811 :)\n",
            "The loss function for the iteration 8760----->38.886550046021625 :)\n",
            "The loss function for the iteration 8770----->38.886140566524084 :)\n",
            "The loss function for the iteration 8780----->38.88573230960023 :)\n",
            "The loss function for the iteration 8790----->38.88532526808005 :)\n",
            "The loss function for the iteration 8800----->38.884919434848314 :)\n",
            "The loss function for the iteration 8810----->38.884514802844144 :)\n",
            "The loss function for the iteration 8820----->38.884111365060555 :)\n",
            "The loss function for the iteration 8830----->38.88370911454405 :)\n",
            "The loss function for the iteration 8840----->38.883308044394155 :)\n",
            "The loss function for the iteration 8850----->38.882908147763004 :)\n",
            "The loss function for the iteration 8860----->38.88250941785489 :)\n",
            "The loss function for the iteration 8870----->38.8821118479259 :)\n",
            "The loss function for the iteration 8880----->38.88171543128341 :)\n",
            "The loss function for the iteration 8890----->38.88132016128575 :)\n",
            "The loss function for the iteration 8900----->38.88092603134175 :)\n",
            "The loss function for the iteration 8910----->38.88053303491031 :)\n",
            "The loss function for the iteration 8920----->38.88014116550004 :)\n",
            "The loss function for the iteration 8930----->38.879750416668855 :)\n",
            "The loss function for the iteration 8940----->38.87936078202353 :)\n",
            "The loss function for the iteration 8950----->38.87897225521935 :)\n",
            "The loss function for the iteration 8960----->38.878584829959685 :)\n",
            "The loss function for the iteration 8970----->38.878198499995634 :)\n",
            "The loss function for the iteration 8980----->38.87781325912558 :)\n",
            "The loss function for the iteration 8990----->38.877429101194906 :)\n",
            "The loss function for the iteration 9000----->38.87704602009549 :)\n",
            "The loss function for the iteration 9010----->38.87666400976546 :)\n",
            "The loss function for the iteration 9020----->38.87628306418869 :)\n",
            "The loss function for the iteration 9030----->38.875903177394534 :)\n",
            "The loss function for the iteration 9040----->38.8755243434574 :)\n",
            "The loss function for the iteration 9050----->38.875146556496404 :)\n",
            "The loss function for the iteration 9060----->38.874769810674984 :)\n",
            "The loss function for the iteration 9070----->38.87439410020062 :)\n",
            "The loss function for the iteration 9080----->38.87401941932437 :)\n",
            "The loss function for the iteration 9090----->38.87364576234057 :)\n",
            "The loss function for the iteration 9100----->38.87327312358651 :)\n",
            "The loss function for the iteration 9110----->38.87290149744204 :)\n",
            "The loss function for the iteration 9120----->38.872530878329236 :)\n",
            "The loss function for the iteration 9130----->38.87216126071209 :)\n",
            "The loss function for the iteration 9140----->38.87179263909616 :)\n",
            "The loss function for the iteration 9150----->38.87142500802816 :)\n",
            "The loss function for the iteration 9160----->38.87105836209574 :)\n",
            "The loss function for the iteration 9170----->38.870692695927104 :)\n",
            "The loss function for the iteration 9180----->38.87032800419067 :)\n",
            "The loss function for the iteration 9190----->38.86996428159474 :)\n",
            "The loss function for the iteration 9200----->38.86960152288722 :)\n",
            "The loss function for the iteration 9210----->38.8692397228553 :)\n",
            "The loss function for the iteration 9220----->38.86887887632504 :)\n",
            "The loss function for the iteration 9230----->38.8685189781612 :)\n",
            "The loss function for the iteration 9240----->38.86816002326682 :)\n",
            "The loss function for the iteration 9250----->38.86780200658294 :)\n",
            "The loss function for the iteration 9260----->38.86744492308833 :)\n",
            "The loss function for the iteration 9270----->38.86708876779912 :)\n",
            "The loss function for the iteration 9280----->38.86673353576857 :)\n",
            "The loss function for the iteration 9290----->38.8663792220867 :)\n",
            "The loss function for the iteration 9300----->38.86602582188006 :)\n",
            "The loss function for the iteration 9310----->38.86567333031137 :)\n",
            "The loss function for the iteration 9320----->38.8653217425793 :)\n",
            "The loss function for the iteration 9330----->38.86497105391808 :)\n",
            "The loss function for the iteration 9340----->38.86462125959735 :)\n",
            "The loss function for the iteration 9350----->38.86427235492175 :)\n",
            "The loss function for the iteration 9360----->38.86392433523068 :)\n",
            "The loss function for the iteration 9370----->38.86357719589807 :)\n",
            "The loss function for the iteration 9380----->38.863230932332016 :)\n",
            "The loss function for the iteration 9390----->38.862885539974556 :)\n",
            "The loss function for the iteration 9400----->38.86254101430141 :)\n",
            "The loss function for the iteration 9410----->38.86219735082166 :)\n",
            "The loss function for the iteration 9420----->38.86185454507751 :)\n",
            "The loss function for the iteration 9430----->38.86151259264402 :)\n",
            "The loss function for the iteration 9440----->38.86117148912885 :)\n",
            "The loss function for the iteration 9450----->38.86083123017197 :)\n",
            "The loss function for the iteration 9460----->38.86049181144542 :)\n",
            "The loss function for the iteration 9470----->38.86015322865305 :)\n",
            "The loss function for the iteration 9480----->38.85981547753023 :)\n",
            "The loss function for the iteration 9490----->38.859478553843694 :)\n",
            "The loss function for the iteration 9500----->38.85914245339113 :)\n",
            "The loss function for the iteration 9510----->38.85880717200111 :)\n",
            "The loss function for the iteration 9520----->38.85847270553269 :)\n",
            "The loss function for the iteration 9530----->38.85813904987528 :)\n",
            "The loss function for the iteration 9540----->38.85780620094829 :)\n",
            "The loss function for the iteration 9550----->38.857474154700974 :)\n",
            "The loss function for the iteration 9560----->38.85714290711219 :)\n",
            "The loss function for the iteration 9570----->38.856812454190084 :)\n",
            "The loss function for the iteration 9580----->38.856482791971956 :)\n",
            "The loss function for the iteration 9590----->38.85615391652392 :)\n",
            "The loss function for the iteration 9600----->38.855825823940776 :)\n",
            "The loss function for the iteration 9610----->38.8554985103457 :)\n",
            "The loss function for the iteration 9620----->38.85517197189007 :)\n",
            "The loss function for the iteration 9630----->38.8548462047532 :)\n",
            "The loss function for the iteration 9640----->38.854521205142134 :)\n",
            "The loss function for the iteration 9650----->38.85419696929142 :)\n",
            "The loss function for the iteration 9660----->38.853873493462935 :)\n",
            "The loss function for the iteration 9670----->38.85355077394552 :)\n",
            "The loss function for the iteration 9680----->38.85322880705498 :)\n",
            "The loss function for the iteration 9690----->38.85290758913369 :)\n",
            "The loss function for the iteration 9700----->38.852587116550446 :)\n",
            "The loss function for the iteration 9710----->38.85226738570025 :)\n",
            "The loss function for the iteration 9720----->38.85194839300415 :)\n",
            "The loss function for the iteration 9730----->38.85163013490892 :)\n",
            "The loss function for the iteration 9740----->38.85131260788695 :)\n",
            "The loss function for the iteration 9750----->38.85099580843601 :)\n",
            "The loss function for the iteration 9760----->38.85067973307903 :)\n",
            "The loss function for the iteration 9770----->38.85036437836391 :)\n",
            "The loss function for the iteration 9780----->38.850049740863355 :)\n",
            "The loss function for the iteration 9790----->38.8497358171746 :)\n",
            "The loss function for the iteration 9800----->38.84942260391928 :)\n",
            "The loss function for the iteration 9810----->38.849110097743214 :)\n",
            "The loss function for the iteration 9820----->38.8487982953162 :)\n",
            "The loss function for the iteration 9830----->38.848487193331856 :)\n",
            "The loss function for the iteration 9840----->38.848176788507374 :)\n",
            "The loss function for the iteration 9850----->38.84786707758337 :)\n",
            "The loss function for the iteration 9860----->38.847558057323724 :)\n",
            "The loss function for the iteration 9870----->38.84724972451531 :)\n",
            "The loss function for the iteration 9880----->38.8469420759679 :)\n",
            "The loss function for the iteration 9890----->38.84663510851393 :)\n",
            "The loss function for the iteration 9900----->38.84632881900833 :)\n",
            "The loss function for the iteration 9910----->38.84602320432835 :)\n",
            "The loss function for the iteration 9920----->38.845718261373385 :)\n",
            "The loss function for the iteration 9930----->38.845413987064774 :)\n",
            "The loss function for the iteration 9940----->38.84511037834566 :)\n",
            "The loss function for the iteration 9950----->38.84480743218077 :)\n",
            "The loss function for the iteration 9960----->38.844505145556326 :)\n",
            "The loss function for the iteration 9970----->38.84420351547976 :)\n",
            "The loss function for the iteration 9980----->38.84390253897963 :)\n",
            "The loss function for the iteration 9990----->38.84360221310542 :)\n",
            "The loss function for the iteration 10000----->38.84330253492737 :)\n",
            "The loss function for the iteration 10010----->38.84300350153634 :)\n",
            "The loss function for the iteration 10020----->38.84270511004361 :)\n",
            "The loss function for the iteration 10030----->38.84240735758072 :)\n",
            "The loss function for the iteration 10040----->38.842110241299316 :)\n",
            "The loss function for the iteration 10050----->38.84181375837104 :)\n",
            "The loss function for the iteration 10060----->38.84151790598727 :)\n",
            "The loss function for the iteration 10070----->38.84122268135904 :)\n",
            "The loss function for the iteration 10080----->38.84092808171686 :)\n",
            "The loss function for the iteration 10090----->38.84063410431056 :)\n",
            "The loss function for the iteration 10100----->38.840340746409126 :)\n",
            "The loss function for the iteration 10110----->38.84004800530059 :)\n",
            "The loss function for the iteration 10120----->38.8397558782918 :)\n",
            "The loss function for the iteration 10130----->38.83946436270834 :)\n",
            "The loss function for the iteration 10140----->38.83917345589441 :)\n",
            "The loss function for the iteration 10150----->38.83888315521254 :)\n",
            "The loss function for the iteration 10160----->38.838593458043576 :)\n",
            "The loss function for the iteration 10170----->38.83830436178649 :)\n",
            "The loss function for the iteration 10180----->38.838015863858224 :)\n",
            "The loss function for the iteration 10190----->38.83772796169359 :)\n",
            "The loss function for the iteration 10200----->38.83744065274502 :)\n",
            "The loss function for the iteration 10210----->38.83715393448261 :)\n",
            "The loss function for the iteration 10220----->38.836867804393776 :)\n",
            "The loss function for the iteration 10230----->38.83658225998328 :)\n",
            "The loss function for the iteration 10240----->38.83629729877299 :)\n",
            "The loss function for the iteration 10250----->38.836012918301805 :)\n",
            "The loss function for the iteration 10260----->38.83572911612546 :)\n",
            "The loss function for the iteration 10270----->38.835445889816455 :)\n",
            "The loss function for the iteration 10280----->38.83516323696391 :)\n",
            "The loss function for the iteration 10290----->38.83488115517335 :)\n",
            "The loss function for the iteration 10300----->38.83459964206672 :)\n",
            "The loss function for the iteration 10310----->38.83431869528216 :)\n",
            "The loss function for the iteration 10320----->38.83403831247387 :)\n",
            "The loss function for the iteration 10330----->38.83375849131201 :)\n",
            "The loss function for the iteration 10340----->38.83347922948258 :)\n",
            "The loss function for the iteration 10350----->38.83320052468731 :)\n",
            "The loss function for the iteration 10360----->38.83292237464346 :)\n",
            "The loss function for the iteration 10370----->38.83264477708381 :)\n",
            "The loss function for the iteration 10380----->38.83236772975641 :)\n",
            "The loss function for the iteration 10390----->38.83209123042459 :)\n",
            "The loss function for the iteration 10400----->38.83181527686672 :)\n",
            "The loss function for the iteration 10410----->38.83153986687619 :)\n",
            "The loss function for the iteration 10420----->38.83126499826122 :)\n",
            "The loss function for the iteration 10430----->38.83099066884478 :)\n",
            "The loss function for the iteration 10440----->38.83071687646445 :)\n",
            "The loss function for the iteration 10450----->38.830443618972346 :)\n",
            "The loss function for the iteration 10460----->38.83017089423497 :)\n",
            "The loss function for the iteration 10470----->38.82989870013306 :)\n",
            "The loss function for the iteration 10480----->38.82962703456159 :)\n",
            "The loss function for the iteration 10490----->38.829355895429565 :)\n",
            "The loss function for the iteration 10500----->38.829085280659896 :)\n",
            "The loss function for the iteration 10510----->38.8288151881894 :)\n",
            "The loss function for the iteration 10520----->38.82854561596856 :)\n",
            "The loss function for the iteration 10530----->38.828276561961516 :)\n",
            "The loss function for the iteration 10540----->38.8280080241459 :)\n",
            "The loss function for the iteration 10550----->38.82774000051275 :)\n",
            "The loss function for the iteration 10560----->38.8274724890664 :)\n",
            "The loss function for the iteration 10570----->38.827205487824415 :)\n",
            "The loss function for the iteration 10580----->38.826938994817404 :)\n",
            "The loss function for the iteration 10590----->38.82667300808898 :)\n",
            "The loss function for the iteration 10600----->38.82640752569567 :)\n",
            "The loss function for the iteration 10610----->38.82614254570672 :)\n",
            "The loss function for the iteration 10620----->38.825878066204154 :)\n",
            "The loss function for the iteration 10630----->38.825614085282496 :)\n",
            "The loss function for the iteration 10640----->38.82535060104882 :)\n",
            "The loss function for the iteration 10650----->38.825087611622536 :)\n",
            "The loss function for the iteration 10660----->38.82482511513536 :)\n",
            "The loss function for the iteration 10670----->38.824563109731244 :)\n",
            "The loss function for the iteration 10680----->38.824301593566204 :)\n",
            "The loss function for the iteration 10690----->38.82404056480824 :)\n",
            "The loss function for the iteration 10700----->38.82378002163732 :)\n",
            "The loss function for the iteration 10710----->38.82351996224518 :)\n",
            "The loss function for the iteration 10720----->38.823260384835294 :)\n",
            "The loss function for the iteration 10730----->38.82300128762279 :)\n",
            "The loss function for the iteration 10740----->38.822742668834294 :)\n",
            "The loss function for the iteration 10750----->38.822484526707946 :)\n",
            "The loss function for the iteration 10760----->38.82222685949319 :)\n",
            "The loss function for the iteration 10770----->38.82196966545078 :)\n",
            "The loss function for the iteration 10780----->38.82171294285264 :)\n",
            "The loss function for the iteration 10790----->38.8214566899818 :)\n",
            "The loss function for the iteration 10800----->38.821200905132294 :)\n",
            "The loss function for the iteration 10810----->38.820945586609085 :)\n",
            "The loss function for the iteration 10820----->38.82069073272797 :)\n",
            "The loss function for the iteration 10830----->38.82043634181553 :)\n",
            "The loss function for the iteration 10840----->38.82018241220898 :)\n",
            "The loss function for the iteration 10850----->38.819928942256134 :)\n",
            "The loss function for the iteration 10860----->38.81967593031534 :)\n",
            "The loss function for the iteration 10870----->38.81942337475535 :)\n",
            "The loss function for the iteration 10880----->38.81917127395524 :)\n",
            "The loss function for the iteration 10890----->38.8189196263044 :)\n",
            "The loss function for the iteration 10900----->38.81866843020233 :)\n",
            "The loss function for the iteration 10910----->38.81841768405871 :)\n",
            "The loss function for the iteration 10920----->38.81816738629321 :)\n",
            "The loss function for the iteration 10930----->38.81791753533542 :)\n",
            "The loss function for the iteration 10940----->38.81766812962487 :)\n",
            "The loss function for the iteration 10950----->38.81741916761082 :)\n",
            "The loss function for the iteration 10960----->38.81717064775227 :)\n",
            "The loss function for the iteration 10970----->38.81692256851788 :)\n",
            "The loss function for the iteration 10980----->38.81667492838584 :)\n",
            "The loss function for the iteration 10990----->38.81642772584387 :)\n",
            "The loss function for the iteration 11000----->38.816180959389094 :)\n",
            "The loss function for the iteration 11010----->38.81593462752798 :)\n",
            "The loss function for the iteration 11020----->38.81568872877627 :)\n",
            "The loss function for the iteration 11030----->38.81544326165891 :)\n",
            "The loss function for the iteration 11040----->38.815198224709995 :)\n",
            "The loss function for the iteration 11050----->38.81495361647265 :)\n",
            "The loss function for the iteration 11060----->38.81470943549902 :)\n",
            "The loss function for the iteration 11070----->38.81446568035015 :)\n",
            "The loss function for the iteration 11080----->38.81422234959595 :)\n",
            "The loss function for the iteration 11090----->38.813979441815114 :)\n",
            "The loss function for the iteration 11100----->38.813736955595026 :)\n",
            "The loss function for the iteration 11110----->38.81349488953177 :)\n",
            "The loss function for the iteration 11120----->38.81325324222996 :)\n",
            "The loss function for the iteration 11130----->38.81301201230276 :)\n",
            "The loss function for the iteration 11140----->38.81277119837177 :)\n",
            "The loss function for the iteration 11150----->38.81253079906699 :)\n",
            "The loss function for the iteration 11160----->38.81229081302673 :)\n",
            "The loss function for the iteration 11170----->38.81205123889754 :)\n",
            "The loss function for the iteration 11180----->38.811812075334196 :)\n",
            "The loss function for the iteration 11190----->38.811573320999585 :)\n",
            "The loss function for the iteration 11200----->38.811334974564666 :)\n",
            "The loss function for the iteration 11210----->38.811097034708396 :)\n",
            "The loss function for the iteration 11220----->38.8108595001177 :)\n",
            "The loss function for the iteration 11230----->38.81062236948737 :)\n",
            "The loss function for the iteration 11240----->38.810385641520014 :)\n",
            "The loss function for the iteration 11250----->38.810149314925994 :)\n",
            "The loss function for the iteration 11260----->38.80991338842344 :)\n",
            "The loss function for the iteration 11270----->38.80967786073804 :)\n",
            "The loss function for the iteration 11280----->38.80944273060311 :)\n",
            "The loss function for the iteration 11290----->38.80920799675951 :)\n",
            "The loss function for the iteration 11300----->38.808973657955534 :)\n",
            "The loss function for the iteration 11310----->38.80873971294691 :)\n",
            "The loss function for the iteration 11320----->38.80850616049673 :)\n",
            "The loss function for the iteration 11330----->38.80827299937536 :)\n",
            "The loss function for the iteration 11340----->38.808040228360426 :)\n",
            "The loss function for the iteration 11350----->38.80780784623676 :)\n",
            "The loss function for the iteration 11360----->38.807575851796294 :)\n",
            "The loss function for the iteration 11370----->38.80734424383807 :)\n",
            "The loss function for the iteration 11380----->38.807113021168135 :)\n",
            "The loss function for the iteration 11390----->38.8068821825995 :)\n",
            "The loss function for the iteration 11400----->38.806651726952154 :)\n",
            "The loss function for the iteration 11410----->38.80642165305288 :)\n",
            "The loss function for the iteration 11420----->38.80619195973531 :)\n",
            "The loss function for the iteration 11430----->38.805962645839855 :)\n",
            "The loss function for the iteration 11440----->38.80573371021359 :)\n",
            "The loss function for the iteration 11450----->38.805505151710285 :)\n",
            "The loss function for the iteration 11460----->38.80527696919034 :)\n",
            "The loss function for the iteration 11470----->38.80504916152065 :)\n",
            "The loss function for the iteration 11480----->38.804821727574684 :)\n",
            "The loss function for the iteration 11490----->38.80459466623233 :)\n",
            "The loss function for the iteration 11500----->38.804367976379915 :)\n",
            "The loss function for the iteration 11510----->38.8041416569101 :)\n",
            "The loss function for the iteration 11520----->38.803915706721895 :)\n",
            "The loss function for the iteration 11530----->38.803690124720546 :)\n",
            "The loss function for the iteration 11540----->38.80346490981756 :)\n",
            "The loss function for the iteration 11550----->38.80324006093055 :)\n",
            "The loss function for the iteration 11560----->38.80301557698331 :)\n",
            "The loss function for the iteration 11570----->38.80279145690572 :)\n",
            "The loss function for the iteration 11580----->38.80256769963364 :)\n",
            "The loss function for the iteration 11590----->38.80234430410897 :)\n",
            "The loss function for the iteration 11600----->38.80212126927954 :)\n",
            "The loss function for the iteration 11610----->38.80189859409907 :)\n",
            "The loss function for the iteration 11620----->38.80167627752711 :)\n",
            "The loss function for the iteration 11630----->38.80145431852909 :)\n",
            "The loss function for the iteration 11640----->38.80123271607614 :)\n",
            "The loss function for the iteration 11650----->38.80101146914515 :)\n",
            "The loss function for the iteration 11660----->38.80079057671867 :)\n",
            "The loss function for the iteration 11670----->38.8005700377849 :)\n",
            "The loss function for the iteration 11680----->38.800349851337636 :)\n",
            "The loss function for the iteration 11690----->38.80013001637621 :)\n",
            "The loss function for the iteration 11700----->38.7999105319055 :)\n",
            "The loss function for the iteration 11710----->38.79969139693581 :)\n",
            "The loss function for the iteration 11720----->38.79947261048291 :)\n",
            "The loss function for the iteration 11730----->38.79925417156796 :)\n",
            "The loss function for the iteration 11740----->38.79903607921742 :)\n",
            "The loss function for the iteration 11750----->38.798818332463114 :)\n",
            "The loss function for the iteration 11760----->38.79860093034213 :)\n",
            "The loss function for the iteration 11770----->38.79838387189677 :)\n",
            "The loss function for the iteration 11780----->38.79816715617448 :)\n",
            "The loss function for the iteration 11790----->38.79795078222796 :)\n",
            "The loss function for the iteration 11800----->38.797734749114944 :)\n",
            "The loss function for the iteration 11810----->38.797519055898285 :)\n",
            "The loss function for the iteration 11820----->38.79730370164584 :)\n",
            "The loss function for the iteration 11830----->38.79708868543048 :)\n",
            "The loss function for the iteration 11840----->38.796874006330064 :)\n",
            "The loss function for the iteration 11850----->38.79665966342735 :)\n",
            "The loss function for the iteration 11860----->38.796445655809976 :)\n",
            "The loss function for the iteration 11870----->38.796231982570475 :)\n",
            "The loss function for the iteration 11880----->38.79601864280616 :)\n",
            "The loss function for the iteration 11890----->38.79580563561913 :)\n",
            "The loss function for the iteration 11900----->38.79559296011628 :)\n",
            "The loss function for the iteration 11910----->38.79538061540914 :)\n",
            "The loss function for the iteration 11920----->38.79516860061394 :)\n",
            "The loss function for the iteration 11930----->38.7949569148516 :)\n",
            "The loss function for the iteration 11940----->38.794745557247616 :)\n",
            "The loss function for the iteration 11950----->38.794534526932054 :)\n",
            "The loss function for the iteration 11960----->38.794323823039505 :)\n",
            "The loss function for the iteration 11970----->38.7941134447091 :)\n",
            "The loss function for the iteration 11980----->38.79390339108443 :)\n",
            "The loss function for the iteration 11990----->38.79369366131351 :)\n",
            "The loss function for the iteration 12000----->38.7934842545488 :)\n",
            "The loss function for the iteration 12010----->38.79327516994712 :)\n",
            "The loss function for the iteration 12020----->38.79306640666961 :)\n",
            "The loss function for the iteration 12030----->38.79285796388174 :)\n",
            "The loss function for the iteration 12040----->38.792649840753285 :)\n",
            "The loss function for the iteration 12050----->38.792442036458226 :)\n",
            "The loss function for the iteration 12060----->38.79223455017479 :)\n",
            "The loss function for the iteration 12070----->38.79202738108538 :)\n",
            "The loss function for the iteration 12080----->38.79182052837657 :)\n",
            "The loss function for the iteration 12090----->38.79161399123903 :)\n",
            "The loss function for the iteration 12100----->38.79140776886756 :)\n",
            "The loss function for the iteration 12110----->38.79120186046101 :)\n",
            "The loss function for the iteration 12120----->38.79099626522226 :)\n",
            "The loss function for the iteration 12130----->38.79079098235821 :)\n",
            "The loss function for the iteration 12140----->38.79058601107973 :)\n",
            "The loss function for the iteration 12150----->38.790381350601656 :)\n",
            "The loss function for the iteration 12160----->38.79017700014272 :)\n",
            "The loss function for the iteration 12170----->38.78997295892555 :)\n",
            "The loss function for the iteration 12180----->38.78976922617666 :)\n",
            "The loss function for the iteration 12190----->38.78956580112637 :)\n",
            "The loss function for the iteration 12200----->38.789362683008854 :)\n",
            "The loss function for the iteration 12210----->38.78915987106199 :)\n",
            "The loss function for the iteration 12220----->38.78895736452749 :)\n",
            "The loss function for the iteration 12230----->38.78875516265073 :)\n",
            "The loss function for the iteration 12240----->38.78855326468084 :)\n",
            "The loss function for the iteration 12250----->38.78835166987058 :)\n",
            "The loss function for the iteration 12260----->38.78815037747637 :)\n",
            "The loss function for the iteration 12270----->38.78794938675826 :)\n",
            "The loss function for the iteration 12280----->38.78774869697986 :)\n",
            "The loss function for the iteration 12290----->38.7875483074084 :)\n",
            "The loss function for the iteration 12300----->38.78734821731461 :)\n",
            "The loss function for the iteration 12310----->38.78714842597276 :)\n",
            "The loss function for the iteration 12320----->38.78694893266059 :)\n",
            "The loss function for the iteration 12330----->38.786749736659324 :)\n",
            "The loss function for the iteration 12340----->38.786550837253635 :)\n",
            "The loss function for the iteration 12350----->38.78635223373159 :)\n",
            "The loss function for the iteration 12360----->38.78615392538466 :)\n",
            "The loss function for the iteration 12370----->38.785955911507706 :)\n",
            "The loss function for the iteration 12380----->38.785758191398884 :)\n",
            "The loss function for the iteration 12390----->38.78556076435973 :)\n",
            "The loss function for the iteration 12400----->38.78536362969502 :)\n",
            "The loss function for the iteration 12410----->38.78516678671284 :)\n",
            "The loss function for the iteration 12420----->38.78497023472452 :)\n",
            "The loss function for the iteration 12430----->38.7847739730446 :)\n",
            "The loss function for the iteration 12440----->38.784578000990855 :)\n",
            "The loss function for the iteration 12450----->38.78438231788424 :)\n",
            "The loss function for the iteration 12460----->38.78418692304881 :)\n",
            "The loss function for the iteration 12470----->38.78399181581184 :)\n",
            "The loss function for the iteration 12480----->38.783796995503664 :)\n",
            "The loss function for the iteration 12490----->38.78360246145772 :)\n",
            "The loss function for the iteration 12500----->38.78340821301054 :)\n",
            "The loss function for the iteration 12510----->38.78321424950169 :)\n",
            "The loss function for the iteration 12520----->38.78302057027374 :)\n",
            "The loss function for the iteration 12530----->38.78282717467229 :)\n",
            "The loss function for the iteration 12540----->38.78263406204593 :)\n",
            "The loss function for the iteration 12550----->38.78244123174621 :)\n",
            "The loss function for the iteration 12560----->38.78224868312762 :)\n",
            "The loss function for the iteration 12570----->38.78205641554757 :)\n",
            "The loss function for the iteration 12580----->38.78186442836639 :)\n",
            "The loss function for the iteration 12590----->38.78167272094724 :)\n",
            "The loss function for the iteration 12600----->38.781481292656196 :)\n",
            "The loss function for the iteration 12610----->38.78129014286218 :)\n",
            "The loss function for the iteration 12620----->38.78109927093691 :)\n",
            "The loss function for the iteration 12630----->38.780908676254896 :)\n",
            "The loss function for the iteration 12640----->38.78071835819345 :)\n",
            "The loss function for the iteration 12650----->38.78052831613264 :)\n",
            "The loss function for the iteration 12660----->38.780338549455294 :)\n",
            "The loss function for the iteration 12670----->38.780149057546964 :)\n",
            "The loss function for the iteration 12680----->38.779959839795886 :)\n",
            "The loss function for the iteration 12690----->38.779770895592975 :)\n",
            "The loss function for the iteration 12700----->38.779582224331875 :)\n",
            "The loss function for the iteration 12710----->38.779393825408796 :)\n",
            "The loss function for the iteration 12720----->38.77920569822268 :)\n",
            "The loss function for the iteration 12730----->38.779017842174994 :)\n",
            "The loss function for the iteration 12740----->38.77883025666983 :)\n",
            "The loss function for the iteration 12750----->38.77864294111389 :)\n",
            "The loss function for the iteration 12760----->38.77845589491639 :)\n",
            "The loss function for the iteration 12770----->38.778269117489124 :)\n",
            "The loss function for the iteration 12780----->38.77808260824638 :)\n",
            "The loss function for the iteration 12790----->38.77789636660499 :)\n",
            "The loss function for the iteration 12800----->38.777710391984236 :)\n",
            "The loss function for the iteration 12810----->38.77752468380593 :)\n",
            "The loss function for the iteration 12820----->38.77733924149425 :)\n",
            "The loss function for the iteration 12830----->38.777154064475944 :)\n",
            "The loss function for the iteration 12840----->38.776969152180065 :)\n",
            "The loss function for the iteration 12850----->38.77678450403811 :)\n",
            "The loss function for the iteration 12860----->38.77660011948402 :)\n",
            "The loss function for the iteration 12870----->38.77641599795403 :)\n",
            "The loss function for the iteration 12880----->38.776232138886776 :)\n",
            "The loss function for the iteration 12890----->38.776048541723235 :)\n",
            "The loss function for the iteration 12900----->38.775865205906705 :)\n",
            "The loss function for the iteration 12910----->38.775682130882785 :)\n",
            "The loss function for the iteration 12920----->38.77549931609939 :)\n",
            "The loss function for the iteration 12930----->38.7753167610067 :)\n",
            "The loss function for the iteration 12940----->38.77513446505718 :)\n",
            "The loss function for the iteration 12950----->38.77495242770547 :)\n",
            "The loss function for the iteration 12960----->38.77477064840858 :)\n",
            "The loss function for the iteration 12970----->38.77458912662559 :)\n",
            "The loss function for the iteration 12980----->38.77440786181791 :)\n",
            "The loss function for the iteration 12990----->38.77422685344905 :)\n",
            "The loss function for the iteration 13000----->38.77404610098473 :)\n",
            "The loss function for the iteration 13010----->38.77386560389284 :)\n",
            "The loss function for the iteration 13020----->38.77368536164338 :)\n",
            "The loss function for the iteration 13030----->38.77350537370853 :)\n",
            "The loss function for the iteration 13040----->38.77332563956256 :)\n",
            "The loss function for the iteration 13050----->38.773146158681826 :)\n",
            "The loss function for the iteration 13060----->38.77296693054479 :)\n",
            "The loss function for the iteration 13070----->38.77278795463201 :)\n",
            "The loss function for the iteration 13080----->38.77260923042608 :)\n",
            "The loss function for the iteration 13090----->38.772430757411634 :)\n",
            "The loss function for the iteration 13100----->38.77225253507538 :)\n",
            "The loss function for the iteration 13110----->38.772074562905985 :)\n",
            "The loss function for the iteration 13120----->38.77189684039419 :)\n",
            "The loss function for the iteration 13130----->38.77171936703269 :)\n",
            "The loss function for the iteration 13140----->38.77154214231616 :)\n",
            "The loss function for the iteration 13150----->38.77136516574126 :)\n",
            "The loss function for the iteration 13160----->38.771188436806604 :)\n",
            "The loss function for the iteration 13170----->38.771011955012725 :)\n",
            "The loss function for the iteration 13180----->38.770835719862134 :)\n",
            "The loss function for the iteration 13190----->38.7706597308592 :)\n",
            "The loss function for the iteration 13200----->38.77048398751023 :)\n",
            "The loss function for the iteration 13210----->38.77030848932342 :)\n",
            "The loss function for the iteration 13220----->38.77013323580886 :)\n",
            "The loss function for the iteration 13230----->38.769958226478465 :)\n",
            "The loss function for the iteration 13240----->38.769783460846035 :)\n",
            "The loss function for the iteration 13250----->38.76960893842723 :)\n",
            "The loss function for the iteration 13260----->38.769434658739506 :)\n",
            "The loss function for the iteration 13270----->38.76926062130215 :)\n",
            "The loss function for the iteration 13280----->38.76908682563626 :)\n",
            "The loss function for the iteration 13290----->38.76891327126474 :)\n",
            "The loss function for the iteration 13300----->38.768739957712256 :)\n",
            "The loss function for the iteration 13310----->38.768566884505276 :)\n",
            "The loss function for the iteration 13320----->38.768394051171995 :)\n",
            "The loss function for the iteration 13330----->38.7682214572424 :)\n",
            "The loss function for the iteration 13340----->38.76804910224817 :)\n",
            "The loss function for the iteration 13350----->38.76787698572274 :)\n",
            "The loss function for the iteration 13360----->38.76770510720127 :)\n",
            "The loss function for the iteration 13370----->38.767533466220605 :)\n",
            "The loss function for the iteration 13380----->38.76736206231929 :)\n",
            "The loss function for the iteration 13390----->38.767190895037565 :)\n",
            "The loss function for the iteration 13400----->38.767019963917356 :)\n",
            "The loss function for the iteration 13410----->38.76684926850219 :)\n",
            "The loss function for the iteration 13420----->38.76667880833733 :)\n",
            "The loss function for the iteration 13430----->38.766508582969614 :)\n",
            "The loss function for the iteration 13440----->38.766338591947545 :)\n",
            "The loss function for the iteration 13450----->38.76616883482125 :)\n",
            "The loss function for the iteration 13460----->38.765999311142444 :)\n",
            "The loss function for the iteration 13470----->38.76583002046446 :)\n",
            "The loss function for the iteration 13480----->38.76566096234224 :)\n",
            "The loss function for the iteration 13490----->38.76549213633226 :)\n",
            "The loss function for the iteration 13500----->38.76532354199259 :)\n",
            "The loss function for the iteration 13510----->38.765155178882885 :)\n",
            "The loss function for the iteration 13520----->38.76498704656431 :)\n",
            "The loss function for the iteration 13530----->38.76481914459961 :)\n",
            "The loss function for the iteration 13540----->38.764651472553034 :)\n",
            "The loss function for the iteration 13550----->38.76448402999036 :)\n",
            "The loss function for the iteration 13560----->38.76431681647891 :)\n",
            "The loss function for the iteration 13570----->38.76414983158745 :)\n",
            "The loss function for the iteration 13580----->38.76398307488628 :)\n",
            "The loss function for the iteration 13590----->38.7638165459472 :)\n",
            "The loss function for the iteration 13600----->38.76365024434346 :)\n",
            "The loss function for the iteration 13610----->38.76348416964978 :)\n",
            "The loss function for the iteration 13620----->38.76331832144232 :)\n",
            "The loss function for the iteration 13630----->38.763152699298736 :)\n",
            "The loss function for the iteration 13640----->38.76298730279809 :)\n",
            "The loss function for the iteration 13650----->38.76282213152088 :)\n",
            "The loss function for the iteration 13660----->38.76265718504903 :)\n",
            "The loss function for the iteration 13670----->38.7624924629659 :)\n",
            "The loss function for the iteration 13680----->38.762327964856205 :)\n",
            "The loss function for the iteration 13690----->38.7621636903061 :)\n",
            "The loss function for the iteration 13700----->38.76199963890311 :)\n",
            "The loss function for the iteration 13710----->38.76183581023615 :)\n",
            "The loss function for the iteration 13720----->38.76167220389549 :)\n",
            "The loss function for the iteration 13730----->38.761508819472795 :)\n",
            "The loss function for the iteration 13740----->38.76134565656105 :)\n",
            "The loss function for the iteration 13750----->38.76118271475459 :)\n",
            "The loss function for the iteration 13760----->38.761019993649114 :)\n",
            "The loss function for the iteration 13770----->38.76085749284164 :)\n",
            "The loss function for the iteration 13780----->38.76069521193051 :)\n",
            "The loss function for the iteration 13790----->38.76053315051537 :)\n",
            "The loss function for the iteration 13800----->38.76037130819718 :)\n",
            "The loss function for the iteration 13810----->38.76020968457821 :)\n",
            "The loss function for the iteration 13820----->38.760048279261994 :)\n",
            "The loss function for the iteration 13830----->38.75988709185338 :)\n",
            "The loss function for the iteration 13840----->38.75972612195849 :)\n",
            "The loss function for the iteration 13850----->38.75956536918469 :)\n",
            "The loss function for the iteration 13860----->38.75940483314063 :)\n",
            "The loss function for the iteration 13870----->38.759244513436194 :)\n",
            "The loss function for the iteration 13880----->38.75908440968254 :)\n",
            "The loss function for the iteration 13890----->38.75892452149204 :)\n",
            "The loss function for the iteration 13900----->38.758764848478314 :)\n",
            "The loss function for the iteration 13910----->38.758605390256186 :)\n",
            "The loss function for the iteration 13920----->38.75844614644174 :)\n",
            "The loss function for the iteration 13930----->38.75828711665223 :)\n",
            "The loss function for the iteration 13940----->38.75812830050609 :)\n",
            "The loss function for the iteration 13950----->38.757969697623025 :)\n",
            "The loss function for the iteration 13960----->38.75781130762389 :)\n",
            "The loss function for the iteration 13970----->38.757653130130706 :)\n",
            "The loss function for the iteration 13980----->38.757495164766695 :)\n",
            "The loss function for the iteration 13990----->38.75733741115623 :)\n",
            "The loss function for the iteration 14000----->38.75717986892487 :)\n",
            "The loss function for the iteration 14010----->38.7570225376993 :)\n",
            "The loss function for the iteration 14020----->38.75686541710738 :)\n",
            "The loss function for the iteration 14030----->38.75670850677807 :)\n",
            "The loss function for the iteration 14040----->38.75655180634154 :)\n",
            "The loss function for the iteration 14050----->38.756395315429025 :)\n",
            "The loss function for the iteration 14060----->38.7562390336729 :)\n",
            "The loss function for the iteration 14070----->38.75608296070665 :)\n",
            "The loss function for the iteration 14080----->38.75592709616491 :)\n",
            "The loss function for the iteration 14090----->38.75577143968336 :)\n",
            "The loss function for the iteration 14100----->38.755615990898804 :)\n",
            "The loss function for the iteration 14110----->38.75546074944916 :)\n",
            "The loss function for the iteration 14120----->38.7553057149734 :)\n",
            "The loss function for the iteration 14130----->38.755150887111576 :)\n",
            "The loss function for the iteration 14140----->38.75499626550483 :)\n",
            "The loss function for the iteration 14150----->38.75484184979536 :)\n",
            "The loss function for the iteration 14160----->38.75468763962644 :)\n",
            "The loss function for the iteration 14170----->38.75453363464237 :)\n",
            "The loss function for the iteration 14180----->38.75437983448852 :)\n",
            "The loss function for the iteration 14190----->38.75422623881129 :)\n",
            "The loss function for the iteration 14200----->38.75407284725815 :)\n",
            "The loss function for the iteration 14210----->38.75391965947757 :)\n",
            "The loss function for the iteration 14220----->38.75376667511904 :)\n",
            "The loss function for the iteration 14230----->38.75361389383309 :)\n",
            "The loss function for the iteration 14240----->38.75346131527128 :)\n",
            "The loss function for the iteration 14250----->38.753308939086146 :)\n",
            "The loss function for the iteration 14260----->38.75315676493123 :)\n",
            "The loss function for the iteration 14270----->38.75300479246109 :)\n",
            "The loss function for the iteration 14280----->38.75285302133127 :)\n",
            "The loss function for the iteration 14290----->38.752701451198305 :)\n",
            "The loss function for the iteration 14300----->38.752550081719704 :)\n",
            "The loss function for the iteration 14310----->38.75239891255397 :)\n",
            "The loss function for the iteration 14320----->38.752247943360544 :)\n",
            "The loss function for the iteration 14330----->38.752097173799854 :)\n",
            "The loss function for the iteration 14340----->38.7519466035333 :)\n",
            "The loss function for the iteration 14350----->38.751796232223214 :)\n",
            "The loss function for the iteration 14360----->38.751646059532895 :)\n",
            "The loss function for the iteration 14370----->38.75149608512659 :)\n",
            "The loss function for the iteration 14380----->38.75134630866947 :)\n",
            "The loss function for the iteration 14390----->38.75119672982766 :)\n",
            "The loss function for the iteration 14400----->38.75104734826819 :)\n",
            "The loss function for the iteration 14410----->38.750898163659045 :)\n",
            "The loss function for the iteration 14420----->38.75074917566909 :)\n",
            "The loss function for the iteration 14430----->38.75060038396818 :)\n",
            "The loss function for the iteration 14440----->38.75045178822699 :)\n",
            "The loss function for the iteration 14450----->38.75030338811717 :)\n",
            "The loss function for the iteration 14460----->38.75015518331122 :)\n",
            "The loss function for the iteration 14470----->38.75000717348257 :)\n",
            "The loss function for the iteration 14480----->38.749859358305564 :)\n",
            "The loss function for the iteration 14490----->38.74971173745537 :)\n",
            "The loss function for the iteration 14500----->38.74956431060807 :)\n",
            "The loss function for the iteration 14510----->38.74941707744064 :)\n",
            "The loss function for the iteration 14520----->38.74927003763091 :)\n",
            "The loss function for the iteration 14530----->38.74912319085758 :)\n",
            "The loss function for the iteration 14540----->38.748976536800214 :)\n",
            "The loss function for the iteration 14550----->38.748830075139246 :)\n",
            "The loss function for the iteration 14560----->38.748683805555956 :)\n",
            "The loss function for the iteration 14570----->38.748537727732476 :)\n",
            "The loss function for the iteration 14580----->38.74839184135179 :)\n",
            "The loss function for the iteration 14590----->38.748246146097706 :)\n",
            "The loss function for the iteration 14600----->38.748100641654915 :)\n",
            "The loss function for the iteration 14610----->38.74795532770887 :)\n",
            "The loss function for the iteration 14620----->38.74781020394594 :)\n",
            "The loss function for the iteration 14630----->38.74766527005323 :)\n",
            "The loss function for the iteration 14640----->38.74752052571875 :)\n",
            "The loss function for the iteration 14650----->38.74737597063125 :)\n",
            "The loss function for the iteration 14660----->38.747231604480355 :)\n",
            "The loss function for the iteration 14670----->38.74708742695647 :)\n",
            "The loss function for the iteration 14680----->38.7469434377508 :)\n",
            "The loss function for the iteration 14690----->38.74679963655536 :)\n",
            "The loss function for the iteration 14700----->38.74665602306297 :)\n",
            "The loss function for the iteration 14710----->38.746512596967214 :)\n",
            "The loss function for the iteration 14720----->38.746369357962514 :)\n",
            "The loss function for the iteration 14730----->38.74622630574401 :)\n",
            "The loss function for the iteration 14740----->38.74608344000768 :)\n",
            "The loss function for the iteration 14750----->38.745940760450274 :)\n",
            "The loss function for the iteration 14760----->38.74579826676927 :)\n",
            "The loss function for the iteration 14770----->38.745655958662965 :)\n",
            "The loss function for the iteration 14780----->38.74551383583039 :)\n",
            "The loss function for the iteration 14790----->38.74537189797135 :)\n",
            "The loss function for the iteration 14800----->38.745230144786426 :)\n",
            "The loss function for the iteration 14810----->38.745088575976915 :)\n",
            "The loss function for the iteration 14820----->38.7449471912449 :)\n",
            "The loss function for the iteration 14830----->38.74480599029319 :)\n",
            "The loss function for the iteration 14840----->38.74466497282533 :)\n",
            "The loss function for the iteration 14850----->38.74452413854564 :)\n",
            "The loss function for the iteration 14860----->38.74438348715916 :)\n",
            "The loss function for the iteration 14870----->38.74424301837165 :)\n",
            "The loss function for the iteration 14880----->38.74410273188961 :)\n",
            "The loss function for the iteration 14890----->38.74396262742024 :)\n",
            "The loss function for the iteration 14900----->38.743822704671516 :)\n",
            "The loss function for the iteration 14910----->38.74368296335209 :)\n",
            "The loss function for the iteration 14920----->38.74354340317135 :)\n",
            "The loss function for the iteration 14930----->38.74340402383939 :)\n",
            "The loss function for the iteration 14940----->38.743264825067016 :)\n",
            "The loss function for the iteration 14950----->38.7431258065657 :)\n",
            "The loss function for the iteration 14960----->38.7429869680477 :)\n",
            "The loss function for the iteration 14970----->38.742848309225906 :)\n",
            "The loss function for the iteration 14980----->38.742709829813904 :)\n",
            "The loss function for the iteration 14990----->38.74257152952602 :)\n",
            "The loss function for the iteration 15000----->38.74243340807721 :)\n",
            "The loss function for the iteration 15010----->38.74229546518317 :)\n",
            "The loss function for the iteration 15020----->38.742157700560234 :)\n",
            "The loss function for the iteration 15030----->38.742020113925456 :)\n",
            "The loss function for the iteration 15040----->38.741882704996534 :)\n",
            "The loss function for the iteration 15050----->38.741745473491854 :)\n",
            "The loss function for the iteration 15060----->38.741608419130465 :)\n",
            "The loss function for the iteration 15070----->38.74147154163209 :)\n",
            "The loss function for the iteration 15080----->38.741334840717116 :)\n",
            "The loss function for the iteration 15090----->38.741198316106605 :)\n",
            "The loss function for the iteration 15100----->38.74106196752223 :)\n",
            "The loss function for the iteration 15110----->38.74092579468637 :)\n",
            "The loss function for the iteration 15120----->38.74078979732203 :)\n",
            "The loss function for the iteration 15130----->38.74065397515288 :)\n",
            "The loss function for the iteration 15140----->38.7405183279032 :)\n",
            "The loss function for the iteration 15150----->38.74038285529798 :)\n",
            "The loss function for the iteration 15160----->38.74024755706278 :)\n",
            "The loss function for the iteration 15170----->38.740112432923844 :)\n",
            "The loss function for the iteration 15180----->38.73997748260803 :)\n",
            "The loss function for the iteration 15190----->38.739842705842825 :)\n",
            "The loss function for the iteration 15200----->38.739708102356374 :)\n",
            "The loss function for the iteration 15210----->38.7395736718774 :)\n",
            "The loss function for the iteration 15220----->38.73943941413531 :)\n",
            "The loss function for the iteration 15230----->38.73930532886008 :)\n",
            "The loss function for the iteration 15240----->38.73917141578231 :)\n",
            "The loss function for the iteration 15250----->38.73903767463326 :)\n",
            "The loss function for the iteration 15260----->38.73890410514475 :)\n",
            "The loss function for the iteration 15270----->38.738770707049234 :)\n",
            "The loss function for the iteration 15280----->38.738637480079774 :)\n",
            "The loss function for the iteration 15290----->38.73850442397003 :)\n",
            "The loss function for the iteration 15300----->38.73837153845429 :)\n",
            "The loss function for the iteration 15310----->38.73823882326739 :)\n",
            "The loss function for the iteration 15320----->38.7381062781448 :)\n",
            "The loss function for the iteration 15330----->38.73797390282258 :)\n",
            "The loss function for the iteration 15340----->38.73784169703739 :)\n",
            "The loss function for the iteration 15350----->38.737709660526455 :)\n",
            "The loss function for the iteration 15360----->38.7375777930276 :)\n",
            "The loss function for the iteration 15370----->38.73744609427924 :)\n",
            "The loss function for the iteration 15380----->38.737314564020366 :)\n",
            "The loss function for the iteration 15390----->38.73718320199055 :)\n",
            "The loss function for the iteration 15400----->38.73705200792993 :)\n",
            "The loss function for the iteration 15410----->38.736920981579246 :)\n",
            "The loss function for the iteration 15420----->38.73679012267977 :)\n",
            "The loss function for the iteration 15430----->38.73665943097338 :)\n",
            "The loss function for the iteration 15440----->38.736528906202516 :)\n",
            "The loss function for the iteration 15450----->38.73639854811015 :)\n",
            "The loss function for the iteration 15460----->38.73626835643987 :)\n",
            "The loss function for the iteration 15470----->38.73613833093578 :)\n",
            "The loss function for the iteration 15480----->38.73600847134256 :)\n",
            "The loss function for the iteration 15490----->38.73587877740544 :)\n",
            "The loss function for the iteration 15500----->38.7357492488702 :)\n",
            "The loss function for the iteration 15510----->38.7356198854832 :)\n",
            "The loss function for the iteration 15520----->38.73549068699132 :)\n",
            "The loss function for the iteration 15530----->38.73536165314197 :)\n",
            "The loss function for the iteration 15540----->38.735232783683145 :)\n",
            "The loss function for the iteration 15550----->38.73510407836339 :)\n",
            "The loss function for the iteration 15560----->38.73497553693173 :)\n",
            "The loss function for the iteration 15570----->38.73484715913777 :)\n",
            "The loss function for the iteration 15580----->38.73471894473167 :)\n",
            "The loss function for the iteration 15590----->38.73459089346406 :)\n",
            "The loss function for the iteration 15600----->38.73446300508617 :)\n",
            "The loss function for the iteration 15610----->38.7343352793497 :)\n",
            "The loss function for the iteration 15620----->38.734207716006935 :)\n",
            "The loss function for the iteration 15630----->38.73408031481064 :)\n",
            "The loss function for the iteration 15640----->38.73395307551411 :)\n",
            "The loss function for the iteration 15650----->38.73382599787119 :)\n",
            "The loss function for the iteration 15660----->38.73369908163621 :)\n",
            "The loss function for the iteration 15670----->38.73357232656404 :)\n",
            "The loss function for the iteration 15680----->38.733445732410054 :)\n",
            "The loss function for the iteration 15690----->38.73331929893013 :)\n",
            "The loss function for the iteration 15700----->38.73319302588067 :)\n",
            "The loss function for the iteration 15710----->38.73306691301858 :)\n",
            "The loss function for the iteration 15720----->38.732940960101296 :)\n",
            "The loss function for the iteration 15730----->38.732815166886716 :)\n",
            "The loss function for the iteration 15740----->38.73268953313327 :)\n",
            "The loss function for the iteration 15750----->38.73256405859986 :)\n",
            "The loss function for the iteration 15760----->38.73243874304596 :)\n",
            "The loss function for the iteration 15770----->38.732313586231456 :)\n",
            "The loss function for the iteration 15780----->38.73218858791677 :)\n",
            "The loss function for the iteration 15790----->38.732063747862796 :)\n",
            "The loss function for the iteration 15800----->38.73193906583095 :)\n",
            "The loss function for the iteration 15810----->38.73181454158313 :)\n",
            "The loss function for the iteration 15820----->38.73169017488168 :)\n",
            "The loss function for the iteration 15830----->38.73156596548951 :)\n",
            "The loss function for the iteration 15840----->38.73144191316993 :)\n",
            "The loss function for the iteration 15850----->38.7313180176868 :)\n",
            "The loss function for the iteration 15860----->38.7311942788044 :)\n",
            "The loss function for the iteration 15870----->38.73107069628756 :)\n",
            "The loss function for the iteration 15880----->38.7309472699015 :)\n",
            "The loss function for the iteration 15890----->38.730823999411996 :)\n",
            "The loss function for the iteration 15900----->38.73070088458526 :)\n",
            "The loss function for the iteration 15910----->38.73057792518797 :)\n",
            "The loss function for the iteration 15920----->38.73045512098729 :)\n",
            "The loss function for the iteration 15930----->38.73033247175083 :)\n",
            "The loss function for the iteration 15940----->38.73020997724671 :)\n",
            "The loss function for the iteration 15950----->38.73008763724346 :)\n",
            "The loss function for the iteration 15960----->38.72996545151011 :)\n",
            "The loss function for the iteration 15970----->38.72984341981616 :)\n",
            "The loss function for the iteration 15980----->38.729721541931525 :)\n",
            "The loss function for the iteration 15990----->38.72959981762662 :)\n",
            "The loss function for the iteration 16000----->38.72947824667229 :)\n",
            "The loss function for the iteration 16010----->38.72935682883986 :)\n",
            "The loss function for the iteration 16020----->38.72923556390108 :)\n",
            "The loss function for the iteration 16030----->38.72911445162817 :)\n",
            "The loss function for the iteration 16040----->38.72899349179381 :)\n",
            "The loss function for the iteration 16050----->38.728872684171094 :)\n",
            "The loss function for the iteration 16060----->38.7287520285336 :)\n",
            "The loss function for the iteration 16070----->38.72863152465532 :)\n",
            "The loss function for the iteration 16080----->38.728511172310725 :)\n",
            "The loss function for the iteration 16090----->38.728390971274685 :)\n",
            "The loss function for the iteration 16100----->38.72827092132253 :)\n",
            "The loss function for the iteration 16110----->38.72815102223006 :)\n",
            "The loss function for the iteration 16120----->38.72803127377346 :)\n",
            "The loss function for the iteration 16130----->38.72791167572939 :)\n",
            "The loss function for the iteration 16140----->38.72779222787493 :)\n",
            "The loss function for the iteration 16150----->38.72767292998758 :)\n",
            "The loss function for the iteration 16160----->38.727553781845295 :)\n",
            "The loss function for the iteration 16170----->38.727434783226435 :)\n",
            "The loss function for the iteration 16180----->38.72731593390982 :)\n",
            "The loss function for the iteration 16190----->38.72719723367469 :)\n",
            "The loss function for the iteration 16200----->38.727078682300665 :)\n",
            "The loss function for the iteration 16210----->38.726960279567855 :)\n",
            "The loss function for the iteration 16220----->38.72684202525676 :)\n",
            "The loss function for the iteration 16230----->38.72672391914828 :)\n",
            "The loss function for the iteration 16240----->38.72660596102378 :)\n",
            "The loss function for the iteration 16250----->38.72648815066502 :)\n",
            "The loss function for the iteration 16260----->38.72637048785419 :)\n",
            "The loss function for the iteration 16270----->38.72625297237385 :)\n",
            "The loss function for the iteration 16280----->38.72613560400704 :)\n",
            "The loss function for the iteration 16290----->38.726018382537184 :)\n",
            "The loss function for the iteration 16300----->38.72590130774809 :)\n",
            "The loss function for the iteration 16310----->38.72578437942402 :)\n",
            "The loss function for the iteration 16320----->38.72566759734963 :)\n",
            "The loss function for the iteration 16330----->38.72555096130996 :)\n",
            "The loss function for the iteration 16340----->38.72543447109051 :)\n",
            "The loss function for the iteration 16350----->38.725318126477106 :)\n",
            "The loss function for the iteration 16360----->38.72520192725607 :)\n",
            "The loss function for the iteration 16370----->38.72508587321405 :)\n",
            "The loss function for the iteration 16380----->38.72496996413813 :)\n",
            "The loss function for the iteration 16390----->38.72485419981578 :)\n",
            "The loss function for the iteration 16400----->38.724738580034895 :)\n",
            "The loss function for the iteration 16410----->38.72462310458372 :)\n",
            "The loss function for the iteration 16420----->38.72450777325095 :)\n",
            "The loss function for the iteration 16430----->38.724392585825626 :)\n",
            "The loss function for the iteration 16440----->38.72427754209721 :)\n",
            "The loss function for the iteration 16450----->38.72416264185556 :)\n",
            "The loss function for the iteration 16460----->38.724047884890894 :)\n",
            "The loss function for the iteration 16470----->38.72393327099387 :)\n",
            "The loss function for the iteration 16480----->38.723818799955474 :)\n",
            "The loss function for the iteration 16490----->38.72370447156711 :)\n",
            "The loss function for the iteration 16500----->38.723590285620595 :)\n",
            "The loss function for the iteration 16510----->38.72347624190806 :)\n",
            "The loss function for the iteration 16520----->38.72336234022211 :)\n",
            "The loss function for the iteration 16530----->38.72324858035564 :)\n",
            "The loss function for the iteration 16540----->38.72313496210199 :)\n",
            "The loss function for the iteration 16550----->38.72302148525487 :)\n",
            "The loss function for the iteration 16560----->38.72290814960833 :)\n",
            "The loss function for the iteration 16570----->38.72279495495683 :)\n",
            "The loss function for the iteration 16580----->38.722681901095235 :)\n",
            "The loss function for the iteration 16590----->38.72256898781871 :)\n",
            "The loss function for the iteration 16600----->38.72245621492286 :)\n",
            "The loss function for the iteration 16610----->38.72234358220363 :)\n",
            "The loss function for the iteration 16620----->38.722231089457345 :)\n",
            "The loss function for the iteration 16630----->38.722118736480695 :)\n",
            "The loss function for the iteration 16640----->38.72200652307075 :)\n",
            "The loss function for the iteration 16650----->38.721894449024965 :)\n",
            "The loss function for the iteration 16660----->38.7217825141411 :)\n",
            "The loss function for the iteration 16670----->38.72167071821734 :)\n",
            "The loss function for the iteration 16680----->38.721559061052226 :)\n",
            "The loss function for the iteration 16690----->38.721447542444636 :)\n",
            "The loss function for the iteration 16700----->38.72133616219384 :)\n",
            "The loss function for the iteration 16710----->38.72122492009945 :)\n",
            "The loss function for the iteration 16720----->38.72111381596145 :)\n",
            "The loss function for the iteration 16730----->38.72100284958019 :)\n",
            "The loss function for the iteration 16740----->38.72089202075634 :)\n",
            "The loss function for the iteration 16750----->38.720781329290986 :)\n",
            "The loss function for the iteration 16760----->38.72067077498552 :)\n",
            "The loss function for the iteration 16770----->38.72056035764173 :)\n",
            "The loss function for the iteration 16780----->38.720450077061706 :)\n",
            "The loss function for the iteration 16790----->38.72033993304797 :)\n",
            "The loss function for the iteration 16800----->38.7202299254033 :)\n",
            "The loss function for the iteration 16810----->38.720120053930906 :)\n",
            "The loss function for the iteration 16820----->38.7200103184343 :)\n",
            "The loss function for the iteration 16830----->38.71990071871736 :)\n",
            "The loss function for the iteration 16840----->38.71979125458434 :)\n",
            "The loss function for the iteration 16850----->38.71968192583977 :)\n",
            "The loss function for the iteration 16860----->38.719572732288604 :)\n",
            "The loss function for the iteration 16870----->38.719463673736094 :)\n",
            "The loss function for the iteration 16880----->38.71935474998784 :)\n",
            "The loss function for the iteration 16890----->38.71924596084981 :)\n",
            "The loss function for the iteration 16900----->38.71913730612828 :)\n",
            "The loss function for the iteration 16910----->38.71902878562991 :)\n",
            "The loss function for the iteration 16920----->38.71892039916167 :)\n",
            "The loss function for the iteration 16930----->38.71881214653085 :)\n",
            "The loss function for the iteration 16940----->38.718704027545115 :)\n",
            "The loss function for the iteration 16950----->38.718596042012486 :)\n",
            "The loss function for the iteration 16960----->38.71848818974125 :)\n",
            "The loss function for the iteration 16970----->38.71838047054009 :)\n",
            "The loss function for the iteration 16980----->38.718272884218 :)\n",
            "The loss function for the iteration 16990----->38.71816543058433 :)\n",
            "The loss function for the iteration 17000----->38.7180581094487 :)\n",
            "The loss function for the iteration 17010----->38.71795092062114 :)\n",
            "The loss function for the iteration 17020----->38.71784386391198 :)\n",
            "The loss function for the iteration 17030----->38.71773693913186 :)\n",
            "The loss function for the iteration 17040----->38.71763014609177 :)\n",
            "The loss function for the iteration 17050----->38.71752348460304 :)\n",
            "The loss function for the iteration 17060----->38.71741695447729 :)\n",
            "The loss function for the iteration 17070----->38.717310555526495 :)\n",
            "The loss function for the iteration 17080----->38.71720428756296 :)\n",
            "The loss function for the iteration 17090----->38.71709815039927 :)\n",
            "The loss function for the iteration 17100----->38.716992143848415 :)\n",
            "The loss function for the iteration 17110----->38.71688626772364 :)\n",
            "The loss function for the iteration 17120----->38.71678052183852 :)\n",
            "The loss function for the iteration 17130----->38.716674906006965 :)\n",
            "The loss function for the iteration 17140----->38.71656942004323 :)\n",
            "The loss function for the iteration 17150----->38.71646406376183 :)\n",
            "The loss function for the iteration 17160----->38.716358836977655 :)\n",
            "The loss function for the iteration 17170----->38.71625373950589 :)\n",
            "The loss function for the iteration 17180----->38.716148771162004 :)\n",
            "The loss function for the iteration 17190----->38.71604393176185 :)\n",
            "The loss function for the iteration 17200----->38.71593922112156 :)\n",
            "The loss function for the iteration 17210----->38.715834639057555 :)\n",
            "The loss function for the iteration 17220----->38.71573018538662 :)\n",
            "The loss function for the iteration 17230----->38.71562585992582 :)\n",
            "The loss function for the iteration 17240----->38.71552166249254 :)\n",
            "The loss function for the iteration 17250----->38.71541759290447 :)\n",
            "The loss function for the iteration 17260----->38.715313650979624 :)\n",
            "The loss function for the iteration 17270----->38.71520983653634 :)\n",
            "The loss function for the iteration 17280----->38.7151061493932 :)\n",
            "The loss function for the iteration 17290----->38.71500258936918 :)\n",
            "The loss function for the iteration 17300----->38.714899156283494 :)\n",
            "The loss function for the iteration 17310----->38.71479584995569 :)\n",
            "The loss function for the iteration 17320----->38.714692670205636 :)\n",
            "The loss function for the iteration 17330----->38.71458961685347 :)\n",
            "The loss function for the iteration 17340----->38.71448668971967 :)\n",
            "The loss function for the iteration 17350----->38.71438388862498 :)\n",
            "The loss function for the iteration 17360----->38.71428121339051 :)\n",
            "The loss function for the iteration 17370----->38.71417866383758 :)\n",
            "The loss function for the iteration 17380----->38.71407623978787 :)\n",
            "The loss function for the iteration 17390----->38.71397394106337 :)\n",
            "The loss function for the iteration 17400----->38.71387176748634 :)\n",
            "The loss function for the iteration 17410----->38.713769718879334 :)\n",
            "The loss function for the iteration 17420----->38.713667795065234 :)\n",
            "The loss function for the iteration 17430----->38.71356599586721 :)\n",
            "The loss function for the iteration 17440----->38.713464321108695 :)\n",
            "The loss function for the iteration 17450----->38.71336277061346 :)\n",
            "The loss function for the iteration 17460----->38.71326134420555 :)\n",
            "The loss function for the iteration 17470----->38.713160041709315 :)\n",
            "The loss function for the iteration 17480----->38.713058862949396 :)\n",
            "The loss function for the iteration 17490----->38.712957807750705 :)\n",
            "The loss function for the iteration 17500----->38.7128568759385 :)\n",
            "The loss function for the iteration 17510----->38.71275606733825 :)\n",
            "The loss function for the iteration 17520----->38.71265538177578 :)\n",
            "The loss function for the iteration 17530----->38.712554819077205 :)\n",
            "The loss function for the iteration 17540----->38.71245437906887 :)\n",
            "The loss function for the iteration 17550----->38.712354061577486 :)\n",
            "The loss function for the iteration 17560----->38.71225386642997 :)\n",
            "The loss function for the iteration 17570----->38.71215379345361 :)\n",
            "The loss function for the iteration 17580----->38.71205384247592 :)\n",
            "The loss function for the iteration 17590----->38.711954013324714 :)\n",
            "The loss function for the iteration 17600----->38.7118543058281 :)\n",
            "The loss function for the iteration 17610----->38.71175471981447 :)\n",
            "The loss function for the iteration 17620----->38.7116552551125 :)\n",
            "The loss function for the iteration 17630----->38.71155591155114 :)\n",
            "The loss function for the iteration 17640----->38.711456688959615 :)\n",
            "The loss function for the iteration 17650----->38.71135758716747 :)\n",
            "The loss function for the iteration 17660----->38.71125860600448 :)\n",
            "The loss function for the iteration 17670----->38.71115974530074 :)\n",
            "The loss function for the iteration 17680----->38.71106100488661 :)\n",
            "The loss function for the iteration 17690----->38.71096238459272 :)\n",
            "The loss function for the iteration 17700----->38.71086388424998 :)\n",
            "The loss function for the iteration 17710----->38.71076550368962 :)\n",
            "The loss function for the iteration 17720----->38.71066724274308 :)\n",
            "The loss function for the iteration 17730----->38.71056910124211 :)\n",
            "The loss function for the iteration 17740----->38.710471079018745 :)\n",
            "The loss function for the iteration 17750----->38.71037317590527 :)\n",
            "The loss function for the iteration 17760----->38.71027539173428 :)\n",
            "The loss function for the iteration 17770----->38.7101777263386 :)\n",
            "The loss function for the iteration 17780----->38.710080179551355 :)\n",
            "The loss function for the iteration 17790----->38.70998275120595 :)\n",
            "The loss function for the iteration 17800----->38.70988544113604 :)\n",
            "The loss function for the iteration 17810----->38.70978824917555 :)\n",
            "The loss function for the iteration 17820----->38.70969117515871 :)\n",
            "The loss function for the iteration 17830----->38.70959421891998 :)\n",
            "The loss function for the iteration 17840----->38.70949738029412 :)\n",
            "The loss function for the iteration 17850----->38.70940065911612 :)\n",
            "The loss function for the iteration 17860----->38.70930405522128 :)\n",
            "The loss function for the iteration 17870----->38.70920756844515 :)\n",
            "The loss function for the iteration 17880----->38.70911119862355 :)\n",
            "The loss function for the iteration 17890----->38.70901494559255 :)\n",
            "The loss function for the iteration 17900----->38.70891880918851 :)\n",
            "The loss function for the iteration 17910----->38.70882278924804 :)\n",
            "The loss function for the iteration 17920----->38.70872688560803 :)\n",
            "The loss function for the iteration 17930----->38.70863109810561 :)\n",
            "The loss function for the iteration 17940----->38.70853542657818 :)\n",
            "The loss function for the iteration 17950----->38.70843987086344 :)\n",
            "The loss function for the iteration 17960----->38.70834443079928 :)\n",
            "The loss function for the iteration 17970----->38.708249106223924 :)\n",
            "The loss function for the iteration 17980----->38.7081538969758 :)\n",
            "The loss function for the iteration 17990----->38.708058802893646 :)\n",
            "The loss function for the iteration 18000----->38.707963823816435 :)\n",
            "The loss function for the iteration 18010----->38.70786895958339 :)\n",
            "The loss function for the iteration 18020----->38.70777421003399 :)\n",
            "The loss function for the iteration 18030----->38.70767957500801 :)\n",
            "The loss function for the iteration 18040----->38.70758505434544 :)\n",
            "The loss function for the iteration 18050----->38.70749064788655 :)\n",
            "The loss function for the iteration 18060----->38.70739635547188 :)\n",
            "The loss function for the iteration 18070----->38.70730217694217 :)\n",
            "The loss function for the iteration 18080----->38.70720811213848 :)\n",
            "The loss function for the iteration 18090----->38.707114160902094 :)\n",
            "The loss function for the iteration 18100----->38.70702032307454 :)\n",
            "The loss function for the iteration 18110----->38.70692659849762 :)\n",
            "The loss function for the iteration 18120----->38.70683298701337 :)\n",
            "The loss function for the iteration 18130----->38.706739488464116 :)\n",
            "The loss function for the iteration 18140----->38.706646102692396 :)\n",
            "The loss function for the iteration 18150----->38.70655282954101 :)\n",
            "The loss function for the iteration 18160----->38.70645966885302 :)\n",
            "The loss function for the iteration 18170----->38.70636662047171 :)\n",
            "The loss function for the iteration 18180----->38.70627368424066 :)\n",
            "The loss function for the iteration 18190----->38.70618086000368 :)\n",
            "The loss function for the iteration 18200----->38.70608814760479 :)\n",
            "The loss function for the iteration 18210----->38.70599554688832 :)\n",
            "The loss function for the iteration 18220----->38.70590305769879 :)\n",
            "The loss function for the iteration 18230----->38.705810679881026 :)\n",
            "The loss function for the iteration 18240----->38.70571841328006 :)\n",
            "The loss function for the iteration 18250----->38.70562625774119 :)\n",
            "The loss function for the iteration 18260----->38.705534213109914 :)\n",
            "The loss function for the iteration 18270----->38.70544227923204 :)\n",
            "The loss function for the iteration 18280----->38.70535045595359 :)\n",
            "The loss function for the iteration 18290----->38.70525874312084 :)\n",
            "The loss function for the iteration 18300----->38.70516714058027 :)\n",
            "The loss function for the iteration 18310----->38.70507564817866 :)\n",
            "The loss function for the iteration 18320----->38.70498426576301 :)\n",
            "The loss function for the iteration 18330----->38.70489299318055 :)\n",
            "The loss function for the iteration 18340----->38.70480183027876 :)\n",
            "The loss function for the iteration 18350----->38.70471077690536 :)\n",
            "The loss function for the iteration 18360----->38.70461983290831 :)\n",
            "The loss function for the iteration 18370----->38.704528998135814 :)\n",
            "The loss function for the iteration 18380----->38.70443827243632 :)\n",
            "The loss function for the iteration 18390----->38.704347655658495 :)\n",
            "The loss function for the iteration 18400----->38.70425714765127 :)\n",
            "The loss function for the iteration 18410----->38.70416674826379 :)\n",
            "The loss function for the iteration 18420----->38.70407645734547 :)\n",
            "The loss function for the iteration 18430----->38.703986274745915 :)\n",
            "The loss function for the iteration 18440----->38.70389620031501 :)\n",
            "The loss function for the iteration 18450----->38.703806233902846 :)\n",
            "The loss function for the iteration 18460----->38.70371637535978 :)\n",
            "The loss function for the iteration 18470----->38.70362662453636 :)\n",
            "The loss function for the iteration 18480----->38.70353698128343 :)\n",
            "The loss function for the iteration 18490----->38.703447445452 :)\n",
            "The loss function for the iteration 18500----->38.70335801689337 :)\n",
            "The loss function for the iteration 18510----->38.70326869545904 :)\n",
            "The loss function for the iteration 18520----->38.703179481000745 :)\n",
            "The loss function for the iteration 18530----->38.703090373370486 :)\n",
            "The loss function for the iteration 18540----->38.70300137242044 :)\n",
            "The loss function for the iteration 18550----->38.702912478003064 :)\n",
            "The loss function for the iteration 18560----->38.702823689971005 :)\n",
            "The loss function for the iteration 18570----->38.70273500817719 :)\n",
            "The loss function for the iteration 18580----->38.70264643247475 :)\n",
            "The loss function for the iteration 18590----->38.702557962716995 :)\n",
            "The loss function for the iteration 18600----->38.702469598757574 :)\n",
            "The loss function for the iteration 18610----->38.70238134045026 :)\n",
            "The loss function for the iteration 18620----->38.70229318764913 :)\n",
            "The loss function for the iteration 18630----->38.70220514020842 :)\n",
            "The loss function for the iteration 18640----->38.70211719798265 :)\n",
            "The loss function for the iteration 18650----->38.70202936082655 :)\n",
            "The loss function for the iteration 18660----->38.70194162859506 :)\n",
            "The loss function for the iteration 18670----->38.70185400114336 :)\n",
            "The loss function for the iteration 18680----->38.70176647832686 :)\n",
            "The loss function for the iteration 18690----->38.701679060001176 :)\n",
            "The loss function for the iteration 18700----->38.701591746022174 :)\n",
            "The loss function for the iteration 18710----->38.70150453624591 :)\n",
            "The loss function for the iteration 18720----->38.70141743052871 :)\n",
            "The loss function for the iteration 18730----->38.701330428727076 :)\n",
            "The loss function for the iteration 18740----->38.701243530697774 :)\n",
            "The loss function for the iteration 18750----->38.70115673629775 :)\n",
            "The loss function for the iteration 18760----->38.70107004538419 :)\n",
            "The loss function for the iteration 18770----->38.70098345781454 :)\n",
            "The loss function for the iteration 18780----->38.70089697344639 :)\n",
            "The loss function for the iteration 18790----->38.700810592137636 :)\n",
            "The loss function for the iteration 18800----->38.70072431374632 :)\n",
            "The loss function for the iteration 18810----->38.70063813813074 :)\n",
            "The loss function for the iteration 18820----->38.70055206514942 :)\n",
            "The loss function for the iteration 18830----->38.70046609466106 :)\n",
            "The loss function for the iteration 18840----->38.70038022652466 :)\n",
            "The loss function for the iteration 18850----->38.70029446059934 :)\n",
            "The loss function for the iteration 18860----->38.7002087967445 :)\n",
            "The loss function for the iteration 18870----->38.70012323481975 :)\n",
            "The loss function for the iteration 18880----->38.70003777468491 :)\n",
            "The loss function for the iteration 18890----->38.699952416199984 :)\n",
            "The loss function for the iteration 18900----->38.69986715922526 :)\n",
            "The loss function for the iteration 18910----->38.69978200362119 :)\n",
            "The loss function for the iteration 18920----->38.69969694924845 :)\n",
            "The loss function for the iteration 18930----->38.69961199596793 :)\n",
            "The loss function for the iteration 18940----->38.699527143640765 :)\n",
            "The loss function for the iteration 18950----->38.69944239212826 :)\n",
            "The loss function for the iteration 18960----->38.69935774129196 :)\n",
            "The loss function for the iteration 18970----->38.69927319099362 :)\n",
            "The loss function for the iteration 18980----->38.699188741095185 :)\n",
            "The loss function for the iteration 18990----->38.69910439145884 :)\n",
            "The loss function for the iteration 19000----->38.69902014194697 :)\n",
            "The loss function for the iteration 19010----->38.698935992422186 :)\n",
            "The loss function for the iteration 19020----->38.69885194274728 :)\n",
            "The loss function for the iteration 19030----->38.69876799278528 :)\n",
            "The loss function for the iteration 19040----->38.69868414239942 :)\n",
            "The loss function for the iteration 19050----->38.69860039145313 :)\n",
            "The loss function for the iteration 19060----->38.69851673981006 :)\n",
            "The loss function for the iteration 19070----->38.69843318733408 :)\n",
            "The loss function for the iteration 19080----->38.698349733889245 :)\n",
            "The loss function for the iteration 19090----->38.69826637933985 :)\n",
            "The loss function for the iteration 19100----->38.69818312355034 :)\n",
            "The loss function for the iteration 19110----->38.69809996638545 :)\n",
            "The loss function for the iteration 19120----->38.69801690771006 :)\n",
            "The loss function for the iteration 19130----->38.69793394738926 :)\n",
            "The loss function for the iteration 19140----->38.69785108528838 :)\n",
            "The loss function for the iteration 19150----->38.69776832127293 :)\n",
            "The loss function for the iteration 19160----->38.69768565520864 :)\n",
            "The loss function for the iteration 19170----->38.6976030869614 :)\n",
            "The loss function for the iteration 19180----->38.697520616397405 :)\n",
            "The loss function for the iteration 19190----->38.69743824338294 :)\n",
            "The loss function for the iteration 19200----->38.697355967784574 :)\n",
            "The loss function for the iteration 19210----->38.69727378946904 :)\n",
            "The loss function for the iteration 19220----->38.69719170830328 :)\n",
            "The loss function for the iteration 19230----->38.697109724154465 :)\n",
            "The loss function for the iteration 19240----->38.69702783688992 :)\n",
            "The loss function for the iteration 19250----->38.69694604637723 :)\n",
            "The loss function for the iteration 19260----->38.696864352484134 :)\n",
            "The loss function for the iteration 19270----->38.69678275507859 :)\n",
            "The loss function for the iteration 19280----->38.69670125402876 :)\n",
            "The loss function for the iteration 19290----->38.696619849202996 :)\n",
            "The loss function for the iteration 19300----->38.69653854046988 :)\n",
            "The loss function for the iteration 19310----->38.696457327698155 :)\n",
            "The loss function for the iteration 19320----->38.696376210756796 :)\n",
            "The loss function for the iteration 19330----->38.69629518951494 :)\n",
            "The loss function for the iteration 19340----->38.69621426384196 :)\n",
            "The loss function for the iteration 19350----->38.69613343360741 :)\n",
            "The loss function for the iteration 19360----->38.696052698681044 :)\n",
            "The loss function for the iteration 19370----->38.69597205893282 :)\n",
            "The loss function for the iteration 19380----->38.695891514232876 :)\n",
            "The loss function for the iteration 19390----->38.69581106445157 :)\n",
            "The loss function for the iteration 19400----->38.69573070945943 :)\n",
            "The loss function for the iteration 19410----->38.695650449127214 :)\n",
            "The loss function for the iteration 19420----->38.69557028332586 :)\n",
            "The loss function for the iteration 19430----->38.69549021192648 :)\n",
            "The loss function for the iteration 19440----->38.69541023480041 :)\n",
            "The loss function for the iteration 19450----->38.69533035181917 :)\n",
            "The loss function for the iteration 19460----->38.69525056285448 :)\n",
            "The loss function for the iteration 19470----->38.695170867778266 :)\n",
            "The loss function for the iteration 19480----->38.695091266462605 :)\n",
            "The loss function for the iteration 19490----->38.695011758779806 :)\n",
            "The loss function for the iteration 19500----->38.69493234460236 :)\n",
            "The loss function for the iteration 19510----->38.69485302380295 :)\n",
            "The loss function for the iteration 19520----->38.694773796254445 :)\n",
            "The loss function for the iteration 19530----->38.694694661829935 :)\n",
            "The loss function for the iteration 19540----->38.694615620402665 :)\n",
            "The loss function for the iteration 19550----->38.694536671846095 :)\n",
            "The loss function for the iteration 19560----->38.694457816033854 :)\n",
            "The loss function for the iteration 19570----->38.694379052839786 :)\n",
            "The loss function for the iteration 19580----->38.69430038213792 :)\n",
            "The loss function for the iteration 19590----->38.69422180380246 :)\n",
            "The loss function for the iteration 19600----->38.69414331770781 :)\n",
            "The loss function for the iteration 19610----->38.69406492372858 :)\n",
            "The loss function for the iteration 19620----->38.693986621739526 :)\n",
            "The loss function for the iteration 19630----->38.693908411615645 :)\n",
            "The loss function for the iteration 19640----->38.69383029323208 :)\n",
            "The loss function for the iteration 19650----->38.693752266464195 :)\n",
            "The loss function for the iteration 19660----->38.69367433118751 :)\n",
            "The loss function for the iteration 19670----->38.69359648727776 :)\n",
            "The loss function for the iteration 19680----->38.693518734610855 :)\n",
            "The loss function for the iteration 19690----->38.69344107306288 :)\n",
            "The loss function for the iteration 19700----->38.693363502510124 :)\n",
            "The loss function for the iteration 19710----->38.69328602282906 :)\n",
            "The loss function for the iteration 19720----->38.69320863389633 :)\n",
            "The loss function for the iteration 19730----->38.69313133558882 :)\n",
            "The loss function for the iteration 19740----->38.69305412778348 :)\n",
            "The loss function for the iteration 19750----->38.692977010357595 :)\n",
            "The loss function for the iteration 19760----->38.692899983188525 :)\n",
            "The loss function for the iteration 19770----->38.69282304615385 :)\n",
            "The loss function for the iteration 19780----->38.692746199131335 :)\n",
            "The loss function for the iteration 19790----->38.69266944199894 :)\n",
            "The loss function for the iteration 19800----->38.69259277463478 :)\n",
            "The loss function for the iteration 19810----->38.692516196917175 :)\n",
            "The loss function for the iteration 19820----->38.69243970872463 :)\n",
            "The loss function for the iteration 19830----->38.6923633099358 :)\n",
            "The loss function for the iteration 19840----->38.692287000429566 :)\n",
            "The loss function for the iteration 19850----->38.692210780084956 :)\n",
            "The loss function for the iteration 19860----->38.692134648781206 :)\n",
            "The loss function for the iteration 19870----->38.6920586063977 :)\n",
            "The loss function for the iteration 19880----->38.69198265281404 :)\n",
            "The loss function for the iteration 19890----->38.69190678790999 :)\n",
            "The loss function for the iteration 19900----->38.69183101156548 :)\n",
            "The loss function for the iteration 19910----->38.69175532366067 :)\n",
            "The loss function for the iteration 19920----->38.69167972407583 :)\n",
            "The loss function for the iteration 19930----->38.69160421269146 :)\n",
            "The loss function for the iteration 19940----->38.691528789388215 :)\n",
            "The loss function for the iteration 19950----->38.69145345404693 :)\n",
            "The loss function for the iteration 19960----->38.691378206548634 :)\n",
            "The loss function for the iteration 19970----->38.691303046774536 :)\n",
            "The loss function for the iteration 19980----->38.69122797460598 :)\n",
            "The loss function for the iteration 19990----->38.69115298992454 :)\n",
            "The loss function for the iteration 20000----->38.69107809261194 :)\n"
          ]
        }
      ],
      "source": [
        "ridge_regression_custom = RidgeRegression(0.001, 1e-2, 20000)\n",
        "y_train_reshaped = y_train.reshape(-1, 1)\n",
        "ridge_regression_custom.train(X_train, y_train_reshaped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etHAUbz3U_PW"
      },
      "source": [
        "# Ridge Regression using scikit-learn (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CHPali0U_PW"
      },
      "source": [
        "Use `sklearn` to train a Ridge Regression Model. To determine the best regularization coefficients, use grid-search (or other techniques you've learned till now)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:37:26.807683Z",
          "iopub.status.busy": "2023-04-30T08:37:26.807262Z",
          "iopub.status.idle": "2023-04-30T08:37:26.832283Z",
          "shell.execute_reply": "2023-04-30T08:37:26.830639Z",
          "shell.execute_reply.started": "2023-04-30T08:37:26.807621Z"
        },
        "id": "IJY3TMVAU_PW",
        "outputId": "ed09f8a4-bc50-472f-9dde-fe60035ba49c",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, estimator=Ridge(max_iter=10000),\n",
              "             param_grid={'alpha': array([1.00000000e-08, 1.45082878e-08, 2.10490414e-08, 3.05385551e-08,\n",
              "       4.43062146e-08, 6.42807312e-08, 9.32603347e-08, 1.35304777e-07,\n",
              "       1.96304065e-07, 2.84803587e-07, 4.13201240e-07, 5.99484250e-07,\n",
              "       8.69749003e-07, 1.26185688e-06, 1.83073828e-06, 2.65608778e-06,\n",
              "       3.85352859e-06, 5.59081018e-06, 8....\n",
              "       1.91791026e+04, 2.78255940e+04, 4.03701726e+04, 5.85702082e+04,\n",
              "       8.49753436e+04, 1.23284674e+05, 1.78864953e+05, 2.59502421e+05,\n",
              "       3.76493581e+05, 5.46227722e+05, 7.92482898e+05, 1.14975700e+06,\n",
              "       1.66810054e+06, 2.42012826e+06, 3.51119173e+06, 5.09413801e+06,\n",
              "       7.39072203e+06, 1.07226722e+07, 1.55567614e+07, 2.25701972e+07,\n",
              "       3.27454916e+07, 4.75081016e+07, 6.89261210e+07, 1.00000000e+08])})"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=Ridge(max_iter=10000),\n",
              "             param_grid={&#x27;alpha&#x27;: array([1.00000000e-08, 1.45082878e-08, 2.10490414e-08, 3.05385551e-08,\n",
              "       4.43062146e-08, 6.42807312e-08, 9.32603347e-08, 1.35304777e-07,\n",
              "       1.96304065e-07, 2.84803587e-07, 4.13201240e-07, 5.99484250e-07,\n",
              "       8.69749003e-07, 1.26185688e-06, 1.83073828e-06, 2.65608778e-06,\n",
              "       3.85352859e-06, 5.59081018e-06, 8....\n",
              "       1.91791026e+04, 2.78255940e+04, 4.03701726e+04, 5.85702082e+04,\n",
              "       8.49753436e+04, 1.23284674e+05, 1.78864953e+05, 2.59502421e+05,\n",
              "       3.76493581e+05, 5.46227722e+05, 7.92482898e+05, 1.14975700e+06,\n",
              "       1.66810054e+06, 2.42012826e+06, 3.51119173e+06, 5.09413801e+06,\n",
              "       7.39072203e+06, 1.07226722e+07, 1.55567614e+07, 2.25701972e+07,\n",
              "       3.27454916e+07, 4.75081016e+07, 6.89261210e+07, 1.00000000e+08])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10, estimator=Ridge(max_iter=10000),\n",
              "             param_grid={&#x27;alpha&#x27;: array([1.00000000e-08, 1.45082878e-08, 2.10490414e-08, 3.05385551e-08,\n",
              "       4.43062146e-08, 6.42807312e-08, 9.32603347e-08, 1.35304777e-07,\n",
              "       1.96304065e-07, 2.84803587e-07, 4.13201240e-07, 5.99484250e-07,\n",
              "       8.69749003e-07, 1.26185688e-06, 1.83073828e-06, 2.65608778e-06,\n",
              "       3.85352859e-06, 5.59081018e-06, 8....\n",
              "       1.91791026e+04, 2.78255940e+04, 4.03701726e+04, 5.85702082e+04,\n",
              "       8.49753436e+04, 1.23284674e+05, 1.78864953e+05, 2.59502421e+05,\n",
              "       3.76493581e+05, 5.46227722e+05, 7.92482898e+05, 1.14975700e+06,\n",
              "       1.66810054e+06, 2.42012826e+06, 3.51119173e+06, 5.09413801e+06,\n",
              "       7.39072203e+06, 1.07226722e+07, 1.55567614e+07, 2.25701972e+07,\n",
              "       3.27454916e+07, 4.75081016e+07, 6.89261210e+07, 1.00000000e+08])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(max_iter=10000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(max_iter=10000)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "params = {'alpha': (np.logspace(-8, 8, 100))}\n",
        "ridge_regression_sklearn = Ridge(max_iter=10000)\n",
        "ridge_model = GridSearchCV(ridge_regression_sklearn, params, cv = 10)\n",
        "ridge_model.fit(X_train, y_train_reshaped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRGCZENmU_PW"
      },
      "source": [
        "# Evaluation (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KGVuWJUU_PW"
      },
      "source": [
        "For each model (the 2 models trained using `sklearn` and the ones based on your code), predict the output for the testing samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-30T08:37:34.818097Z",
          "iopub.status.busy": "2023-04-30T08:37:34.817713Z",
          "iopub.status.idle": "2023-04-30T08:37:34.828388Z",
          "shell.execute_reply": "2023-04-30T08:37:34.826441Z",
          "shell.execute_reply.started": "2023-04-30T08:37:34.818063Z"
        },
        "id": "JWkNVRW7U_PW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "y_pred_lasso_custom = lasso_regression_custom.predict(X_test)\n",
        "y_pred_lasso_sklearn = lasso_model.predict(X_test)\n",
        "y_pred_ridge_custom = ridge_regression_custom.predict(X_test)\n",
        "y_pred_ridge_sklearn = ridge_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm-OLTTvU_PX"
      },
      "source": [
        "Measure the performance of the models based on \"mean squared error\" and the \"coefficient of determination\" of the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-30T08:37:38.137975Z",
          "iopub.status.busy": "2023-04-30T08:37:38.137503Z",
          "iopub.status.idle": "2023-04-30T08:37:38.147255Z",
          "shell.execute_reply": "2023-04-30T08:37:38.145677Z",
          "shell.execute_reply.started": "2023-04-30T08:37:38.137931Z"
        },
        "id": "-MqKWn_WU_PX",
        "outputId": "e82a6eee-d9dc-4f6e-e8e6-4dbe282ae69c",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Lasso:\n",
            "MSE: 78.1737756345305\n",
            "R2: 0.014108361846872697\n",
            "\n",
            "Sklearn Lasso:\n",
            "MSE: 74.87432600898913\n",
            "R2: 0.05571950023599581\n",
            "\n",
            "Custom Ridge:\n",
            "MSE: 77.88756975083413\n",
            "R2: 0.017717858576896783\n",
            "\n",
            "Sklearn Ridge:\n",
            "MSE: 74.84371248472534\n",
            "R2: 0.05610558389820974\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Custom Lasso\n",
        "mse_lasso_custom = mean_squared_error(y_test, y_pred_lasso_custom)\n",
        "r2_lasso_custom = r2_score(y_test, y_pred_lasso_custom)\n",
        "print(\"Custom Lasso:\" + \"\\n\" + \"MSE: \" + str(mse_lasso_custom) + \"\\n\" + \"R2: \" + str(r2_lasso_custom) + \"\\n\")\n",
        "\n",
        "#Sklearn Lasso\n",
        "mse_lasso_sklearn = mean_squared_error(y_test, y_pred_lasso_sklearn)\n",
        "r2_lasso_sklearn = r2_score(y_test, y_pred_lasso_sklearn)\n",
        "print(\"Sklearn Lasso:\" + \"\\n\" + \"MSE: \" + str(mse_lasso_sklearn) + \"\\n\" + \"R2: \" + str(r2_lasso_sklearn) + \"\\n\")\n",
        "\n",
        "#Custom Ridge\n",
        "mse_ridge_custom = mean_squared_error(y_test, y_pred_ridge_custom)\n",
        "r2_ridge_custom = r2_score(y_test, y_pred_ridge_custom)\n",
        "print(\"Custom Ridge:\" + \"\\n\" + \"MSE: \" + str(mse_ridge_custom) + \"\\n\" + \"R2: \" + str(r2_ridge_custom) + \"\\n\")\n",
        "\n",
        "#Sklearn Ridge\n",
        "mse_ridge_sklearn = mean_squared_error(y_test, y_pred_ridge_sklearn)\n",
        "r2_ridge_sklearn = r2_score(y_test, y_pred_ridge_sklearn)\n",
        "print(\"Sklearn Ridge:\" + \"\\n\" + \"MSE: \" + str(mse_ridge_sklearn) + \"\\n\" + \"R2: \" + str(r2_ridge_sklearn) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKvQNxCBd4bH"
      },
      "source": [
        "# Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWVaPC-Yd6yb"
      },
      "source": [
        "Competition Link: https://www.kaggle.com/t/adbf95666e7c4f41a6be1129a9e4415c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "VzpnAmIUEkEn",
        "outputId": "2e8e5de8-9bfb-453f-e087-3aa52b937b68"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-49745265-ceef-4a09-b948-c3adb5b985ad\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-49745265-ceef-4a09-b948-c3adb5b985ad\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"mohammadabolnejadian\",\"key\":\"63ee2f88a756c683494eba9ae099ffb0\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "mrh3a_YpFk34"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c sharif-ml-1401-c5\n",
        "! unzip sharif-ml-1401-c5.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbAaacrSHoqW",
        "outputId": "8591c4b2-56e6-429f-f89d-aeee91c314d1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sharif-ml-1401-c5.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  sharif-ml-1401-c5.zip\n",
            "  inflating: assignment5-test-data-sample-submission.csv  \n",
            "  inflating: assignment5-test-data.csv  \n",
            "  inflating: assignment5-training-data.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"assignment5-training-data.csv\")\n",
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "fcKHMpIfTkil",
        "outputId": "ea95c179-eb60-48df-c439-9dbc385deb7a"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Country  Year      Status  Life expectancy   \\\n",
              "0                   Finland  2013  Developing              87.0   \n",
              "1                     Japan  2015   Developed              83.7   \n",
              "2      Syrian Arab Republic  2014  Developing              64.4   \n",
              "3                    Latvia  2010   Developed              72.8   \n",
              "4     Sao Tome and Principe  2013  Developing              67.1   \n",
              "...                     ...   ...         ...               ...   \n",
              "2492                Tunisia  2000  Developing              72.9   \n",
              "2493                Myanmar  2001  Developing              62.5   \n",
              "2494            Netherlands  2008   Developed              83.0   \n",
              "2495            South Sudan  2013  Developing              56.4   \n",
              "2496              Guatemala  2000  Developing              67.7   \n",
              "\n",
              "      Adult Mortality  infant deaths  Alcohol  percentage expenditure  \\\n",
              "0                79.0              0     8.97             6115.496624   \n",
              "1                55.0              2      NaN                0.000000   \n",
              "2               294.0              7     0.01                0.000000   \n",
              "3                18.0              0     9.80             1109.969508   \n",
              "4               192.0              0     0.01              200.660099   \n",
              "...               ...            ...      ...                     ...   \n",
              "2492            112.0              4     1.21              264.784220   \n",
              "2493            239.0             72     0.38                1.917164   \n",
              "2494             68.0              1     9.62            10873.405540   \n",
              "2495            345.0             26      NaN               47.444530   \n",
              "2496            221.0             17     2.63              238.736981   \n",
              "\n",
              "      Hepatitis B  Measles   ...  Polio  Total expenditure  Diphtheria   \\\n",
              "0             NaN         2  ...   98.0               9.55         98.0   \n",
              "1             NaN        35  ...   99.0                NaN         96.0   \n",
              "2            47.0       594  ...   52.0               3.25         43.0   \n",
              "3            91.0         0  ...   92.0               6.55         92.0   \n",
              "4            97.0         0  ...   97.0               9.76         97.0   \n",
              "...           ...       ...  ...    ...                ...          ...   \n",
              "2492         94.0        47  ...   97.0               5.40         97.0   \n",
              "2493          NaN      2519  ...   77.0               1.80         73.0   \n",
              "2494          NaN       109  ...   97.0               9.57         97.0   \n",
              "2495          NaN       525  ...    5.0               2.62         45.0   \n",
              "2496          NaN         0  ...    8.0               5.25         81.0   \n",
              "\n",
              "       HIV/AIDS           GDP  Population   thinness  1-19 years  \\\n",
              "0           0.1  49638.771300   5438972.0                    0.9   \n",
              "1           0.1  34474.137360    127141.0                    2.1   \n",
              "2           0.1           NaN     19239.0                    6.3   \n",
              "3           0.1  11326.219470    297555.0                    2.2   \n",
              "4           0.2   1619.532678     18745.0                    5.7   \n",
              "...         ...           ...         ...                    ...   \n",
              "2492        0.1   2213.914880   9699197.0                    6.6   \n",
              "2493        0.4    138.924927  46627994.0                   13.3   \n",
              "2494        0.1  56928.824800  16445593.0                    1.0   \n",
              "2495        3.6   1186.113250   1117749.0                    NaN   \n",
              "2496        0.2   1655.596261   1165743.0                    1.6   \n",
              "\n",
              "       thinness 5-9 years  Income composition of resources  Schooling  \n",
              "0                     0.8                            0.887       17.0  \n",
              "1                     1.8                            0.902       15.3  \n",
              "2                     6.1                            0.575        9.0  \n",
              "3                     2.3                            0.815       16.0  \n",
              "4                     5.5                            0.559       11.0  \n",
              "...                   ...                              ...        ...  \n",
              "2492                  6.5                            0.646       12.8  \n",
              "2493                 13.7                            0.427        7.6  \n",
              "2494                  0.9                            0.905       16.8  \n",
              "2495                  NaN                            0.417        4.9  \n",
              "2496                  1.6                            0.539        8.2  \n",
              "\n",
              "[2497 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-43fe645f-8c96-437d-a9cf-543d73e87379\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>Year</th>\n",
              "      <th>Status</th>\n",
              "      <th>Life expectancy</th>\n",
              "      <th>Adult Mortality</th>\n",
              "      <th>infant deaths</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>percentage expenditure</th>\n",
              "      <th>Hepatitis B</th>\n",
              "      <th>Measles</th>\n",
              "      <th>...</th>\n",
              "      <th>Polio</th>\n",
              "      <th>Total expenditure</th>\n",
              "      <th>Diphtheria</th>\n",
              "      <th>HIV/AIDS</th>\n",
              "      <th>GDP</th>\n",
              "      <th>Population</th>\n",
              "      <th>thinness  1-19 years</th>\n",
              "      <th>thinness 5-9 years</th>\n",
              "      <th>Income composition of resources</th>\n",
              "      <th>Schooling</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Finland</td>\n",
              "      <td>2013</td>\n",
              "      <td>Developing</td>\n",
              "      <td>87.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.97</td>\n",
              "      <td>6115.496624</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>98.0</td>\n",
              "      <td>9.55</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>49638.771300</td>\n",
              "      <td>5438972.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.887</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Japan</td>\n",
              "      <td>2015</td>\n",
              "      <td>Developed</td>\n",
              "      <td>83.7</td>\n",
              "      <td>55.0</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35</td>\n",
              "      <td>...</td>\n",
              "      <td>99.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>34474.137360</td>\n",
              "      <td>127141.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.902</td>\n",
              "      <td>15.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Syrian Arab Republic</td>\n",
              "      <td>2014</td>\n",
              "      <td>Developing</td>\n",
              "      <td>64.4</td>\n",
              "      <td>294.0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>47.0</td>\n",
              "      <td>594</td>\n",
              "      <td>...</td>\n",
              "      <td>52.0</td>\n",
              "      <td>3.25</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19239.0</td>\n",
              "      <td>6.3</td>\n",
              "      <td>6.1</td>\n",
              "      <td>0.575</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Latvia</td>\n",
              "      <td>2010</td>\n",
              "      <td>Developed</td>\n",
              "      <td>72.8</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.80</td>\n",
              "      <td>1109.969508</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>92.0</td>\n",
              "      <td>6.55</td>\n",
              "      <td>92.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>11326.219470</td>\n",
              "      <td>297555.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.815</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sao Tome and Principe</td>\n",
              "      <td>2013</td>\n",
              "      <td>Developing</td>\n",
              "      <td>67.1</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>200.660099</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>97.0</td>\n",
              "      <td>9.76</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1619.532678</td>\n",
              "      <td>18745.0</td>\n",
              "      <td>5.7</td>\n",
              "      <td>5.5</td>\n",
              "      <td>0.559</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2492</th>\n",
              "      <td>Tunisia</td>\n",
              "      <td>2000</td>\n",
              "      <td>Developing</td>\n",
              "      <td>72.9</td>\n",
              "      <td>112.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.21</td>\n",
              "      <td>264.784220</td>\n",
              "      <td>94.0</td>\n",
              "      <td>47</td>\n",
              "      <td>...</td>\n",
              "      <td>97.0</td>\n",
              "      <td>5.40</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2213.914880</td>\n",
              "      <td>9699197.0</td>\n",
              "      <td>6.6</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0.646</td>\n",
              "      <td>12.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2493</th>\n",
              "      <td>Myanmar</td>\n",
              "      <td>2001</td>\n",
              "      <td>Developing</td>\n",
              "      <td>62.5</td>\n",
              "      <td>239.0</td>\n",
              "      <td>72</td>\n",
              "      <td>0.38</td>\n",
              "      <td>1.917164</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2519</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>138.924927</td>\n",
              "      <td>46627994.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>13.7</td>\n",
              "      <td>0.427</td>\n",
              "      <td>7.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2494</th>\n",
              "      <td>Netherlands</td>\n",
              "      <td>2008</td>\n",
              "      <td>Developed</td>\n",
              "      <td>83.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1</td>\n",
              "      <td>9.62</td>\n",
              "      <td>10873.405540</td>\n",
              "      <td>NaN</td>\n",
              "      <td>109</td>\n",
              "      <td>...</td>\n",
              "      <td>97.0</td>\n",
              "      <td>9.57</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>56928.824800</td>\n",
              "      <td>16445593.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.905</td>\n",
              "      <td>16.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2495</th>\n",
              "      <td>South Sudan</td>\n",
              "      <td>2013</td>\n",
              "      <td>Developing</td>\n",
              "      <td>56.4</td>\n",
              "      <td>345.0</td>\n",
              "      <td>26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>47.444530</td>\n",
              "      <td>NaN</td>\n",
              "      <td>525</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.62</td>\n",
              "      <td>45.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1186.113250</td>\n",
              "      <td>1117749.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.417</td>\n",
              "      <td>4.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>Guatemala</td>\n",
              "      <td>2000</td>\n",
              "      <td>Developing</td>\n",
              "      <td>67.7</td>\n",
              "      <td>221.0</td>\n",
              "      <td>17</td>\n",
              "      <td>2.63</td>\n",
              "      <td>238.736981</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.25</td>\n",
              "      <td>81.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1655.596261</td>\n",
              "      <td>1165743.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.539</td>\n",
              "      <td>8.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2497 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43fe645f-8c96-437d-a9cf-543d73e87379')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-43fe645f-8c96-437d-a9cf-543d73e87379 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-43fe645f-8c96-437d-a9cf-543d73e87379');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"assignment5-test-data.csv\")\n",
        "#Align\n",
        "test_df_aligned, train_df_aligned = test_df.align(train_df, axis = 1)\n",
        "test_df_aligned = test_df_aligned.drop(columns = 'ID')\n",
        "train_df_aligned = train_df_aligned.drop(columns = 'ID')\n",
        "test_df_aligned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "AMkH47pHUHLA",
        "outputId": "35f37bf3-c8b3-4f98-cc43-525a322448ae"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      BMI    HIV/AIDS   thinness  1-19 years   thinness 5-9 years  \\\n",
              "0     67.6        0.1                    0.8                  0.7   \n",
              "1     36.5        4.6                    4.5                  4.6   \n",
              "2     45.0        0.3                    5.5                  5.3   \n",
              "3     51.7        0.1                    2.8                  2.8   \n",
              "4     64.5        0.1                    0.7                  0.7   \n",
              "..     ...        ...                    ...                  ...   \n",
              "436   46.7        0.4                    1.4                  1.4   \n",
              "437   23.2        0.3                    9.9                  9.7   \n",
              "438   19.2       13.6                    6.9                  6.8   \n",
              "439   51.4        0.1                    3.8                  3.8   \n",
              "440   64.5        0.1                    1.8                  1.9   \n",
              "\n",
              "     Adult Mortality  Alcohol   Country  Diphtheria            GDP  \\\n",
              "0                 62     7.91     Malta         76.0   2187.794690   \n",
              "1                  3     6.64     Haiti         53.0    329.782946   \n",
              "2                218     7.56    Guyana         98.0   3944.178173   \n",
              "3                135     7.53    Serbia         93.0   1634.875610   \n",
              "4                 69     6.53     Malta         89.0   1519.549700   \n",
              "..               ...      ...       ...          ...           ...   \n",
              "436              144     4.03      Peru         95.0    259.192958   \n",
              "437              196     0.26   Senegal         92.0    148.912351   \n",
              "438              487     2.08    Zambia          8.0    114.587985   \n",
              "439               14     8.51  Barbados         91.0  15534.157400   \n",
              "440               93    12.71   Czechia         99.0           NaN   \n",
              "\n",
              "     Hepatitis B  ...  Measles   Polio  Population  Schooling      Status  \\\n",
              "0           75.0  ...         0   76.0     41458.0       14.6   Developed   \n",
              "1            NaN  ...         0   56.0   8976552.0        8.1  Developing   \n",
              "2           98.0  ...         0   98.0     75881.0       10.3  Developing   \n",
              "3            NaN  ...        35   93.0    753433.0       13.1  Developing   \n",
              "4           85.0  ...         4   89.0     41268.0       14.2   Developed   \n",
              "..           ...  ...       ...    ...         ...        ...         ...   \n",
              "436          NaN  ...         0   95.0   2661467.0       13.9  Developing   \n",
              "437         92.0  ...        17   89.0    141232.0        8.7  Developing   \n",
              "438          8.0  ...       535   77.0  12725974.0       11.1  Developing   \n",
              "439         91.0  ...         0   91.0         NaN       15.5  Developing   \n",
              "440         99.0  ...        22   99.0         NaN        NaN   Developed   \n",
              "\n",
              "     Total expenditure  Year  infant deaths  percentage expenditure  \\\n",
              "0                 8.30  2010              0              278.068705   \n",
              "1                 5.32  2003             18               44.256871   \n",
              "2                 5.10  2013              0              345.904426   \n",
              "3                 6.89  2001              1              222.506571   \n",
              "4                 8.46  2004              0              203.315750   \n",
              "..                 ...   ...            ...                     ...   \n",
              "436               4.94  2002             16               40.537779   \n",
              "437               4.51  2013             20               11.838532   \n",
              "438               4.37  2007             32               10.851482   \n",
              "439               6.67  2011              0              173.982563   \n",
              "440               7.55  2012              0                0.000000   \n",
              "\n",
              "     under-five deaths   \n",
              "0                     0  \n",
              "1                    25  \n",
              "2                     1  \n",
              "3                     1  \n",
              "4                     0  \n",
              "..                  ...  \n",
              "436                  20  \n",
              "437                  28  \n",
              "438                  51  \n",
              "439                   0  \n",
              "440                   0  \n",
              "\n",
              "[441 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4599b448-9541-4623-9d2f-57f44e453d78\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BMI</th>\n",
              "      <th>HIV/AIDS</th>\n",
              "      <th>thinness  1-19 years</th>\n",
              "      <th>thinness 5-9 years</th>\n",
              "      <th>Adult Mortality</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Country</th>\n",
              "      <th>Diphtheria</th>\n",
              "      <th>GDP</th>\n",
              "      <th>Hepatitis B</th>\n",
              "      <th>...</th>\n",
              "      <th>Measles</th>\n",
              "      <th>Polio</th>\n",
              "      <th>Population</th>\n",
              "      <th>Schooling</th>\n",
              "      <th>Status</th>\n",
              "      <th>Total expenditure</th>\n",
              "      <th>Year</th>\n",
              "      <th>infant deaths</th>\n",
              "      <th>percentage expenditure</th>\n",
              "      <th>under-five deaths</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>67.6</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.7</td>\n",
              "      <td>62</td>\n",
              "      <td>7.91</td>\n",
              "      <td>Malta</td>\n",
              "      <td>76.0</td>\n",
              "      <td>2187.794690</td>\n",
              "      <td>75.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>41458.0</td>\n",
              "      <td>14.6</td>\n",
              "      <td>Developed</td>\n",
              "      <td>8.30</td>\n",
              "      <td>2010</td>\n",
              "      <td>0</td>\n",
              "      <td>278.068705</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>36.5</td>\n",
              "      <td>4.6</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3</td>\n",
              "      <td>6.64</td>\n",
              "      <td>Haiti</td>\n",
              "      <td>53.0</td>\n",
              "      <td>329.782946</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>8976552.0</td>\n",
              "      <td>8.1</td>\n",
              "      <td>Developing</td>\n",
              "      <td>5.32</td>\n",
              "      <td>2003</td>\n",
              "      <td>18</td>\n",
              "      <td>44.256871</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>5.5</td>\n",
              "      <td>5.3</td>\n",
              "      <td>218</td>\n",
              "      <td>7.56</td>\n",
              "      <td>Guyana</td>\n",
              "      <td>98.0</td>\n",
              "      <td>3944.178173</td>\n",
              "      <td>98.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>75881.0</td>\n",
              "      <td>10.3</td>\n",
              "      <td>Developing</td>\n",
              "      <td>5.10</td>\n",
              "      <td>2013</td>\n",
              "      <td>0</td>\n",
              "      <td>345.904426</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>51.7</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2.8</td>\n",
              "      <td>2.8</td>\n",
              "      <td>135</td>\n",
              "      <td>7.53</td>\n",
              "      <td>Serbia</td>\n",
              "      <td>93.0</td>\n",
              "      <td>1634.875610</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>35</td>\n",
              "      <td>93.0</td>\n",
              "      <td>753433.0</td>\n",
              "      <td>13.1</td>\n",
              "      <td>Developing</td>\n",
              "      <td>6.89</td>\n",
              "      <td>2001</td>\n",
              "      <td>1</td>\n",
              "      <td>222.506571</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.7</td>\n",
              "      <td>69</td>\n",
              "      <td>6.53</td>\n",
              "      <td>Malta</td>\n",
              "      <td>89.0</td>\n",
              "      <td>1519.549700</td>\n",
              "      <td>85.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>89.0</td>\n",
              "      <td>41268.0</td>\n",
              "      <td>14.2</td>\n",
              "      <td>Developed</td>\n",
              "      <td>8.46</td>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>203.315750</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>46.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>144</td>\n",
              "      <td>4.03</td>\n",
              "      <td>Peru</td>\n",
              "      <td>95.0</td>\n",
              "      <td>259.192958</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>2661467.0</td>\n",
              "      <td>13.9</td>\n",
              "      <td>Developing</td>\n",
              "      <td>4.94</td>\n",
              "      <td>2002</td>\n",
              "      <td>16</td>\n",
              "      <td>40.537779</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>23.2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>9.9</td>\n",
              "      <td>9.7</td>\n",
              "      <td>196</td>\n",
              "      <td>0.26</td>\n",
              "      <td>Senegal</td>\n",
              "      <td>92.0</td>\n",
              "      <td>148.912351</td>\n",
              "      <td>92.0</td>\n",
              "      <td>...</td>\n",
              "      <td>17</td>\n",
              "      <td>89.0</td>\n",
              "      <td>141232.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>Developing</td>\n",
              "      <td>4.51</td>\n",
              "      <td>2013</td>\n",
              "      <td>20</td>\n",
              "      <td>11.838532</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>19.2</td>\n",
              "      <td>13.6</td>\n",
              "      <td>6.9</td>\n",
              "      <td>6.8</td>\n",
              "      <td>487</td>\n",
              "      <td>2.08</td>\n",
              "      <td>Zambia</td>\n",
              "      <td>8.0</td>\n",
              "      <td>114.587985</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>535</td>\n",
              "      <td>77.0</td>\n",
              "      <td>12725974.0</td>\n",
              "      <td>11.1</td>\n",
              "      <td>Developing</td>\n",
              "      <td>4.37</td>\n",
              "      <td>2007</td>\n",
              "      <td>32</td>\n",
              "      <td>10.851482</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>51.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>3.8</td>\n",
              "      <td>14</td>\n",
              "      <td>8.51</td>\n",
              "      <td>Barbados</td>\n",
              "      <td>91.0</td>\n",
              "      <td>15534.157400</td>\n",
              "      <td>91.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.5</td>\n",
              "      <td>Developing</td>\n",
              "      <td>6.67</td>\n",
              "      <td>2011</td>\n",
              "      <td>0</td>\n",
              "      <td>173.982563</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>64.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>1.9</td>\n",
              "      <td>93</td>\n",
              "      <td>12.71</td>\n",
              "      <td>Czechia</td>\n",
              "      <td>99.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>99.0</td>\n",
              "      <td>...</td>\n",
              "      <td>22</td>\n",
              "      <td>99.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Developed</td>\n",
              "      <td>7.55</td>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>441 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4599b448-9541-4623-9d2f-57f44e453d78')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4599b448-9541-4623-9d2f-57f44e453d78 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4599b448-9541-4623-9d2f-57f44e453d78');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess"
      ],
      "metadata": {
        "id": "wPC0EunqXF-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_df_aligned = test_df_aligned.dropna()\n",
        "# train_df_aligned = train_df_aligned.dropna()\n",
        "#We replace NaN values with mean of each column instead of dropping the rows\n",
        "test_df_aligned = test_df_aligned.fillna(test_df_aligned.mean())\n",
        "train_df_aligned = train_df_aligned.fillna(train_df_aligned.mean())\n",
        "test_df_aligned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "-Mjw1qGxXIVv",
        "outputId": "5e59cf45-cfeb-41ee-f0d7-f7df23911762"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-122-f2c8cefd4f59>:4: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  test_df_aligned = test_df_aligned.fillna(test_df_aligned.mean())\n",
            "<ipython-input-122-f2c8cefd4f59>:5: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  train_df_aligned = train_df_aligned.fillna(train_df_aligned.mean())\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      BMI    HIV/AIDS   thinness  1-19 years   thinness 5-9 years  \\\n",
              "0     67.6        0.1                    0.8                  0.7   \n",
              "1     36.5        4.6                    4.5                  4.6   \n",
              "2     45.0        0.3                    5.5                  5.3   \n",
              "3     51.7        0.1                    2.8                  2.8   \n",
              "4     64.5        0.1                    0.7                  0.7   \n",
              "..     ...        ...                    ...                  ...   \n",
              "436   46.7        0.4                    1.4                  1.4   \n",
              "437   23.2        0.3                    9.9                  9.7   \n",
              "438   19.2       13.6                    6.9                  6.8   \n",
              "439   51.4        0.1                    3.8                  3.8   \n",
              "440   64.5        0.1                    1.8                  1.9   \n",
              "\n",
              "     Adult Mortality  Alcohol   Country  Diphtheria            GDP  \\\n",
              "0                 62     7.91     Malta         76.0   2187.794690   \n",
              "1                  3     6.64     Haiti         53.0    329.782946   \n",
              "2                218     7.56    Guyana         98.0   3944.178173   \n",
              "3                135     7.53    Serbia         93.0   1634.875610   \n",
              "4                 69     6.53     Malta         89.0   1519.549700   \n",
              "..               ...      ...       ...          ...           ...   \n",
              "436              144     4.03      Peru         95.0    259.192958   \n",
              "437              196     0.26   Senegal         92.0    148.912351   \n",
              "438              487     2.08    Zambia          8.0    114.587985   \n",
              "439               14     8.51  Barbados         91.0  15534.157400   \n",
              "440               93    12.71   Czechia         99.0   7784.991238   \n",
              "\n",
              "     Hepatitis B  ...  Measles   Polio    Population  Schooling      Status  \\\n",
              "0      75.000000  ...         0   76.0  4.145800e+04  14.600000   Developed   \n",
              "1      80.759207  ...         0   56.0  8.976552e+06   8.100000  Developing   \n",
              "2      98.000000  ...         0   98.0  7.588100e+04  10.300000  Developing   \n",
              "3      80.759207  ...        35   93.0  7.534330e+05  13.100000  Developing   \n",
              "4      85.000000  ...         4   89.0  4.126800e+04  14.200000   Developed   \n",
              "..           ...  ...       ...    ...           ...        ...         ...   \n",
              "436    80.759207  ...         0   95.0  2.661467e+06  13.900000  Developing   \n",
              "437    92.000000  ...        17   89.0  1.412320e+05   8.700000  Developing   \n",
              "438     8.000000  ...       535   77.0  1.272597e+07  11.100000  Developing   \n",
              "439    91.000000  ...         0   91.0  9.992214e+06  15.500000  Developing   \n",
              "440    99.000000  ...        22   99.0  9.992214e+06  12.115348   Developed   \n",
              "\n",
              "     Total expenditure  Year  infant deaths  percentage expenditure  \\\n",
              "0                 8.30  2010              0              278.068705   \n",
              "1                 5.32  2003             18               44.256871   \n",
              "2                 5.10  2013              0              345.904426   \n",
              "3                 6.89  2001              1              222.506571   \n",
              "4                 8.46  2004              0              203.315750   \n",
              "..                 ...   ...            ...                     ...   \n",
              "436               4.94  2002             16               40.537779   \n",
              "437               4.51  2013             20               11.838532   \n",
              "438               4.37  2007             32               10.851482   \n",
              "439               6.67  2011              0              173.982563   \n",
              "440               7.55  2012              0                0.000000   \n",
              "\n",
              "     under-five deaths   \n",
              "0                     0  \n",
              "1                    25  \n",
              "2                     1  \n",
              "3                     1  \n",
              "4                     0  \n",
              "..                  ...  \n",
              "436                  20  \n",
              "437                  28  \n",
              "438                  51  \n",
              "439                   0  \n",
              "440                   0  \n",
              "\n",
              "[441 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df091aab-afa0-4f13-9a61-f08870f8262f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BMI</th>\n",
              "      <th>HIV/AIDS</th>\n",
              "      <th>thinness  1-19 years</th>\n",
              "      <th>thinness 5-9 years</th>\n",
              "      <th>Adult Mortality</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Country</th>\n",
              "      <th>Diphtheria</th>\n",
              "      <th>GDP</th>\n",
              "      <th>Hepatitis B</th>\n",
              "      <th>...</th>\n",
              "      <th>Measles</th>\n",
              "      <th>Polio</th>\n",
              "      <th>Population</th>\n",
              "      <th>Schooling</th>\n",
              "      <th>Status</th>\n",
              "      <th>Total expenditure</th>\n",
              "      <th>Year</th>\n",
              "      <th>infant deaths</th>\n",
              "      <th>percentage expenditure</th>\n",
              "      <th>under-five deaths</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>67.6</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.7</td>\n",
              "      <td>62</td>\n",
              "      <td>7.91</td>\n",
              "      <td>Malta</td>\n",
              "      <td>76.0</td>\n",
              "      <td>2187.794690</td>\n",
              "      <td>75.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>4.145800e+04</td>\n",
              "      <td>14.600000</td>\n",
              "      <td>Developed</td>\n",
              "      <td>8.30</td>\n",
              "      <td>2010</td>\n",
              "      <td>0</td>\n",
              "      <td>278.068705</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>36.5</td>\n",
              "      <td>4.6</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3</td>\n",
              "      <td>6.64</td>\n",
              "      <td>Haiti</td>\n",
              "      <td>53.0</td>\n",
              "      <td>329.782946</td>\n",
              "      <td>80.759207</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>8.976552e+06</td>\n",
              "      <td>8.100000</td>\n",
              "      <td>Developing</td>\n",
              "      <td>5.32</td>\n",
              "      <td>2003</td>\n",
              "      <td>18</td>\n",
              "      <td>44.256871</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>5.5</td>\n",
              "      <td>5.3</td>\n",
              "      <td>218</td>\n",
              "      <td>7.56</td>\n",
              "      <td>Guyana</td>\n",
              "      <td>98.0</td>\n",
              "      <td>3944.178173</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>7.588100e+04</td>\n",
              "      <td>10.300000</td>\n",
              "      <td>Developing</td>\n",
              "      <td>5.10</td>\n",
              "      <td>2013</td>\n",
              "      <td>0</td>\n",
              "      <td>345.904426</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>51.7</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2.8</td>\n",
              "      <td>2.8</td>\n",
              "      <td>135</td>\n",
              "      <td>7.53</td>\n",
              "      <td>Serbia</td>\n",
              "      <td>93.0</td>\n",
              "      <td>1634.875610</td>\n",
              "      <td>80.759207</td>\n",
              "      <td>...</td>\n",
              "      <td>35</td>\n",
              "      <td>93.0</td>\n",
              "      <td>7.534330e+05</td>\n",
              "      <td>13.100000</td>\n",
              "      <td>Developing</td>\n",
              "      <td>6.89</td>\n",
              "      <td>2001</td>\n",
              "      <td>1</td>\n",
              "      <td>222.506571</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.7</td>\n",
              "      <td>69</td>\n",
              "      <td>6.53</td>\n",
              "      <td>Malta</td>\n",
              "      <td>89.0</td>\n",
              "      <td>1519.549700</td>\n",
              "      <td>85.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>89.0</td>\n",
              "      <td>4.126800e+04</td>\n",
              "      <td>14.200000</td>\n",
              "      <td>Developed</td>\n",
              "      <td>8.46</td>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>203.315750</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>46.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>144</td>\n",
              "      <td>4.03</td>\n",
              "      <td>Peru</td>\n",
              "      <td>95.0</td>\n",
              "      <td>259.192958</td>\n",
              "      <td>80.759207</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>2.661467e+06</td>\n",
              "      <td>13.900000</td>\n",
              "      <td>Developing</td>\n",
              "      <td>4.94</td>\n",
              "      <td>2002</td>\n",
              "      <td>16</td>\n",
              "      <td>40.537779</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>23.2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>9.9</td>\n",
              "      <td>9.7</td>\n",
              "      <td>196</td>\n",
              "      <td>0.26</td>\n",
              "      <td>Senegal</td>\n",
              "      <td>92.0</td>\n",
              "      <td>148.912351</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>17</td>\n",
              "      <td>89.0</td>\n",
              "      <td>1.412320e+05</td>\n",
              "      <td>8.700000</td>\n",
              "      <td>Developing</td>\n",
              "      <td>4.51</td>\n",
              "      <td>2013</td>\n",
              "      <td>20</td>\n",
              "      <td>11.838532</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>19.2</td>\n",
              "      <td>13.6</td>\n",
              "      <td>6.9</td>\n",
              "      <td>6.8</td>\n",
              "      <td>487</td>\n",
              "      <td>2.08</td>\n",
              "      <td>Zambia</td>\n",
              "      <td>8.0</td>\n",
              "      <td>114.587985</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>535</td>\n",
              "      <td>77.0</td>\n",
              "      <td>1.272597e+07</td>\n",
              "      <td>11.100000</td>\n",
              "      <td>Developing</td>\n",
              "      <td>4.37</td>\n",
              "      <td>2007</td>\n",
              "      <td>32</td>\n",
              "      <td>10.851482</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>51.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>3.8</td>\n",
              "      <td>14</td>\n",
              "      <td>8.51</td>\n",
              "      <td>Barbados</td>\n",
              "      <td>91.0</td>\n",
              "      <td>15534.157400</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>9.992214e+06</td>\n",
              "      <td>15.500000</td>\n",
              "      <td>Developing</td>\n",
              "      <td>6.67</td>\n",
              "      <td>2011</td>\n",
              "      <td>0</td>\n",
              "      <td>173.982563</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>64.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>1.9</td>\n",
              "      <td>93</td>\n",
              "      <td>12.71</td>\n",
              "      <td>Czechia</td>\n",
              "      <td>99.0</td>\n",
              "      <td>7784.991238</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>22</td>\n",
              "      <td>99.0</td>\n",
              "      <td>9.992214e+06</td>\n",
              "      <td>12.115348</td>\n",
              "      <td>Developed</td>\n",
              "      <td>7.55</td>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>441 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df091aab-afa0-4f13-9a61-f08870f8262f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-df091aab-afa0-4f13-9a61-f08870f8262f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-df091aab-afa0-4f13-9a61-f08870f8262f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One-Hot Encoding\n",
        "test_df_aligned_encoded = pd.get_dummies(test_df_aligned, columns=['Country', 'Status'])\n",
        "train_df_aligned_encoded = pd.get_dummies(train_df_aligned, columns=['Country', 'Status'])\n",
        "test_df_aligned_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "mdYhBbUIYZkC",
        "outputId": "a1950e1a-c748-4f0e-bced-a1c30c1d5ed3"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      BMI    HIV/AIDS   thinness  1-19 years   thinness 5-9 years  \\\n",
              "0     67.6        0.1                    0.8                  0.7   \n",
              "1     36.5        4.6                    4.5                  4.6   \n",
              "2     45.0        0.3                    5.5                  5.3   \n",
              "3     51.7        0.1                    2.8                  2.8   \n",
              "4     64.5        0.1                    0.7                  0.7   \n",
              "..     ...        ...                    ...                  ...   \n",
              "436   46.7        0.4                    1.4                  1.4   \n",
              "437   23.2        0.3                    9.9                  9.7   \n",
              "438   19.2       13.6                    6.9                  6.8   \n",
              "439   51.4        0.1                    3.8                  3.8   \n",
              "440   64.5        0.1                    1.8                  1.9   \n",
              "\n",
              "     Adult Mortality  Alcohol  Diphtheria            GDP  Hepatitis B  \\\n",
              "0                 62     7.91         76.0   2187.794690    75.000000   \n",
              "1                  3     6.64         53.0    329.782946    80.759207   \n",
              "2                218     7.56         98.0   3944.178173    98.000000   \n",
              "3                135     7.53         93.0   1634.875610    80.759207   \n",
              "4                 69     6.53         89.0   1519.549700    85.000000   \n",
              "..               ...      ...          ...           ...          ...   \n",
              "436              144     4.03         95.0    259.192958    80.759207   \n",
              "437              196     0.26         92.0    148.912351    92.000000   \n",
              "438              487     2.08          8.0    114.587985     8.000000   \n",
              "439               14     8.51         91.0  15534.157400    91.000000   \n",
              "440               93    12.71         99.0   7784.991238    99.000000   \n",
              "\n",
              "     Income composition of resources  ...  \\\n",
              "0                           0.819000  ...   \n",
              "1                           0.447000  ...   \n",
              "2                           0.633000  ...   \n",
              "3                           0.709000  ...   \n",
              "4                           0.797000  ...   \n",
              "..                               ...  ...   \n",
              "436                         0.686000  ...   \n",
              "437                         0.474000  ...   \n",
              "438                         0.492000  ...   \n",
              "439                         0.780000  ...   \n",
              "440                         0.635902  ...   \n",
              "\n",
              "     Country_United Republic of Tanzania  Country_United States of America  \\\n",
              "0                                      0                                 0   \n",
              "1                                      0                                 0   \n",
              "2                                      0                                 0   \n",
              "3                                      0                                 0   \n",
              "4                                      0                                 0   \n",
              "..                                   ...                               ...   \n",
              "436                                    0                                 0   \n",
              "437                                    0                                 0   \n",
              "438                                    0                                 0   \n",
              "439                                    0                                 0   \n",
              "440                                    0                                 0   \n",
              "\n",
              "     Country_Uruguay  Country_Uzbekistan  Country_Viet Nam  Country_Yemen  \\\n",
              "0                  0                   0                 0              0   \n",
              "1                  0                   0                 0              0   \n",
              "2                  0                   0                 0              0   \n",
              "3                  0                   0                 0              0   \n",
              "4                  0                   0                 0              0   \n",
              "..               ...                 ...               ...            ...   \n",
              "436                0                   0                 0              0   \n",
              "437                0                   0                 0              0   \n",
              "438                0                   0                 0              0   \n",
              "439                0                   0                 0              0   \n",
              "440                0                   0                 0              0   \n",
              "\n",
              "     Country_Zambia  Country_Zimbabwe  Status_Developed  Status_Developing  \n",
              "0                 0                 0                 1                  0  \n",
              "1                 0                 0                 0                  1  \n",
              "2                 0                 0                 0                  1  \n",
              "3                 0                 0                 0                  1  \n",
              "4                 0                 0                 1                  0  \n",
              "..              ...               ...               ...                ...  \n",
              "436               0                 0                 0                  1  \n",
              "437               0                 0                 0                  1  \n",
              "438               1                 0                 0                  1  \n",
              "439               0                 0                 0                  1  \n",
              "440               0                 0                 1                  0  \n",
              "\n",
              "[441 rows x 190 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0551f869-6207-4fb3-94b2-704a271cfc63\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BMI</th>\n",
              "      <th>HIV/AIDS</th>\n",
              "      <th>thinness  1-19 years</th>\n",
              "      <th>thinness 5-9 years</th>\n",
              "      <th>Adult Mortality</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Diphtheria</th>\n",
              "      <th>GDP</th>\n",
              "      <th>Hepatitis B</th>\n",
              "      <th>Income composition of resources</th>\n",
              "      <th>...</th>\n",
              "      <th>Country_United Republic of Tanzania</th>\n",
              "      <th>Country_United States of America</th>\n",
              "      <th>Country_Uruguay</th>\n",
              "      <th>Country_Uzbekistan</th>\n",
              "      <th>Country_Viet Nam</th>\n",
              "      <th>Country_Yemen</th>\n",
              "      <th>Country_Zambia</th>\n",
              "      <th>Country_Zimbabwe</th>\n",
              "      <th>Status_Developed</th>\n",
              "      <th>Status_Developing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>67.6</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.7</td>\n",
              "      <td>62</td>\n",
              "      <td>7.91</td>\n",
              "      <td>76.0</td>\n",
              "      <td>2187.794690</td>\n",
              "      <td>75.000000</td>\n",
              "      <td>0.819000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>36.5</td>\n",
              "      <td>4.6</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3</td>\n",
              "      <td>6.64</td>\n",
              "      <td>53.0</td>\n",
              "      <td>329.782946</td>\n",
              "      <td>80.759207</td>\n",
              "      <td>0.447000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>5.5</td>\n",
              "      <td>5.3</td>\n",
              "      <td>218</td>\n",
              "      <td>7.56</td>\n",
              "      <td>98.0</td>\n",
              "      <td>3944.178173</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>0.633000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>51.7</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2.8</td>\n",
              "      <td>2.8</td>\n",
              "      <td>135</td>\n",
              "      <td>7.53</td>\n",
              "      <td>93.0</td>\n",
              "      <td>1634.875610</td>\n",
              "      <td>80.759207</td>\n",
              "      <td>0.709000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.7</td>\n",
              "      <td>69</td>\n",
              "      <td>6.53</td>\n",
              "      <td>89.0</td>\n",
              "      <td>1519.549700</td>\n",
              "      <td>85.000000</td>\n",
              "      <td>0.797000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>46.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>144</td>\n",
              "      <td>4.03</td>\n",
              "      <td>95.0</td>\n",
              "      <td>259.192958</td>\n",
              "      <td>80.759207</td>\n",
              "      <td>0.686000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>23.2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>9.9</td>\n",
              "      <td>9.7</td>\n",
              "      <td>196</td>\n",
              "      <td>0.26</td>\n",
              "      <td>92.0</td>\n",
              "      <td>148.912351</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>0.474000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>19.2</td>\n",
              "      <td>13.6</td>\n",
              "      <td>6.9</td>\n",
              "      <td>6.8</td>\n",
              "      <td>487</td>\n",
              "      <td>2.08</td>\n",
              "      <td>8.0</td>\n",
              "      <td>114.587985</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.492000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>51.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>3.8</td>\n",
              "      <td>14</td>\n",
              "      <td>8.51</td>\n",
              "      <td>91.0</td>\n",
              "      <td>15534.157400</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>64.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>1.9</td>\n",
              "      <td>93</td>\n",
              "      <td>12.71</td>\n",
              "      <td>99.0</td>\n",
              "      <td>7784.991238</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>0.635902</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>441 rows × 190 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0551f869-6207-4fb3-94b2-704a271cfc63')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0551f869-6207-4fb3-94b2-704a271cfc63 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0551f869-6207-4fb3-94b2-704a271cfc63');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Align\n",
        "test_df_aligned_encoded_aligned, train_df_aligned_encoded_aligned = test_df_aligned_encoded.align(train_df_aligned_encoded, axis = 1)\n",
        "#fill nans with 0 (Country doesn't exist)\n",
        "test_df_aligned_encoded_aligned = test_df_aligned_encoded_aligned.fillna(0)\n",
        "train_df_aligned_encoded_aligned = train_df_aligned_encoded_aligned.fillna(0)\n",
        "test_df_aligned_encoded_aligned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "S-hAKcvYa-r7",
        "outputId": "62b8939f-4752-4a3f-91bb-e0a8946f03f9"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      BMI    HIV/AIDS   thinness  1-19 years   thinness 5-9 years  \\\n",
              "0     67.6        0.1                    0.8                  0.7   \n",
              "1     36.5        4.6                    4.5                  4.6   \n",
              "2     45.0        0.3                    5.5                  5.3   \n",
              "3     51.7        0.1                    2.8                  2.8   \n",
              "4     64.5        0.1                    0.7                  0.7   \n",
              "..     ...        ...                    ...                  ...   \n",
              "436   46.7        0.4                    1.4                  1.4   \n",
              "437   23.2        0.3                    9.9                  9.7   \n",
              "438   19.2       13.6                    6.9                  6.8   \n",
              "439   51.4        0.1                    3.8                  3.8   \n",
              "440   64.5        0.1                    1.8                  1.9   \n",
              "\n",
              "     Adult Mortality  Alcohol  Country_Afghanistan  Country_Albania  \\\n",
              "0                 62     7.91                    0                0   \n",
              "1                  3     6.64                    0                0   \n",
              "2                218     7.56                    0                0   \n",
              "3                135     7.53                    0                0   \n",
              "4                 69     6.53                    0                0   \n",
              "..               ...      ...                  ...              ...   \n",
              "436              144     4.03                    0                0   \n",
              "437              196     0.26                    0                0   \n",
              "438              487     2.08                    0                0   \n",
              "439               14     8.51                    0                0   \n",
              "440               93    12.71                    0                0   \n",
              "\n",
              "     Country_Algeria  Country_Angola  ...  Polio    Population  Schooling  \\\n",
              "0                  0               0  ...   76.0  4.145800e+04  14.600000   \n",
              "1                  0               0  ...   56.0  8.976552e+06   8.100000   \n",
              "2                  0               0  ...   98.0  7.588100e+04  10.300000   \n",
              "3                  0               0  ...   93.0  7.534330e+05  13.100000   \n",
              "4                  0               0  ...   89.0  4.126800e+04  14.200000   \n",
              "..               ...             ...  ...    ...           ...        ...   \n",
              "436                0               0  ...   95.0  2.661467e+06  13.900000   \n",
              "437                0               0  ...   89.0  1.412320e+05   8.700000   \n",
              "438                0               0  ...   77.0  1.272597e+07  11.100000   \n",
              "439                0               0  ...   91.0  9.992214e+06  15.500000   \n",
              "440                0               0  ...   99.0  9.992214e+06  12.115348   \n",
              "\n",
              "     Status_Developed  Status_Developing  Total expenditure  Year  \\\n",
              "0                   1                  0               8.30  2010   \n",
              "1                   0                  1               5.32  2003   \n",
              "2                   0                  1               5.10  2013   \n",
              "3                   0                  1               6.89  2001   \n",
              "4                   1                  0               8.46  2004   \n",
              "..                ...                ...                ...   ...   \n",
              "436                 0                  1               4.94  2002   \n",
              "437                 0                  1               4.51  2013   \n",
              "438                 0                  1               4.37  2007   \n",
              "439                 0                  1               6.67  2011   \n",
              "440                 1                  0               7.55  2012   \n",
              "\n",
              "     infant deaths  percentage expenditure  under-five deaths   \n",
              "0                0              278.068705                   0  \n",
              "1               18               44.256871                  25  \n",
              "2                0              345.904426                   1  \n",
              "3                1              222.506571                   1  \n",
              "4                0              203.315750                   0  \n",
              "..             ...                     ...                 ...  \n",
              "436             16               40.537779                  20  \n",
              "437             20               11.838532                  28  \n",
              "438             32               10.851482                  51  \n",
              "439              0              173.982563                   0  \n",
              "440              0                0.000000                   0  \n",
              "\n",
              "[441 rows x 215 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dfed2cf6-f750-4601-9e54-c488cef34a3d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BMI</th>\n",
              "      <th>HIV/AIDS</th>\n",
              "      <th>thinness  1-19 years</th>\n",
              "      <th>thinness 5-9 years</th>\n",
              "      <th>Adult Mortality</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Country_Afghanistan</th>\n",
              "      <th>Country_Albania</th>\n",
              "      <th>Country_Algeria</th>\n",
              "      <th>Country_Angola</th>\n",
              "      <th>...</th>\n",
              "      <th>Polio</th>\n",
              "      <th>Population</th>\n",
              "      <th>Schooling</th>\n",
              "      <th>Status_Developed</th>\n",
              "      <th>Status_Developing</th>\n",
              "      <th>Total expenditure</th>\n",
              "      <th>Year</th>\n",
              "      <th>infant deaths</th>\n",
              "      <th>percentage expenditure</th>\n",
              "      <th>under-five deaths</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>67.6</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.7</td>\n",
              "      <td>62</td>\n",
              "      <td>7.91</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>76.0</td>\n",
              "      <td>4.145800e+04</td>\n",
              "      <td>14.600000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8.30</td>\n",
              "      <td>2010</td>\n",
              "      <td>0</td>\n",
              "      <td>278.068705</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>36.5</td>\n",
              "      <td>4.6</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3</td>\n",
              "      <td>6.64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>56.0</td>\n",
              "      <td>8.976552e+06</td>\n",
              "      <td>8.100000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.32</td>\n",
              "      <td>2003</td>\n",
              "      <td>18</td>\n",
              "      <td>44.256871</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>5.5</td>\n",
              "      <td>5.3</td>\n",
              "      <td>218</td>\n",
              "      <td>7.56</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>98.0</td>\n",
              "      <td>7.588100e+04</td>\n",
              "      <td>10.300000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.10</td>\n",
              "      <td>2013</td>\n",
              "      <td>0</td>\n",
              "      <td>345.904426</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>51.7</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2.8</td>\n",
              "      <td>2.8</td>\n",
              "      <td>135</td>\n",
              "      <td>7.53</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>93.0</td>\n",
              "      <td>7.534330e+05</td>\n",
              "      <td>13.100000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.89</td>\n",
              "      <td>2001</td>\n",
              "      <td>1</td>\n",
              "      <td>222.506571</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.7</td>\n",
              "      <td>69</td>\n",
              "      <td>6.53</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>89.0</td>\n",
              "      <td>4.126800e+04</td>\n",
              "      <td>14.200000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8.46</td>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>203.315750</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>46.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>144</td>\n",
              "      <td>4.03</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>95.0</td>\n",
              "      <td>2.661467e+06</td>\n",
              "      <td>13.900000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.94</td>\n",
              "      <td>2002</td>\n",
              "      <td>16</td>\n",
              "      <td>40.537779</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>23.2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>9.9</td>\n",
              "      <td>9.7</td>\n",
              "      <td>196</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>89.0</td>\n",
              "      <td>1.412320e+05</td>\n",
              "      <td>8.700000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.51</td>\n",
              "      <td>2013</td>\n",
              "      <td>20</td>\n",
              "      <td>11.838532</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>19.2</td>\n",
              "      <td>13.6</td>\n",
              "      <td>6.9</td>\n",
              "      <td>6.8</td>\n",
              "      <td>487</td>\n",
              "      <td>2.08</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>1.272597e+07</td>\n",
              "      <td>11.100000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.37</td>\n",
              "      <td>2007</td>\n",
              "      <td>32</td>\n",
              "      <td>10.851482</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>51.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>3.8</td>\n",
              "      <td>14</td>\n",
              "      <td>8.51</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>91.0</td>\n",
              "      <td>9.992214e+06</td>\n",
              "      <td>15.500000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.67</td>\n",
              "      <td>2011</td>\n",
              "      <td>0</td>\n",
              "      <td>173.982563</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>64.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>1.9</td>\n",
              "      <td>93</td>\n",
              "      <td>12.71</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>99.0</td>\n",
              "      <td>9.992214e+06</td>\n",
              "      <td>12.115348</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.55</td>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>441 rows × 215 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dfed2cf6-f750-4601-9e54-c488cef34a3d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dfed2cf6-f750-4601-9e54-c488cef34a3d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dfed2cf6-f750-4601-9e54-c488cef34a3d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train, y_train\n",
        "x_train_df_aligned_encoded_aligned = train_df_aligned_encoded_aligned.drop(\"Life expectancy \", axis = 1)\n",
        "X_train_kaggle = x_train_df_aligned_encoded_aligned.to_numpy()\n",
        "y_train_kaggle = train_df_aligned_encoded_aligned.loc[:, \"Life expectancy \"].values\n",
        "X_train_kaggle.shape, y_train_kaggle.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3J5lwEJbuxU",
        "outputId": "47f60784-2c26-458b-df38-ea554aebaf9c"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2497, 214), (2497,))"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_test, y_test\n",
        "x_test_df_aligned_encoded_aligned = test_df_aligned_encoded_aligned.drop(\"Life expectancy \", axis = 1)\n",
        "X_test_kaggle = x_test_df_aligned_encoded_aligned.to_numpy()\n",
        "y_test_kaggle = test_df_aligned_encoded_aligned.loc[:, \"Life expectancy \"].values\n",
        "X_test_kaggle.shape, y_test_kaggle.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDIXz5itcQ4y",
        "outputId": "698aaf77-4d95-4a6a-b71c-c0569208cebc"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((441, 214), (441,))"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize\n",
        "X_train_kaggle = preprocessing.normalize(X_train_kaggle)\n",
        "X_test_kaggle = preprocessing.normalize(X_test_kaggle)\n",
        "X_train_kaggle.sum(axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4ta0ecjcn-x",
        "outputId": "8c7a1a0c-3588-4e22-8187-c6966637c9af"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.01066269, 1.24528243, 1.43789852, ..., 1.00427229, 1.00391589,\n",
              "       1.00372876])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "TrIBFYCwc6FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_regression_custom_kaggle = RidgeRegression(0.0001, 0.9, 20000)\n",
        "y_train_kaggle_reshaped = y_train_kaggle.reshape(-1, 1)\n",
        "ridge_regression_custom_kaggle.train(X_train_kaggle, y_train_kaggle_reshaped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psOnxl2qczCR",
        "outputId": "e1af8795-c5f6-4be2-c64b-616563a9723d"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss function for the iteration 10----->69.81936376828716 :)\n",
            "The loss function for the iteration 20----->48.85651373722139 :)\n",
            "The loss function for the iteration 30----->46.516803842753326 :)\n",
            "The loss function for the iteration 40----->45.26151592792854 :)\n",
            "The loss function for the iteration 50----->44.56381069396811 :)\n",
            "The loss function for the iteration 60----->44.172187802739145 :)\n",
            "The loss function for the iteration 70----->43.948908701226614 :)\n",
            "The loss function for the iteration 80----->43.818438144451285 :)\n",
            "The loss function for the iteration 90----->43.73933682549656 :)\n",
            "The loss function for the iteration 100----->43.68885620013486 :)\n",
            "The loss function for the iteration 110----->43.65449474202852 :)\n",
            "The loss function for the iteration 120----->43.6293697506694 :)\n",
            "The loss function for the iteration 130----->43.60968046733731 :)\n",
            "The loss function for the iteration 140----->43.593317444679144 :)\n",
            "The loss function for the iteration 150----->43.579100187417744 :)\n",
            "The loss function for the iteration 160----->43.56635915858584 :)\n",
            "The loss function for the iteration 170----->43.55470654257844 :)\n",
            "The loss function for the iteration 180----->43.54391047552527 :)\n",
            "The loss function for the iteration 190----->43.53382599528998 :)\n",
            "The loss function for the iteration 200----->43.52435708810792 :)\n",
            "The loss function for the iteration 210----->43.51543578731544 :)\n",
            "The loss function for the iteration 220----->43.50701062578173 :)\n",
            "The loss function for the iteration 230----->43.49904022201112 :)\n",
            "The loss function for the iteration 240----->43.49148968640341 :)\n",
            "The loss function for the iteration 250----->43.48432857915753 :)\n",
            "The loss function for the iteration 260----->43.47752972410034 :)\n",
            "The loss function for the iteration 270----->43.471068496704845 :)\n",
            "The loss function for the iteration 280----->43.46492237668614 :)\n",
            "The loss function for the iteration 290----->43.45907064993316 :)\n",
            "The loss function for the iteration 300----->43.453494196285035 :)\n",
            "The loss function for the iteration 310----->43.44817532805015 :)\n",
            "The loss function for the iteration 320----->43.44309765974878 :)\n",
            "The loss function for the iteration 330----->43.43824599812265 :)\n",
            "The loss function for the iteration 340----->43.433606246167045 :)\n",
            "The loss function for the iteration 350----->43.429165317542186 :)\n",
            "The loss function for the iteration 360----->43.42491105916319 :)\n",
            "The loss function for the iteration 370----->43.42083218057356 :)\n",
            "The loss function for the iteration 380----->43.41691818916334 :)\n",
            "The loss function for the iteration 390----->43.413159330556674 :)\n",
            "The loss function for the iteration 400----->43.40954653364927 :)\n",
            "The loss function for the iteration 410----->43.40607135987385 :)\n",
            "The loss function for the iteration 420----->43.40272595633508 :)\n",
            "The loss function for the iteration 430----->43.39950301249993 :)\n",
            "The loss function for the iteration 440----->43.39639572016293 :)\n",
            "The loss function for the iteration 450----->43.39339773643198 :)\n",
            "The loss function for the iteration 460----->43.3905031495027 :)\n",
            "The loss function for the iteration 470----->43.3877064470085 :)\n",
            "The loss function for the iteration 480----->43.3850024867502 :)\n",
            "The loss function for the iteration 490----->43.38238646962498 :)\n",
            "The loss function for the iteration 500----->43.37985391458756 :)\n",
            "The loss function for the iteration 510----->43.37740063549007 :)\n",
            "The loss function for the iteration 520----->43.37502271965845 :)\n",
            "The loss function for the iteration 530----->43.37271650807377 :)\n",
            "The loss function for the iteration 540----->43.37047857703739 :)\n",
            "The loss function for the iteration 550----->43.3683057212078 :)\n",
            "The loss function for the iteration 560----->43.366194937905306 :)\n",
            "The loss function for the iteration 570----->43.364143412589 :)\n",
            "The loss function for the iteration 580----->43.36214850541736 :)\n",
            "The loss function for the iteration 590----->43.36020773881064 :)\n",
            "The loss function for the iteration 600----->43.35831878593966 :)\n",
            "The loss function for the iteration 610----->43.356479460070666 :)\n",
            "The loss function for the iteration 620----->43.3546877047022 :)\n",
            "The loss function for the iteration 630----->43.35294158443384 :)\n",
            "The loss function for the iteration 640----->43.35123927651177 :)\n",
            "The loss function for the iteration 650----->43.34957906300038 :)\n",
            "The loss function for the iteration 660----->43.347959323532315 :)\n",
            "The loss function for the iteration 670----->43.34637852859387 :)\n",
            "The loss function for the iteration 680----->43.34483523330507 :)\n",
            "The loss function for the iteration 690----->43.34332807165751 :)\n",
            "The loss function for the iteration 700----->43.34185575117528 :)\n",
            "The loss function for the iteration 710----->43.34041704796735 :)\n",
            "The loss function for the iteration 720----->43.33901080214202 :)\n",
            "The loss function for the iteration 730----->43.33763591355604 :)\n",
            "The loss function for the iteration 740----->43.336291337873575 :)\n",
            "The loss function for the iteration 750----->43.33497608291161 :)\n",
            "The loss function for the iteration 760----->43.33368920525029 :)\n",
            "The loss function for the iteration 770----->43.33242980708859 :)\n",
            "The loss function for the iteration 780----->43.331197033326625 :)\n",
            "The loss function for the iteration 790----->43.329990068857995 :)\n",
            "The loss function for the iteration 800----->43.328808136056246 :)\n",
            "The loss function for the iteration 810----->43.32765049244094 :)\n",
            "The loss function for the iteration 820----->43.32651642851025 :)\n",
            "The loss function for the iteration 830----->43.32540526572726 :)\n",
            "The loss function for the iteration 840----->43.32431635464879 :)\n",
            "The loss function for the iteration 850----->43.32324907318617 :)\n",
            "The loss function for the iteration 860----->43.32220282498792 :)\n",
            "The loss function for the iteration 870----->43.32117703793567 :)\n",
            "The loss function for the iteration 880----->43.32017116274461 :)\n",
            "The loss function for the iteration 890----->43.31918467166083 :)\n",
            "The loss function for the iteration 900----->43.318217057248525 :)\n",
            "The loss function for the iteration 910----->43.317267831260324 :)\n",
            "The loss function for the iteration 920----->43.31633652358459 :)\n",
            "The loss function for the iteration 930----->43.3154226812641 :)\n",
            "The loss function for the iteration 940----->43.31452586758089 :)\n",
            "The loss function for the iteration 950----->43.31364566120241 :)\n",
            "The loss function for the iteration 960----->43.31278165538439 :)\n",
            "The loss function for the iteration 970----->43.31193345722653 :)\n",
            "The loss function for the iteration 980----->43.31110068697688 :)\n",
            "The loss function for the iteration 990----->43.3102829773816 :)\n",
            "The loss function for the iteration 1000----->43.30947997307674 :)\n",
            "The loss function for the iteration 1010----->43.30869133001899 :)\n",
            "The loss function for the iteration 1020----->43.3079167149526 :)\n",
            "The loss function for the iteration 1030----->43.30715580490989 :)\n",
            "The loss function for the iteration 1040----->43.30640828674294 :)\n",
            "The loss function for the iteration 1050----->43.30567385668427 :)\n",
            "The loss function for the iteration 1060----->43.30495221993434 :)\n",
            "The loss function for the iteration 1070----->43.304243090274156 :)\n",
            "The loss function for the iteration 1080----->43.303546189700945 :)\n",
            "The loss function for the iteration 1090----->43.30286124808561 :)\n",
            "The loss function for the iteration 1100----->43.3021880028502 :)\n",
            "The loss function for the iteration 1110----->43.30152619866399 :)\n",
            "The loss function for the iteration 1120----->43.300875587157044 :)\n",
            "The loss function for the iteration 1130----->43.30023592664996 :)\n",
            "The loss function for the iteration 1140----->43.29960698189872 :)\n",
            "The loss function for the iteration 1150----->43.298988523853446 :)\n",
            "The loss function for the iteration 1160----->43.2983803294305 :)\n",
            "The loss function for the iteration 1170----->43.29778218129653 :)\n",
            "The loss function for the iteration 1180----->43.29719386766417 :)\n",
            "The loss function for the iteration 1190----->43.2966151820981 :)\n",
            "The loss function for the iteration 1200----->43.29604592333134 :)\n",
            "The loss function for the iteration 1210----->43.29548589509051 :)\n",
            "The loss function for the iteration 1220----->43.29493490593004 :)\n",
            "The loss function for the iteration 1230----->43.29439276907429 :)\n",
            "The loss function for the iteration 1240----->43.2938593022674 :)\n",
            "The loss function for the iteration 1250----->43.29333432763013 :)\n",
            "The loss function for the iteration 1260----->43.29281767152344 :)\n",
            "The loss function for the iteration 1270----->43.29230916441829 :)\n",
            "The loss function for the iteration 1280----->43.2918086407713 :)\n",
            "The loss function for the iteration 1290----->43.29131593890585 :)\n",
            "The loss function for the iteration 1300----->43.29083090089847 :)\n",
            "The loss function for the iteration 1310----->43.29035337246988 :)\n",
            "The loss function for the iteration 1320----->43.28988320288083 :)\n",
            "The loss function for the iteration 1330----->43.28942024483206 :)\n",
            "The loss function for the iteration 1340----->43.28896435436835 :)\n",
            "The loss function for the iteration 1350----->43.28851539078645 :)\n",
            "The loss function for the iteration 1360----->43.28807321654648 :)\n",
            "The loss function for the iteration 1370----->43.287637697186824 :)\n",
            "The loss function for the iteration 1380----->43.287208701242264 :)\n",
            "The loss function for the iteration 1390----->43.28678610016498 :)\n",
            "The loss function for the iteration 1400----->43.28636976824859 :)\n",
            "The loss function for the iteration 1410----->43.28595958255487 :)\n",
            "The loss function for the iteration 1420----->43.28555542284292 :)\n",
            "The loss function for the iteration 1430----->43.28515717150101 :)\n",
            "The loss function for the iteration 1440----->43.28476471348052 :)\n",
            "The loss function for the iteration 1450----->43.284377936232225 :)\n",
            "The loss function for the iteration 1460----->43.28399672964458 :)\n",
            "The loss function for the iteration 1470----->43.28362098598411 :)\n",
            "The loss function for the iteration 1480----->43.28325059983757 :)\n",
            "The loss function for the iteration 1490----->43.28288546805605 :)\n",
            "The loss function for the iteration 1500----->43.28252548970068 :)\n",
            "The loss function for the iteration 1510----->43.282170565990086 :)\n",
            "The loss function for the iteration 1520----->43.28182060024939 :)\n",
            "The loss function for the iteration 1530----->43.28147549786064 :)\n",
            "The loss function for the iteration 1540----->43.28113516621486 :)\n",
            "The loss function for the iteration 1550----->43.280799514665276 :)\n",
            "The loss function for the iteration 1560----->43.28046845448203 :)\n",
            "The loss function for the iteration 1570----->43.28014189880807 :)\n",
            "The loss function for the iteration 1580----->43.27981976261633 :)\n",
            "The loss function for the iteration 1590----->43.279501962667965 :)\n",
            "The loss function for the iteration 1600----->43.279188417471914 :)\n",
            "The loss function for the iteration 1610----->43.27887904724533 :)\n",
            "The loss function for the iteration 1620----->43.27857377387517 :)\n",
            "The loss function for the iteration 1630----->43.2782725208808 :)\n",
            "The loss function for the iteration 1640----->43.277975213377545 :)\n",
            "The loss function for the iteration 1650----->43.277681778041135 :)\n",
            "The loss function for the iteration 1660----->43.277392143073136 :)\n",
            "The loss function for the iteration 1670----->43.277106238167185 :)\n",
            "The loss function for the iteration 1680----->43.27682399447609 :)\n",
            "The loss function for the iteration 1690----->43.27654534457979 :)\n",
            "The loss function for the iteration 1700----->43.27627022245401 :)\n",
            "The loss function for the iteration 1710----->43.27599856343976 :)\n",
            "The loss function for the iteration 1720----->43.27573030421354 :)\n",
            "The loss function for the iteration 1730----->43.2754653827583 :)\n",
            "The loss function for the iteration 1740----->43.27520373833501 :)\n",
            "The loss function for the iteration 1750----->43.274945311455 :)\n",
            "The loss function for the iteration 1760----->43.274690043852885 :)\n",
            "The loss function for the iteration 1770----->43.27443787846018 :)\n",
            "The loss function for the iteration 1780----->43.274188759379484 :)\n",
            "The loss function for the iteration 1790----->43.27394263185928 :)\n",
            "The loss function for the iteration 1800----->43.273699442269304 :)\n",
            "The loss function for the iteration 1810----->43.27345913807657 :)\n",
            "The loss function for the iteration 1820----->43.27322166782177 :)\n",
            "The loss function for the iteration 1830----->43.27298698109639 :)\n",
            "The loss function for the iteration 1840----->43.2727550285202 :)\n",
            "The loss function for the iteration 1850----->43.27252576171938 :)\n",
            "The loss function for the iteration 1860----->43.272299133305054 :)\n",
            "The loss function for the iteration 1870----->43.272075096852305 :)\n",
            "The loss function for the iteration 1880----->43.27185360687973 :)\n",
            "The loss function for the iteration 1890----->43.27163461882936 :)\n",
            "The loss function for the iteration 1900----->43.27141808904711 :)\n",
            "The loss function for the iteration 1910----->43.27120397476358 :)\n",
            "The loss function for the iteration 1920----->43.270992234075386 :)\n",
            "The loss function for the iteration 1930----->43.270782825926744 :)\n",
            "The loss function for the iteration 1940----->43.27057571009167 :)\n",
            "The loss function for the iteration 1950----->43.27037084715635 :)\n",
            "The loss function for the iteration 1960----->43.27016819850208 :)\n",
            "The loss function for the iteration 1970----->43.26996772628843 :)\n",
            "The loss function for the iteration 1980----->43.26976939343692 :)\n",
            "The loss function for the iteration 1990----->43.26957316361488 :)\n",
            "The loss function for the iteration 2000----->43.26937900121983 :)\n",
            "The loss function for the iteration 2010----->43.269186871364155 :)\n",
            "The loss function for the iteration 2020----->43.26899673985993 :)\n",
            "The loss function for the iteration 2030----->43.26880857320437 :)\n",
            "The loss function for the iteration 2040----->43.268622338565415 :)\n",
            "The loss function for the iteration 2050----->43.268438003767606 :)\n",
            "The loss function for the iteration 2060----->43.26825553727835 :)\n",
            "The loss function for the iteration 2070----->43.26807490819446 :)\n",
            "The loss function for the iteration 2080----->43.267896086228944 :)\n",
            "The loss function for the iteration 2090----->43.26771904169813 :)\n",
            "The loss function for the iteration 2100----->43.26754374550897 :)\n",
            "The loss function for the iteration 2110----->43.26737016914682 :)\n",
            "The loss function for the iteration 2120----->43.26719828466321 :)\n",
            "The loss function for the iteration 2130----->43.26702806466406 :)\n",
            "The loss function for the iteration 2140----->43.26685948229815 :)\n",
            "The loss function for the iteration 2150----->43.2666925112457 :)\n",
            "The loss function for the iteration 2160----->43.26652712570734 :)\n",
            "The loss function for the iteration 2170----->43.26636330039324 :)\n",
            "The loss function for the iteration 2180----->43.26620101051245 :)\n",
            "The loss function for the iteration 2190----->43.26604023176253 :)\n",
            "The loss function for the iteration 2200----->43.26588094031938 :)\n",
            "The loss function for the iteration 2210----->43.26572311282725 :)\n",
            "The loss function for the iteration 2220----->43.26556672638902 :)\n",
            "The loss function for the iteration 2230----->43.26541175855661 :)\n",
            "The loss function for the iteration 2240----->43.26525818732169 :)\n",
            "The loss function for the iteration 2250----->43.26510599110649 :)\n",
            "The loss function for the iteration 2260----->43.26495514875487 :)\n",
            "The loss function for the iteration 2270----->43.26480563952355 :)\n",
            "The loss function for the iteration 2280----->43.26465744307355 :)\n",
            "The loss function for the iteration 2290----->43.264510539461796 :)\n",
            "The loss function for the iteration 2300----->43.26436490913284 :)\n",
            "The loss function for the iteration 2310----->43.26422053291092 :)\n",
            "The loss function for the iteration 2320----->43.26407739199201 :)\n",
            "The loss function for the iteration 2330----->43.263935467936115 :)\n",
            "The loss function for the iteration 2340----->43.26379474265976 :)\n",
            "The loss function for the iteration 2350----->43.263655198428594 :)\n",
            "The loss function for the iteration 2360----->43.26351681785014 :)\n",
            "The loss function for the iteration 2370----->43.26337958386674 :)\n",
            "The loss function for the iteration 2380----->43.2632434797486 :)\n",
            "The loss function for the iteration 2390----->43.26310848908703 :)\n",
            "The loss function for the iteration 2400----->43.26297459578782 :)\n",
            "The loss function for the iteration 2410----->43.262841784064676 :)\n",
            "The loss function for the iteration 2420----->43.262710038432935 :)\n",
            "The loss function for the iteration 2430----->43.26257934370326 :)\n",
            "The loss function for the iteration 2440----->43.26244968497562 :)\n",
            "The loss function for the iteration 2450----->43.26232104763327 :)\n",
            "The loss function for the iteration 2460----->43.2621934173369 :)\n",
            "The loss function for the iteration 2470----->43.26206678001894 :)\n",
            "The loss function for the iteration 2480----->43.26194112187799 :)\n",
            "The loss function for the iteration 2490----->43.26181642937321 :)\n",
            "The loss function for the iteration 2500----->43.261692689219124 :)\n",
            "The loss function for the iteration 2510----->43.26156988838025 :)\n",
            "The loss function for the iteration 2520----->43.26144801406597 :)\n",
            "The loss function for the iteration 2530----->43.26132705372555 :)\n",
            "The loss function for the iteration 2540----->43.2612069950431 :)\n",
            "The loss function for the iteration 2550----->43.26108782593291 :)\n",
            "The loss function for the iteration 2560----->43.26096953453455 :)\n",
            "The loss function for the iteration 2570----->43.26085210920837 :)\n",
            "The loss function for the iteration 2580----->43.260735538530895 :)\n",
            "The loss function for the iteration 2590----->43.26061981129045 :)\n",
            "The loss function for the iteration 2600----->43.26050491648274 :)\n",
            "The loss function for the iteration 2610----->43.260390843306716 :)\n",
            "The loss function for the iteration 2620----->43.260277581160274 :)\n",
            "The loss function for the iteration 2630----->43.260165119636326 :)\n",
            "The loss function for the iteration 2640----->43.26005344851869 :)\n",
            "The loss function for the iteration 2650----->43.25994255777824 :)\n",
            "The loss function for the iteration 2660----->43.259832437569095 :)\n",
            "The loss function for the iteration 2670----->43.259723078224845 :)\n",
            "The loss function for the iteration 2680----->43.2596144702549 :)\n",
            "The loss function for the iteration 2690----->43.25950660434094 :)\n",
            "The loss function for the iteration 2700----->43.259399471333325 :)\n",
            "The loss function for the iteration 2710----->43.25929306224769 :)\n",
            "The loss function for the iteration 2720----->43.25918736826162 :)\n",
            "The loss function for the iteration 2730----->43.259082380711305 :)\n",
            "The loss function for the iteration 2740----->43.25897809108835 :)\n",
            "The loss function for the iteration 2750----->43.258874491036536 :)\n",
            "The loss function for the iteration 2760----->43.258771572348856 :)\n",
            "The loss function for the iteration 2770----->43.258669326964366 :)\n",
            "The loss function for the iteration 2780----->43.258567746965284 :)\n",
            "The loss function for the iteration 2790----->43.258466824574064 :)\n",
            "The loss function for the iteration 2800----->43.25836655215059 :)\n",
            "The loss function for the iteration 2810----->43.258266922189314 :)\n",
            "The loss function for the iteration 2820----->43.258167927316634 :)\n",
            "The loss function for the iteration 2830----->43.25806956028813 :)\n",
            "The loss function for the iteration 2840----->43.257971813986025 :)\n",
            "The loss function for the iteration 2850----->43.257874681416574 :)\n",
            "The loss function for the iteration 2860----->43.25777815570762 :)\n",
            "The loss function for the iteration 2870----->43.25768223010605 :)\n",
            "The loss function for the iteration 2880----->43.257586897975514 :)\n",
            "The loss function for the iteration 2890----->43.257492152793986 :)\n",
            "The loss function for the iteration 2900----->43.25739798815145 :)\n",
            "The loss function for the iteration 2910----->43.25730439774776 :)\n",
            "The loss function for the iteration 2920----->43.2572113753903 :)\n",
            "The loss function for the iteration 2930----->43.257118914991906 :)\n",
            "The loss function for the iteration 2940----->43.25702701056872 :)\n",
            "The loss function for the iteration 2950----->43.25693565623817 :)\n",
            "The loss function for the iteration 2960----->43.256844846216815 :)\n",
            "The loss function for the iteration 2970----->43.25675457481853 :)\n",
            "The loss function for the iteration 2980----->43.25666483645242 :)\n",
            "The loss function for the iteration 2990----->43.256575625621025 :)\n",
            "The loss function for the iteration 3000----->43.25648693691835 :)\n",
            "The loss function for the iteration 3010----->43.25639876502814 :)\n",
            "The loss function for the iteration 3020----->43.256311104722016 :)\n",
            "The loss function for the iteration 3030----->43.25622395085779 :)\n",
            "The loss function for the iteration 3040----->43.25613729837768 :)\n",
            "The loss function for the iteration 3050----->43.25605114230672 :)\n",
            "The loss function for the iteration 3060----->43.255965477751026 :)\n",
            "The loss function for the iteration 3070----->43.25588029989625 :)\n",
            "The loss function for the iteration 3080----->43.25579560400602 :)\n",
            "The loss function for the iteration 3090----->43.25571138542029 :)\n",
            "The loss function for the iteration 3100----->43.255627639553964 :)\n",
            "The loss function for the iteration 3110----->43.25554436189536 :)\n",
            "The loss function for the iteration 3120----->43.25546154800473 :)\n",
            "The loss function for the iteration 3130----->43.25537919351287 :)\n",
            "The loss function for the iteration 3140----->43.25529729411977 :)\n",
            "The loss function for the iteration 3150----->43.25521584559319 :)\n",
            "The loss function for the iteration 3160----->43.255134843767365 :)\n",
            "The loss function for the iteration 3170----->43.2550542845417 :)\n",
            "The loss function for the iteration 3180----->43.25497416387948 :)\n",
            "The loss function for the iteration 3190----->43.25489447780662 :)\n",
            "The loss function for the iteration 3200----->43.2548152224105 :)\n",
            "The loss function for the iteration 3210----->43.25473639383863 :)\n",
            "The loss function for the iteration 3220----->43.25465798829762 :)\n",
            "The loss function for the iteration 3230----->43.25458000205196 :)\n",
            "The loss function for the iteration 3240----->43.254502431422914 :)\n",
            "The loss function for the iteration 3250----->43.254425272787394 :)\n",
            "The loss function for the iteration 3260----->43.25434852257688 :)\n",
            "The loss function for the iteration 3270----->43.254272177276434 :)\n",
            "The loss function for the iteration 3280----->43.25419623342355 :)\n",
            "The loss function for the iteration 3290----->43.25412068760725 :)\n",
            "The loss function for the iteration 3300----->43.25404553646701 :)\n",
            "The loss function for the iteration 3310----->43.25397077669183 :)\n",
            "The loss function for the iteration 3320----->43.25389640501925 :)\n",
            "The loss function for the iteration 3330----->43.25382241823447 :)\n",
            "The loss function for the iteration 3340----->43.25374881316941 :)\n",
            "The loss function for the iteration 3350----->43.25367558670176 :)\n",
            "The loss function for the iteration 3360----->43.25360273575421 :)\n",
            "The loss function for the iteration 3370----->43.2535302572935 :)\n",
            "The loss function for the iteration 3380----->43.25345814832964 :)\n",
            "The loss function for the iteration 3390----->43.25338640591503 :)\n",
            "The loss function for the iteration 3400----->43.25331502714375 :)\n",
            "The loss function for the iteration 3410----->43.25324400915064 :)\n",
            "The loss function for the iteration 3420----->43.253173349110654 :)\n",
            "The loss function for the iteration 3430----->43.25310304423799 :)\n",
            "The loss function for the iteration 3440----->43.25303309178543 :)\n",
            "The loss function for the iteration 3450----->43.25296348904361 :)\n",
            "The loss function for the iteration 3460----->43.252894233340236 :)\n",
            "The loss function for the iteration 3470----->43.25282532203949 :)\n",
            "The loss function for the iteration 3480----->43.252756752541245 :)\n",
            "The loss function for the iteration 3490----->43.25268852228048 :)\n",
            "The loss function for the iteration 3500----->43.25262062872658 :)\n",
            "The loss function for the iteration 3510----->43.252553069382714 :)\n",
            "The loss function for the iteration 3520----->43.252485841785166 :)\n",
            "The loss function for the iteration 3530----->43.252418943502796 :)\n",
            "The loss function for the iteration 3540----->43.25235237213639 :)\n",
            "The loss function for the iteration 3550----->43.25228612531804 :)\n",
            "The loss function for the iteration 3560----->43.252220200710646 :)\n",
            "The loss function for the iteration 3570----->43.25215459600726 :)\n",
            "The loss function for the iteration 3580----->43.25208930893061 :)\n",
            "The loss function for the iteration 3590----->43.25202433723249 :)\n",
            "The loss function for the iteration 3600----->43.251959678693304 :)\n",
            "The loss function for the iteration 3610----->43.25189533112144 :)\n",
            "The loss function for the iteration 3620----->43.25183129235288 :)\n",
            "The loss function for the iteration 3630----->43.251767560250606 :)\n",
            "The loss function for the iteration 3640----->43.25170413270415 :)\n",
            "The loss function for the iteration 3650----->43.251641007629104 :)\n",
            "The loss function for the iteration 3660----->43.251578182966654 :)\n",
            "The loss function for the iteration 3670----->43.25151565668311 :)\n",
            "The loss function for the iteration 3680----->43.251453426769494 :)\n",
            "The loss function for the iteration 3690----->43.25139149124102 :)\n",
            "The loss function for the iteration 3700----->43.251329848136756 :)\n",
            "The loss function for the iteration 3710----->43.2512684955191 :)\n",
            "The loss function for the iteration 3720----->43.251207431473446 :)\n",
            "The loss function for the iteration 3730----->43.25114665410773 :)\n",
            "The loss function for the iteration 3740----->43.25108616155207 :)\n",
            "The loss function for the iteration 3750----->43.25102595195832 :)\n",
            "The loss function for the iteration 3760----->43.25096602349975 :)\n",
            "The loss function for the iteration 3770----->43.25090637437061 :)\n",
            "The loss function for the iteration 3780----->43.2508470027858 :)\n",
            "The loss function for the iteration 3790----->43.2507879069805 :)\n",
            "The loss function for the iteration 3800----->43.25072908520982 :)\n",
            "The loss function for the iteration 3810----->43.250670535748455 :)\n",
            "The loss function for the iteration 3820----->43.25061225689033 :)\n",
            "The loss function for the iteration 3830----->43.25055424694828 :)\n",
            "The loss function for the iteration 3840----->43.25049650425375 :)\n",
            "The loss function for the iteration 3850----->43.25043902715643 :)\n",
            "The loss function for the iteration 3860----->43.25038181402393 :)\n",
            "The loss function for the iteration 3870----->43.25032486324158 :)\n",
            "The loss function for the iteration 3880----->43.250268173212 :)\n",
            "The loss function for the iteration 3890----->43.25021174235487 :)\n",
            "The loss function for the iteration 3900----->43.250155569106646 :)\n",
            "The loss function for the iteration 3910----->43.25009965192029 :)\n",
            "The loss function for the iteration 3920----->43.250043989264896 :)\n",
            "The loss function for the iteration 3930----->43.24998857962557 :)\n",
            "The loss function for the iteration 3940----->43.24993342150304 :)\n",
            "The loss function for the iteration 3950----->43.24987851341345 :)\n",
            "The loss function for the iteration 3960----->43.24982385388812 :)\n",
            "The loss function for the iteration 3970----->43.24976944147321 :)\n",
            "The loss function for the iteration 3980----->43.24971527472961 :)\n",
            "The loss function for the iteration 3990----->43.249661352232586 :)\n",
            "The loss function for the iteration 4000----->43.24960767257158 :)\n",
            "The loss function for the iteration 4010----->43.24955423435 :)\n",
            "The loss function for the iteration 4020----->43.24950103618495 :)\n",
            "The loss function for the iteration 4030----->43.24944807670709 :)\n",
            "The loss function for the iteration 4040----->43.24939535456029 :)\n",
            "The loss function for the iteration 4050----->43.24934286840156 :)\n",
            "The loss function for the iteration 4060----->43.249290616900716 :)\n",
            "The loss function for the iteration 4070----->43.24923859874025 :)\n",
            "The loss function for the iteration 4080----->43.24918681261511 :)\n",
            "The loss function for the iteration 4090----->43.24913525723252 :)\n",
            "The loss function for the iteration 4100----->43.24908393131172 :)\n",
            "The loss function for the iteration 4110----->43.249032833583875 :)\n",
            "The loss function for the iteration 4120----->43.24898196279182 :)\n",
            "The loss function for the iteration 4130----->43.248931317689895 :)\n",
            "The loss function for the iteration 4140----->43.24888089704377 :)\n",
            "The loss function for the iteration 4150----->43.24883069963028 :)\n",
            "The loss function for the iteration 4160----->43.24878072423723 :)\n",
            "The loss function for the iteration 4170----->43.24873096966325 :)\n",
            "The loss function for the iteration 4180----->43.24868143471761 :)\n",
            "The loss function for the iteration 4190----->43.24863211822009 :)\n",
            "The loss function for the iteration 4200----->43.24858301900075 :)\n",
            "The loss function for the iteration 4210----->43.2485341358999 :)\n",
            "The loss function for the iteration 4220----->43.24848546776781 :)\n",
            "The loss function for the iteration 4230----->43.24843701346463 :)\n",
            "The loss function for the iteration 4240----->43.24838877186028 :)\n",
            "The loss function for the iteration 4250----->43.24834074183421 :)\n",
            "The loss function for the iteration 4260----->43.24829292227532 :)\n",
            "The loss function for the iteration 4270----->43.24824531208185 :)\n",
            "The loss function for the iteration 4280----->43.248197910161146 :)\n",
            "The loss function for the iteration 4290----->43.24815071542966 :)\n",
            "The loss function for the iteration 4300----->43.24810372681267 :)\n",
            "The loss function for the iteration 4310----->43.24805694324431 :)\n",
            "The loss function for the iteration 4320----->43.24801036366729 :)\n",
            "The loss function for the iteration 4330----->43.24796398703288 :)\n",
            "The loss function for the iteration 4340----->43.24791781230076 :)\n",
            "The loss function for the iteration 4350----->43.24787183843889 :)\n",
            "The loss function for the iteration 4360----->43.2478260644234 :)\n",
            "The loss function for the iteration 4370----->43.24778048923847 :)\n",
            "The loss function for the iteration 4380----->43.24773511187623 :)\n",
            "The loss function for the iteration 4390----->43.24768993133665 :)\n",
            "The loss function for the iteration 4400----->43.24764494662742 :)\n",
            "The loss function for the iteration 4410----->43.24760015676385 :)\n",
            "The loss function for the iteration 4420----->43.24755556076876 :)\n",
            "The loss function for the iteration 4430----->43.2475111576724 :)\n",
            "The loss function for the iteration 4440----->43.24746694651235 :)\n",
            "The loss function for the iteration 4450----->43.24742292633335 :)\n",
            "The loss function for the iteration 4460----->43.247379096187316 :)\n",
            "The loss function for the iteration 4470----->43.24733545513317 :)\n",
            "The loss function for the iteration 4480----->43.24729200223676 :)\n",
            "The loss function for the iteration 4490----->43.2472487365708 :)\n",
            "The loss function for the iteration 4500----->43.24720565721473 :)\n",
            "The loss function for the iteration 4510----->43.24716276325466 :)\n",
            "The loss function for the iteration 4520----->43.2471200537833 :)\n",
            "The loss function for the iteration 4530----->43.24707752789983 :)\n",
            "The loss function for the iteration 4540----->43.24703518470987 :)\n",
            "The loss function for the iteration 4550----->43.24699302332534 :)\n",
            "The loss function for the iteration 4560----->43.24695104286444 :)\n",
            "The loss function for the iteration 4570----->43.246909242451494 :)\n",
            "The loss function for the iteration 4580----->43.24686762121697 :)\n",
            "The loss function for the iteration 4590----->43.246826178297354 :)\n",
            "The loss function for the iteration 4600----->43.246784912835054 :)\n",
            "The loss function for the iteration 4610----->43.24674382397834 :)\n",
            "The loss function for the iteration 4620----->43.246702910881325 :)\n",
            "The loss function for the iteration 4630----->43.24666217270381 :)\n",
            "The loss function for the iteration 4640----->43.246621608611264 :)\n",
            "The loss function for the iteration 4650----->43.246581217774775 :)\n",
            "The loss function for the iteration 4660----->43.246540999370936 :)\n",
            "The loss function for the iteration 4670----->43.2465009525818 :)\n",
            "The loss function for the iteration 4680----->43.24646107659483 :)\n",
            "The loss function for the iteration 4690----->43.24642137060279 :)\n",
            "The loss function for the iteration 4700----->43.24638183380377 :)\n",
            "The loss function for the iteration 4710----->43.246342465401014 :)\n",
            "The loss function for the iteration 4720----->43.24630326460295 :)\n",
            "The loss function for the iteration 4730----->43.24626423062309 :)\n",
            "The loss function for the iteration 4740----->43.246225362679965 :)\n",
            "The loss function for the iteration 4750----->43.24618665999711 :)\n",
            "The loss function for the iteration 4760----->43.24614812180296 :)\n",
            "The loss function for the iteration 4770----->43.24610974733081 :)\n",
            "The loss function for the iteration 4780----->43.246071535818785 :)\n",
            "The loss function for the iteration 4790----->43.24603348650976 :)\n",
            "The loss function for the iteration 4800----->43.24599559865133 :)\n",
            "The loss function for the iteration 4810----->43.24595787149571 :)\n",
            "The loss function for the iteration 4820----->43.245920304299766 :)\n",
            "The loss function for the iteration 4830----->43.245882896324886 :)\n",
            "The loss function for the iteration 4840----->43.245845646836976 :)\n",
            "The loss function for the iteration 4850----->43.24580855510639 :)\n",
            "The loss function for the iteration 4860----->43.245771620407936 :)\n",
            "The loss function for the iteration 4870----->43.24573484202072 :)\n",
            "The loss function for the iteration 4880----->43.24569821922823 :)\n",
            "The loss function for the iteration 4890----->43.245661751318195 :)\n",
            "The loss function for the iteration 4900----->43.2456254375826 :)\n",
            "The loss function for the iteration 4910----->43.24558927731757 :)\n",
            "The loss function for the iteration 4920----->43.24555326982344 :)\n",
            "The loss function for the iteration 4930----->43.24551741440461 :)\n",
            "The loss function for the iteration 4940----->43.245481710369546 :)\n",
            "The loss function for the iteration 4950----->43.24544615703074 :)\n",
            "The loss function for the iteration 4960----->43.24541075370468 :)\n",
            "The loss function for the iteration 4970----->43.245375499711784 :)\n",
            "The loss function for the iteration 4980----->43.24534039437635 :)\n",
            "The loss function for the iteration 4990----->43.24530543702661 :)\n",
            "The loss function for the iteration 5000----->43.24527062699455 :)\n",
            "The loss function for the iteration 5010----->43.24523596361599 :)\n",
            "The loss function for the iteration 5020----->43.245201446230524 :)\n",
            "The loss function for the iteration 5030----->43.24516707418142 :)\n",
            "The loss function for the iteration 5040----->43.24513284681565 :)\n",
            "The loss function for the iteration 5050----->43.24509876348385 :)\n",
            "The loss function for the iteration 5060----->43.24506482354028 :)\n",
            "The loss function for the iteration 5070----->43.245031026342765 :)\n",
            "The loss function for the iteration 5080----->43.24499737125268 :)\n",
            "The loss function for the iteration 5090----->43.24496385763492 :)\n",
            "The loss function for the iteration 5100----->43.24493048485789 :)\n",
            "The loss function for the iteration 5110----->43.24489725229342 :)\n",
            "The loss function for the iteration 5120----->43.2448641593168 :)\n",
            "The loss function for the iteration 5130----->43.24483120530667 :)\n",
            "The loss function for the iteration 5140----->43.24479838964507 :)\n",
            "The loss function for the iteration 5150----->43.244765711717356 :)\n",
            "The loss function for the iteration 5160----->43.244733170912205 :)\n",
            "The loss function for the iteration 5170----->43.24470076662158 :)\n",
            "The loss function for the iteration 5180----->43.24466849824065 :)\n",
            "The loss function for the iteration 5190----->43.24463636516784 :)\n",
            "The loss function for the iteration 5200----->43.244604366804765 :)\n",
            "The loss function for the iteration 5210----->43.24457250255621 :)\n",
            "The loss function for the iteration 5220----->43.24454077183008 :)\n",
            "The loss function for the iteration 5230----->43.244509174037425 :)\n",
            "The loss function for the iteration 5240----->43.24447770859234 :)\n",
            "The loss function for the iteration 5250----->43.244446374912044 :)\n",
            "The loss function for the iteration 5260----->43.24441517241673 :)\n",
            "The loss function for the iteration 5270----->43.24438410052966 :)\n",
            "The loss function for the iteration 5280----->43.24435315867703 :)\n",
            "The loss function for the iteration 5290----->43.24432234628806 :)\n",
            "The loss function for the iteration 5300----->43.24429166279486 :)\n",
            "The loss function for the iteration 5310----->43.24426110763248 :)\n",
            "The loss function for the iteration 5320----->43.24423068023886 :)\n",
            "The loss function for the iteration 5330----->43.24420038005482 :)\n",
            "The loss function for the iteration 5340----->43.24417020652404 :)\n",
            "The loss function for the iteration 5350----->43.244140159092986 :)\n",
            "The loss function for the iteration 5360----->43.244110237210975 :)\n",
            "The loss function for the iteration 5370----->43.24408044033008 :)\n",
            "The loss function for the iteration 5380----->43.24405076790514 :)\n",
            "The loss function for the iteration 5390----->43.24402121939375 :)\n",
            "The loss function for the iteration 5400----->43.243991794256225 :)\n",
            "The loss function for the iteration 5410----->43.243962491955564 :)\n",
            "The loss function for the iteration 5420----->43.24393331195746 :)\n",
            "The loss function for the iteration 5430----->43.24390425373026 :)\n",
            "The loss function for the iteration 5440----->43.24387531674498 :)\n",
            "The loss function for the iteration 5450----->43.243846500475215 :)\n",
            "The loss function for the iteration 5460----->43.24381780439719 :)\n",
            "The loss function for the iteration 5470----->43.2437892279897 :)\n",
            "The loss function for the iteration 5480----->43.24376077073413 :)\n",
            "The loss function for the iteration 5490----->43.24373243211439 :)\n",
            "The loss function for the iteration 5500----->43.24370421161693 :)\n",
            "The loss function for the iteration 5510----->43.2436761087307 :)\n",
            "The loss function for the iteration 5520----->43.24364812294716 :)\n",
            "The loss function for the iteration 5530----->43.24362025376024 :)\n",
            "The loss function for the iteration 5540----->43.24359250066632 :)\n",
            "The loss function for the iteration 5550----->43.243564863164224 :)\n",
            "The loss function for the iteration 5560----->43.24353734075522 :)\n",
            "The loss function for the iteration 5570----->43.24350993294297 :)\n",
            "The loss function for the iteration 5580----->43.24348263923354 :)\n",
            "The loss function for the iteration 5590----->43.24345545913535 :)\n",
            "The loss function for the iteration 5600----->43.24342839215919 :)\n",
            "The loss function for the iteration 5610----->43.243401437818214 :)\n",
            "The loss function for the iteration 5620----->43.24337459562788 :)\n",
            "The loss function for the iteration 5630----->43.243347865105946 :)\n",
            "The loss function for the iteration 5640----->43.24332124577253 :)\n",
            "The loss function for the iteration 5650----->43.243294737149974 :)\n",
            "The loss function for the iteration 5660----->43.243268338762896 :)\n",
            "The loss function for the iteration 5670----->43.243242050138214 :)\n",
            "The loss function for the iteration 5680----->43.24321587080501 :)\n",
            "The loss function for the iteration 5690----->43.24318980029465 :)\n",
            "The loss function for the iteration 5700----->43.24316383814068 :)\n",
            "The loss function for the iteration 5710----->43.24313798387887 :)\n",
            "The loss function for the iteration 5720----->43.243112237047136 :)\n",
            "The loss function for the iteration 5730----->43.243086597185595 :)\n",
            "The loss function for the iteration 5740----->43.243061063836485 :)\n",
            "The loss function for the iteration 5750----->43.24303563654424 :)\n",
            "The loss function for the iteration 5760----->43.24301031485536 :)\n",
            "The loss function for the iteration 5770----->43.2429850983185 :)\n",
            "The loss function for the iteration 5780----->43.2429599864844 :)\n",
            "The loss function for the iteration 5790----->43.24293497890587 :)\n",
            "The loss function for the iteration 5800----->43.24291007513786 :)\n",
            "The loss function for the iteration 5810----->43.24288527473732 :)\n",
            "The loss function for the iteration 5820----->43.24286057726327 :)\n",
            "The loss function for the iteration 5830----->43.24283598227679 :)\n",
            "The loss function for the iteration 5840----->43.242811489340944 :)\n",
            "The loss function for the iteration 5850----->43.24278709802085 :)\n",
            "The loss function for the iteration 5860----->43.242762807883636 :)\n",
            "The loss function for the iteration 5870----->43.24273861849834 :)\n",
            "The loss function for the iteration 5880----->43.24271452943609 :)\n",
            "The loss function for the iteration 5890----->43.24269054026991 :)\n",
            "The loss function for the iteration 5900----->43.24266665057479 :)\n",
            "The loss function for the iteration 5910----->43.242642859927685 :)\n",
            "The loss function for the iteration 5920----->43.24261916790745 :)\n",
            "The loss function for the iteration 5930----->43.2425955740949 :)\n",
            "The loss function for the iteration 5940----->43.24257207807273 :)\n",
            "The loss function for the iteration 5950----->43.24254867942555 :)\n",
            "The loss function for the iteration 5960----->43.242525377739874 :)\n",
            "The loss function for the iteration 5970----->43.242502172604034 :)\n",
            "The loss function for the iteration 5980----->43.24247906360831 :)\n",
            "The loss function for the iteration 5990----->43.24245605034477 :)\n",
            "The loss function for the iteration 6000----->43.242433132407385 :)\n",
            "The loss function for the iteration 6010----->43.24241030939193 :)\n",
            "The loss function for the iteration 6020----->43.24238758089597 :)\n",
            "The loss function for the iteration 6030----->43.24236494651898 :)\n",
            "The loss function for the iteration 6040----->43.24234240586217 :)\n",
            "The loss function for the iteration 6050----->43.242319958528554 :)\n",
            "The loss function for the iteration 6060----->43.24229760412295 :)\n",
            "The loss function for the iteration 6070----->43.24227534225194 :)\n",
            "The loss function for the iteration 6080----->43.24225317252388 :)\n",
            "The loss function for the iteration 6090----->43.242231094548856 :)\n",
            "The loss function for the iteration 6100----->43.24220910793875 :)\n",
            "The loss function for the iteration 6110----->43.24218721230712 :)\n",
            "The loss function for the iteration 6120----->43.24216540726934 :)\n",
            "The loss function for the iteration 6130----->43.24214369244239 :)\n",
            "The loss function for the iteration 6140----->43.242122067445045 :)\n",
            "The loss function for the iteration 6150----->43.24210053189776 :)\n",
            "The loss function for the iteration 6160----->43.242079085422674 :)\n",
            "The loss function for the iteration 6170----->43.2420577276436 :)\n",
            "The loss function for the iteration 6180----->43.24203645818603 :)\n",
            "The loss function for the iteration 6190----->43.24201527667714 :)\n",
            "The loss function for the iteration 6200----->43.24199418274573 :)\n",
            "The loss function for the iteration 6210----->43.24197317602229 :)\n",
            "The loss function for the iteration 6220----->43.24195225613889 :)\n",
            "The loss function for the iteration 6230----->43.24193142272929 :)\n",
            "The loss function for the iteration 6240----->43.241910675428834 :)\n",
            "The loss function for the iteration 6250----->43.24189001387449 :)\n",
            "The loss function for the iteration 6260----->43.24186943770482 :)\n",
            "The loss function for the iteration 6270----->43.24184894656001 :)\n",
            "The loss function for the iteration 6280----->43.241828540081826 :)\n",
            "The loss function for the iteration 6290----->43.24180821791357 :)\n",
            "The loss function for the iteration 6300----->43.24178797970018 :)\n",
            "The loss function for the iteration 6310----->43.24176782508812 :)\n",
            "The loss function for the iteration 6320----->43.24174775372541 :)\n",
            "The loss function for the iteration 6330----->43.24172776526166 :)\n",
            "The loss function for the iteration 6340----->43.241707859347954 :)\n",
            "The loss function for the iteration 6350----->43.24168803563694 :)\n",
            "The loss function for the iteration 6360----->43.241668293782816 :)\n",
            "The loss function for the iteration 6370----->43.241648633441265 :)\n",
            "The loss function for the iteration 6380----->43.24162905426951 :)\n",
            "The loss function for the iteration 6390----->43.24160955592621 :)\n",
            "The loss function for the iteration 6400----->43.24159013807159 :)\n",
            "The loss function for the iteration 6410----->43.24157080036735 :)\n",
            "The loss function for the iteration 6420----->43.241551542476614 :)\n",
            "The loss function for the iteration 6430----->43.24153236406406 :)\n",
            "The loss function for the iteration 6440----->43.241513264795756 :)\n",
            "The loss function for the iteration 6450----->43.24149424433929 :)\n",
            "The loss function for the iteration 6460----->43.241475302363654 :)\n",
            "The loss function for the iteration 6470----->43.241456438539316 :)\n",
            "The loss function for the iteration 6480----->43.241437652538146 :)\n",
            "The loss function for the iteration 6490----->43.241418944033484 :)\n",
            "The loss function for the iteration 6500----->43.24140031270005 :)\n",
            "The loss function for the iteration 6510----->43.24138175821403 :)\n",
            "The loss function for the iteration 6520----->43.24136328025296 :)\n",
            "The loss function for the iteration 6530----->43.24134487849584 :)\n",
            "The loss function for the iteration 6540----->43.241326552622986 :)\n",
            "The loss function for the iteration 6550----->43.24130830231618 :)\n",
            "The loss function for the iteration 6560----->43.24129012725855 :)\n",
            "The loss function for the iteration 6570----->43.24127202713459 :)\n",
            "The loss function for the iteration 6580----->43.24125400163021 :)\n",
            "The loss function for the iteration 6590----->43.241236050432576 :)\n",
            "The loss function for the iteration 6600----->43.241218173230344 :)\n",
            "The loss function for the iteration 6610----->43.241200369713425 :)\n",
            "The loss function for the iteration 6620----->43.241182639573104 :)\n",
            "The loss function for the iteration 6630----->43.241164982501985 :)\n",
            "The loss function for the iteration 6640----->43.24114739819404 :)\n",
            "The loss function for the iteration 6650----->43.24112988634452 :)\n",
            "The loss function for the iteration 6660----->43.241112446650014 :)\n",
            "The loss function for the iteration 6670----->43.24109507880842 :)\n",
            "The loss function for the iteration 6680----->43.24107778251894 :)\n",
            "The loss function for the iteration 6690----->43.24106055748206 :)\n",
            "The loss function for the iteration 6700----->43.24104340339959 :)\n",
            "The loss function for the iteration 6710----->43.24102631997457 :)\n",
            "The loss function for the iteration 6720----->43.24100930691141 :)\n",
            "The loss function for the iteration 6730----->43.240992363915694 :)\n",
            "The loss function for the iteration 6740----->43.240975490694346 :)\n",
            "The loss function for the iteration 6750----->43.240958686955516 :)\n",
            "The loss function for the iteration 6760----->43.240941952408605 :)\n",
            "The loss function for the iteration 6770----->43.2409252867643 :)\n",
            "The loss function for the iteration 6780----->43.2409086897345 :)\n",
            "The loss function for the iteration 6790----->43.24089216103237 :)\n",
            "The loss function for the iteration 6800----->43.240875700372264 :)\n",
            "The loss function for the iteration 6810----->43.2408593074698 :)\n",
            "The loss function for the iteration 6820----->43.24084298204182 :)\n",
            "The loss function for the iteration 6830----->43.24082672380636 :)\n",
            "The loss function for the iteration 6840----->43.240810532482676 :)\n",
            "The loss function for the iteration 6850----->43.24079440779122 :)\n",
            "The loss function for the iteration 6860----->43.240778349453656 :)\n",
            "The loss function for the iteration 6870----->43.24076235719285 :)\n",
            "The loss function for the iteration 6880----->43.24074643073283 :)\n",
            "The loss function for the iteration 6890----->43.2407305697988 :)\n",
            "The loss function for the iteration 6900----->43.240714774117194 :)\n",
            "The loss function for the iteration 6910----->43.24069904341555 :)\n",
            "The loss function for the iteration 6920----->43.24068337742264 :)\n",
            "The loss function for the iteration 6930----->43.24066777586833 :)\n",
            "The loss function for the iteration 6940----->43.24065223848369 :)\n",
            "The loss function for the iteration 6950----->43.24063676500093 :)\n",
            "The loss function for the iteration 6960----->43.24062135515337 :)\n",
            "The loss function for the iteration 6970----->43.24060600867554 :)\n",
            "The loss function for the iteration 6980----->43.240590725303036 :)\n",
            "The loss function for the iteration 6990----->43.240575504772615 :)\n",
            "The loss function for the iteration 7000----->43.24056034682216 :)\n",
            "The loss function for the iteration 7010----->43.24054525119066 :)\n",
            "The loss function for the iteration 7020----->43.24053021761823 :)\n",
            "The loss function for the iteration 7030----->43.24051524584609 :)\n",
            "The loss function for the iteration 7040----->43.240500335616574 :)\n",
            "The loss function for the iteration 7050----->43.24048548667309 :)\n",
            "The loss function for the iteration 7060----->43.240470698760156 :)\n",
            "The loss function for the iteration 7070----->43.240455971623405 :)\n",
            "The loss function for the iteration 7080----->43.24044130500949 :)\n",
            "The loss function for the iteration 7090----->43.24042669866621 :)\n",
            "The loss function for the iteration 7100----->43.240412152342394 :)\n",
            "The loss function for the iteration 7110----->43.24039766578797 :)\n",
            "The loss function for the iteration 7120----->43.2403832387539 :)\n",
            "The loss function for the iteration 7130----->43.24036887099225 :)\n",
            "The loss function for the iteration 7140----->43.24035456225608 :)\n",
            "The loss function for the iteration 7150----->43.24034031229958 :)\n",
            "The loss function for the iteration 7160----->43.24032612087791 :)\n",
            "The loss function for the iteration 7170----->43.240311987747305 :)\n",
            "The loss function for the iteration 7180----->43.240297912665056 :)\n",
            "The loss function for the iteration 7190----->43.240283895389446 :)\n",
            "The loss function for the iteration 7200----->43.24026993567981 :)\n",
            "The loss function for the iteration 7210----->43.24025603329651 :)\n",
            "The loss function for the iteration 7220----->43.2402421880009 :)\n",
            "The loss function for the iteration 7230----->43.24022839955538 :)\n",
            "The loss function for the iteration 7240----->43.240214667723336 :)\n",
            "The loss function for the iteration 7250----->43.24020099226918 :)\n",
            "The loss function for the iteration 7260----->43.24018737295829 :)\n",
            "The loss function for the iteration 7270----->43.240173809557064 :)\n",
            "The loss function for the iteration 7280----->43.24016030183291 :)\n",
            "The loss function for the iteration 7290----->43.24014684955418 :)\n",
            "The loss function for the iteration 7300----->43.24013345249023 :)\n",
            "The loss function for the iteration 7310----->43.24012011041141 :)\n",
            "The loss function for the iteration 7320----->43.240106823089015 :)\n",
            "The loss function for the iteration 7330----->43.240093590295324 :)\n",
            "The loss function for the iteration 7340----->43.24008041180358 :)\n",
            "The loss function for the iteration 7350----->43.24006728738801 :)\n",
            "The loss function for the iteration 7360----->43.24005421682373 :)\n",
            "The loss function for the iteration 7370----->43.240041199886896 :)\n",
            "The loss function for the iteration 7380----->43.240028236354554 :)\n",
            "The loss function for the iteration 7390----->43.24001532600471 :)\n",
            "The loss function for the iteration 7400----->43.24000246861631 :)\n",
            "The loss function for the iteration 7410----->43.239989663969254 :)\n",
            "The loss function for the iteration 7420----->43.23997691184433 :)\n",
            "The loss function for the iteration 7430----->43.239964212023295 :)\n",
            "The loss function for the iteration 7440----->43.23995156428881 :)\n",
            "The loss function for the iteration 7450----->43.23993896842446 :)\n",
            "The loss function for the iteration 7460----->43.23992642421477 :)\n",
            "The loss function for the iteration 7470----->43.239913931445116 :)\n",
            "The loss function for the iteration 7480----->43.23990148990185 :)\n",
            "The loss function for the iteration 7490----->43.23988909937218 :)\n",
            "The loss function for the iteration 7500----->43.23987675964422 :)\n",
            "The loss function for the iteration 7510----->43.23986447050701 :)\n",
            "The loss function for the iteration 7520----->43.23985223175046 :)\n",
            "The loss function for the iteration 7530----->43.23984004316535 :)\n",
            "The loss function for the iteration 7540----->43.239827904543375 :)\n",
            "The loss function for the iteration 7550----->43.23981581567711 :)\n",
            "The loss function for the iteration 7560----->43.23980377635999 :)\n",
            "The loss function for the iteration 7570----->43.23979178638632 :)\n",
            "The loss function for the iteration 7580----->43.23977984555128 :)\n",
            "The loss function for the iteration 7590----->43.23976795365091 :)\n",
            "The loss function for the iteration 7600----->43.23975611048213 :)\n",
            "The loss function for the iteration 7610----->43.23974431584271 :)\n",
            "The loss function for the iteration 7620----->43.239732569531235 :)\n",
            "The loss function for the iteration 7630----->43.23972087134721 :)\n",
            "The loss function for the iteration 7640----->43.23970922109094 :)\n",
            "The loss function for the iteration 7650----->43.23969761856357 :)\n",
            "The loss function for the iteration 7660----->43.23968606356709 :)\n",
            "The loss function for the iteration 7670----->43.239674555904365 :)\n",
            "The loss function for the iteration 7680----->43.23966309537901 :)\n",
            "The loss function for the iteration 7690----->43.23965168179555 :)\n",
            "The loss function for the iteration 7700----->43.23964031495928 :)\n",
            "The loss function for the iteration 7710----->43.239628994676345 :)\n",
            "The loss function for the iteration 7720----->43.23961772075371 :)\n",
            "The loss function for the iteration 7730----->43.239606492999116 :)\n",
            "The loss function for the iteration 7740----->43.239595311221144 :)\n",
            "The loss function for the iteration 7750----->43.23958417522919 :)\n",
            "The loss function for the iteration 7760----->43.23957308483343 :)\n",
            "The loss function for the iteration 7770----->43.23956203984485 :)\n",
            "The loss function for the iteration 7780----->43.239551040075234 :)\n",
            "The loss function for the iteration 7790----->43.23954008533717 :)\n",
            "The loss function for the iteration 7800----->43.239529175444 :)\n",
            "The loss function for the iteration 7810----->43.239518310209874 :)\n",
            "The loss function for the iteration 7820----->43.23950748944974 :)\n",
            "The loss function for the iteration 7830----->43.2394967129793 :)\n",
            "The loss function for the iteration 7840----->43.23948598061504 :)\n",
            "The loss function for the iteration 7850----->43.239475292174234 :)\n",
            "The loss function for the iteration 7860----->43.2394646474749 :)\n",
            "The loss function for the iteration 7870----->43.23945404633582 :)\n",
            "The loss function for the iteration 7880----->43.23944348857656 :)\n",
            "The loss function for the iteration 7890----->43.23943297401746 :)\n",
            "The loss function for the iteration 7900----->43.23942250247955 :)\n",
            "The loss function for the iteration 7910----->43.239412073784685 :)\n",
            "The loss function for the iteration 7920----->43.23940168775543 :)\n",
            "The loss function for the iteration 7930----->43.23939134421511 :)\n",
            "The loss function for the iteration 7940----->43.239381042987766 :)\n",
            "The loss function for the iteration 7950----->43.23937078389822 :)\n",
            "The loss function for the iteration 7960----->43.23936056677201 :)\n",
            "The loss function for the iteration 7970----->43.2393503914354 :)\n",
            "The loss function for the iteration 7980----->43.23934025771541 :)\n",
            "The loss function for the iteration 7990----->43.23933016543975 :)\n",
            "The loss function for the iteration 8000----->43.239320114436886 :)\n",
            "The loss function for the iteration 8010----->43.23931010453599 :)\n",
            "The loss function for the iteration 8020----->43.23930013556695 :)\n",
            "The loss function for the iteration 8030----->43.239290207360376 :)\n",
            "The loss function for the iteration 8040----->43.23928031974758 :)\n",
            "The loss function for the iteration 8050----->43.239270472560605 :)\n",
            "The loss function for the iteration 8060----->43.23926066563217 :)\n",
            "The loss function for the iteration 8070----->43.23925089879572 :)\n",
            "The loss function for the iteration 8080----->43.239241171885375 :)\n",
            "The loss function for the iteration 8090----->43.239231484735974 :)\n",
            "The loss function for the iteration 8100----->43.239221837183045 :)\n",
            "The loss function for the iteration 8110----->43.239212229062794 :)\n",
            "The loss function for the iteration 8120----->43.23920266021212 :)\n",
            "The loss function for the iteration 8130----->43.23919313046863 :)\n",
            "The loss function for the iteration 8140----->43.239183639670564 :)\n",
            "The loss function for the iteration 8150----->43.239174187656886 :)\n",
            "The loss function for the iteration 8160----->43.23916477426722 :)\n",
            "The loss function for the iteration 8170----->43.239155399341854 :)\n",
            "The loss function for the iteration 8180----->43.23914606272175 :)\n",
            "The loss function for the iteration 8190----->43.239136764248556 :)\n",
            "The loss function for the iteration 8200----->43.23912750376455 :)\n",
            "The loss function for the iteration 8210----->43.23911828111271 :)\n",
            "The loss function for the iteration 8220----->43.23910909613664 :)\n",
            "The loss function for the iteration 8230----->43.23909994868062 :)\n",
            "The loss function for the iteration 8240----->43.239090838589554 :)\n",
            "The loss function for the iteration 8250----->43.239081765709045 :)\n",
            "The loss function for the iteration 8260----->43.23907272988531 :)\n",
            "The loss function for the iteration 8270----->43.23906373096519 :)\n",
            "The loss function for the iteration 8280----->43.23905476879623 :)\n",
            "The loss function for the iteration 8290----->43.23904584322656 :)\n",
            "The loss function for the iteration 8300----->43.23903695410497 :)\n",
            "The loss function for the iteration 8310----->43.239028101280894 :)\n",
            "The loss function for the iteration 8320----->43.23901928460433 :)\n",
            "The loss function for the iteration 8330----->43.23901050392602 :)\n",
            "The loss function for the iteration 8340----->43.23900175909723 :)\n",
            "The loss function for the iteration 8350----->43.23899304996988 :)\n",
            "The loss function for the iteration 8360----->43.238984376396544 :)\n",
            "The loss function for the iteration 8370----->43.23897573823037 :)\n",
            "The loss function for the iteration 8380----->43.23896713532515 :)\n",
            "The loss function for the iteration 8390----->43.23895856753524 :)\n",
            "The loss function for the iteration 8400----->43.23895003471569 :)\n",
            "The loss function for the iteration 8410----->43.238941536722066 :)\n",
            "The loss function for the iteration 8420----->43.23893307341062 :)\n",
            "The loss function for the iteration 8430----->43.23892464463812 :)\n",
            "The loss function for the iteration 8440----->43.23891625026203 :)\n",
            "The loss function for the iteration 8450----->43.23890789014032 :)\n",
            "The loss function for the iteration 8460----->43.238899564131614 :)\n",
            "The loss function for the iteration 8470----->43.2388912720951 :)\n",
            "The loss function for the iteration 8480----->43.23888301389057 :)\n",
            "The loss function for the iteration 8490----->43.2388747893784 :)\n",
            "The loss function for the iteration 8500----->43.23886659841954 :)\n",
            "The loss function for the iteration 8510----->43.23885844087553 :)\n",
            "The loss function for the iteration 8520----->43.23885031660849 :)\n",
            "The loss function for the iteration 8530----->43.238842225481115 :)\n",
            "The loss function for the iteration 8540----->43.23883416735667 :)\n",
            "The loss function for the iteration 8550----->43.23882614209901 :)\n",
            "The loss function for the iteration 8560----->43.238818149572516 :)\n",
            "The loss function for the iteration 8570----->43.23881018964219 :)\n",
            "The loss function for the iteration 8580----->43.238802262173564 :)\n",
            "The loss function for the iteration 8590----->43.238794367032746 :)\n",
            "The loss function for the iteration 8600----->43.23878650408639 :)\n",
            "The loss function for the iteration 8610----->43.238778673201736 :)\n",
            "The loss function for the iteration 8620----->43.23877087424655 :)\n",
            "The loss function for the iteration 8630----->43.238763107089156 :)\n",
            "The loss function for the iteration 8640----->43.23875537159843 :)\n",
            "The loss function for the iteration 8650----->43.23874766764382 :)\n",
            "The loss function for the iteration 8660----->43.238739995095266 :)\n",
            "The loss function for the iteration 8670----->43.23873235382332 :)\n",
            "The loss function for the iteration 8680----->43.23872474369901 :)\n",
            "The loss function for the iteration 8690----->43.23871716459396 :)\n",
            "The loss function for the iteration 8700----->43.238709616380284 :)\n",
            "The loss function for the iteration 8710----->43.238702098930645 :)\n",
            "The loss function for the iteration 8720----->43.23869461211825 :)\n",
            "The loss function for the iteration 8730----->43.23868715581683 :)\n",
            "The loss function for the iteration 8740----->43.238679729900625 :)\n",
            "The loss function for the iteration 8750----->43.23867233424444 :)\n",
            "The loss function for the iteration 8760----->43.238664968723555 :)\n",
            "The loss function for the iteration 8770----->43.238657633213805 :)\n",
            "The loss function for the iteration 8780----->43.238650327591536 :)\n",
            "The loss function for the iteration 8790----->43.2386430517336 :)\n",
            "The loss function for the iteration 8800----->43.238635805517376 :)\n",
            "The loss function for the iteration 8810----->43.238628588820745 :)\n",
            "The loss function for the iteration 8820----->43.23862140152209 :)\n",
            "The loss function for the iteration 8830----->43.238614243500336 :)\n",
            "The loss function for the iteration 8840----->43.23860711463488 :)\n",
            "The loss function for the iteration 8850----->43.238600014805634 :)\n",
            "The loss function for the iteration 8860----->43.23859294389302 :)\n",
            "The loss function for the iteration 8870----->43.23858590177793 :)\n",
            "The loss function for the iteration 8880----->43.23857888834178 :)\n",
            "The loss function for the iteration 8890----->43.23857190346648 :)\n",
            "The loss function for the iteration 8900----->43.23856494703441 :)\n",
            "The loss function for the iteration 8910----->43.23855801892848 :)\n",
            "The loss function for the iteration 8920----->43.23855111903203 :)\n",
            "The loss function for the iteration 8930----->43.23854424722896 :)\n",
            "The loss function for the iteration 8940----->43.23853740340358 :)\n",
            "The loss function for the iteration 8950----->43.23853058744075 :)\n",
            "The loss function for the iteration 8960----->43.23852379922576 :)\n",
            "The loss function for the iteration 8970----->43.23851703864439 :)\n",
            "The loss function for the iteration 8980----->43.23851030558293 :)\n",
            "The loss function for the iteration 8990----->43.23850359992809 :)\n",
            "The loss function for the iteration 9000----->43.238496921567084 :)\n",
            "The loss function for the iteration 9010----->43.23849027038759 :)\n",
            "The loss function for the iteration 9020----->43.23848364627777 :)\n",
            "The loss function for the iteration 9030----->43.23847704912623 :)\n",
            "The loss function for the iteration 9040----->43.23847047882204 :)\n",
            "The loss function for the iteration 9050----->43.23846393525476 :)\n",
            "The loss function for the iteration 9060----->43.238457418314376 :)\n",
            "The loss function for the iteration 9070----->43.238450927891336 :)\n",
            "The loss function for the iteration 9080----->43.23844446387657 :)\n",
            "The loss function for the iteration 9090----->43.23843802616145 :)\n",
            "The loss function for the iteration 9100----->43.23843161463779 :)\n",
            "The loss function for the iteration 9110----->43.23842522919787 :)\n",
            "The loss function for the iteration 9120----->43.23841886973442 :)\n",
            "The loss function for the iteration 9130----->43.238412536140586 :)\n",
            "The loss function for the iteration 9140----->43.23840622831 :)\n",
            "The loss function for the iteration 9150----->43.23839994613672 :)\n",
            "The loss function for the iteration 9160----->43.238393689515235 :)\n",
            "The loss function for the iteration 9170----->43.238387458340476 :)\n",
            "The loss function for the iteration 9180----->43.23838125250784 :)\n",
            "The loss function for the iteration 9190----->43.23837507191311 :)\n",
            "The loss function for the iteration 9200----->43.23836891645257 :)\n",
            "The loss function for the iteration 9210----->43.23836278602287 :)\n",
            "The loss function for the iteration 9220----->43.23835668052111 :)\n",
            "The loss function for the iteration 9230----->43.23835059984483 :)\n",
            "The loss function for the iteration 9240----->43.238344543892 :)\n",
            "The loss function for the iteration 9250----->43.23833851256101 :)\n",
            "The loss function for the iteration 9260----->43.23833250575065 :)\n",
            "The loss function for the iteration 9270----->43.238326523360165 :)\n",
            "The loss function for the iteration 9280----->43.23832056528921 :)\n",
            "The loss function for the iteration 9290----->43.23831463143784 :)\n",
            "The loss function for the iteration 9300----->43.23830872170654 :)\n",
            "The loss function for the iteration 9310----->43.23830283599621 :)\n",
            "The loss function for the iteration 9320----->43.238296974208154 :)\n",
            "The loss function for the iteration 9330----->43.2382911362441 :)\n",
            "The loss function for the iteration 9340----->43.238285322006185 :)\n",
            "The loss function for the iteration 9350----->43.238279531396934 :)\n",
            "The loss function for the iteration 9360----->43.238273764319274 :)\n",
            "The loss function for the iteration 9370----->43.23826802067657 :)\n",
            "The loss function for the iteration 9380----->43.23826230037257 :)\n",
            "The loss function for the iteration 9390----->43.23825660331141 :)\n",
            "The loss function for the iteration 9400----->43.23825092939764 :)\n",
            "The loss function for the iteration 9410----->43.23824527853621 :)\n",
            "The loss function for the iteration 9420----->43.23823965063246 :)\n",
            "The loss function for the iteration 9430----->43.23823404559211 :)\n",
            "The loss function for the iteration 9440----->43.23822846332128 :)\n",
            "The loss function for the iteration 9450----->43.238222903726495 :)\n",
            "The loss function for the iteration 9460----->43.23821736671466 :)\n",
            "The loss function for the iteration 9470----->43.238211852193054 :)\n",
            "The loss function for the iteration 9480----->43.238206360069356 :)\n",
            "The loss function for the iteration 9490----->43.23820089025164 :)\n",
            "The loss function for the iteration 9500----->43.238195442648326 :)\n",
            "The loss function for the iteration 9510----->43.238190017168236 :)\n",
            "The loss function for the iteration 9520----->43.238184613720584 :)\n",
            "The loss function for the iteration 9530----->43.23817923221494 :)\n",
            "The loss function for the iteration 9540----->43.238173872561255 :)\n",
            "The loss function for the iteration 9550----->43.23816853466986 :)\n",
            "The loss function for the iteration 9560----->43.23816321845145 :)\n",
            "The loss function for the iteration 9570----->43.23815792381709 :)\n",
            "The loss function for the iteration 9580----->43.23815265067824 :)\n",
            "The loss function for the iteration 9590----->43.238147398946694 :)\n",
            "The loss function for the iteration 9600----->43.23814216853464 :)\n",
            "The loss function for the iteration 9610----->43.23813695935459 :)\n",
            "The loss function for the iteration 9620----->43.23813177131947 :)\n",
            "The loss function for the iteration 9630----->43.23812660434253 :)\n",
            "The loss function for the iteration 9640----->43.23812145833741 :)\n",
            "The loss function for the iteration 9650----->43.23811633321808 :)\n",
            "The loss function for the iteration 9660----->43.23811122889888 :)\n",
            "The loss function for the iteration 9670----->43.238106145294516 :)\n",
            "The loss function for the iteration 9680----->43.23810108232002 :)\n",
            "The loss function for the iteration 9690----->43.23809603989081 :)\n",
            "The loss function for the iteration 9700----->43.23809101792265 :)\n",
            "The loss function for the iteration 9710----->43.2380860163316 :)\n",
            "The loss function for the iteration 9720----->43.238081035034156 :)\n",
            "The loss function for the iteration 9730----->43.23807607394709 :)\n",
            "The loss function for the iteration 9740----->43.23807113298756 :)\n",
            "The loss function for the iteration 9750----->43.23806621207304 :)\n",
            "The loss function for the iteration 9760----->43.23806131112138 :)\n",
            "The loss function for the iteration 9770----->43.23805643005073 :)\n",
            "The loss function for the iteration 9780----->43.23805156877962 :)\n",
            "The loss function for the iteration 9790----->43.23804672722686 :)\n",
            "The loss function for the iteration 9800----->43.23804190531169 :)\n",
            "The loss function for the iteration 9810----->43.23803710295357 :)\n",
            "The loss function for the iteration 9820----->43.23803232007241 :)\n",
            "The loss function for the iteration 9830----->43.238027556588364 :)\n",
            "The loss function for the iteration 9840----->43.23802281242196 :)\n",
            "The loss function for the iteration 9850----->43.238018087494034 :)\n",
            "The loss function for the iteration 9860----->43.23801338172577 :)\n",
            "The loss function for the iteration 9870----->43.23800869503866 :)\n",
            "The loss function for the iteration 9880----->43.238004027354556 :)\n",
            "The loss function for the iteration 9890----->43.23799937859558 :)\n",
            "The loss function for the iteration 9900----->43.237994748684244 :)\n",
            "The loss function for the iteration 9910----->43.23799013754329 :)\n",
            "The loss function for the iteration 9920----->43.23798554509589 :)\n",
            "The loss function for the iteration 9930----->43.237980971265436 :)\n",
            "The loss function for the iteration 9940----->43.2379764159757 :)\n",
            "The loss function for the iteration 9950----->43.23797187915075 :)\n",
            "The loss function for the iteration 9960----->43.23796736071496 :)\n",
            "The loss function for the iteration 9970----->43.237962860593036 :)\n",
            "The loss function for the iteration 9980----->43.23795837870998 :)\n",
            "The loss function for the iteration 9990----->43.23795391499112 :)\n",
            "The loss function for the iteration 10000----->43.23794946936207 :)\n",
            "The loss function for the iteration 10010----->43.23794504174878 :)\n",
            "The loss function for the iteration 10020----->43.23794063207749 :)\n",
            "The loss function for the iteration 10030----->43.237936240274735 :)\n",
            "The loss function for the iteration 10040----->43.23793186626739 :)\n",
            "The loss function for the iteration 10050----->43.23792750998258 :)\n",
            "The loss function for the iteration 10060----->43.237923171347816 :)\n",
            "The loss function for the iteration 10070----->43.2379188502908 :)\n",
            "The loss function for the iteration 10080----->43.237914546739624 :)\n",
            "The loss function for the iteration 10090----->43.237910260622634 :)\n",
            "The loss function for the iteration 10100----->43.23790599186849 :)\n",
            "The loss function for the iteration 10110----->43.23790174040612 :)\n",
            "The loss function for the iteration 10120----->43.23789750616478 :)\n",
            "The loss function for the iteration 10130----->43.237893289074016 :)\n",
            "The loss function for the iteration 10140----->43.237889089063614 :)\n",
            "The loss function for the iteration 10150----->43.23788490606375 :)\n",
            "The loss function for the iteration 10160----->43.237880740004776 :)\n",
            "The loss function for the iteration 10170----->43.237876590817415 :)\n",
            "The loss function for the iteration 10180----->43.23787245843265 :)\n",
            "The loss function for the iteration 10190----->43.23786834278175 :)\n",
            "The loss function for the iteration 10200----->43.23786424379625 :)\n",
            "The loss function for the iteration 10210----->43.23786016140799 :)\n",
            "The loss function for the iteration 10220----->43.237856095549105 :)\n",
            "The loss function for the iteration 10230----->43.23785204615197 :)\n",
            "The loss function for the iteration 10240----->43.23784801314927 :)\n",
            "The loss function for the iteration 10250----->43.23784399647397 :)\n",
            "The loss function for the iteration 10260----->43.237839996059286 :)\n",
            "The loss function for the iteration 10270----->43.237836011838745 :)\n",
            "The loss function for the iteration 10280----->43.23783204374613 :)\n",
            "The loss function for the iteration 10290----->43.237828091715485 :)\n",
            "The loss function for the iteration 10300----->43.23782415568116 :)\n",
            "The loss function for the iteration 10310----->43.23782023557776 :)\n",
            "The loss function for the iteration 10320----->43.237816331340134 :)\n",
            "The loss function for the iteration 10330----->43.23781244290345 :)\n",
            "The loss function for the iteration 10340----->43.2378085702031 :)\n",
            "The loss function for the iteration 10350----->43.237804713174796 :)\n",
            "The loss function for the iteration 10360----->43.23780087175443 :)\n",
            "The loss function for the iteration 10370----->43.23779704587826 :)\n",
            "The loss function for the iteration 10380----->43.237793235482734 :)\n",
            "The loss function for the iteration 10390----->43.2377894405046 :)\n",
            "The loss function for the iteration 10400----->43.23778566088084 :)\n",
            "The loss function for the iteration 10410----->43.237781896548725 :)\n",
            "The loss function for the iteration 10420----->43.237778147445766 :)\n",
            "The loss function for the iteration 10430----->43.23777441350974 :)\n",
            "The loss function for the iteration 10440----->43.23777069467868 :)\n",
            "The loss function for the iteration 10450----->43.237766990890876 :)\n",
            "The loss function for the iteration 10460----->43.237763302084865 :)\n",
            "The loss function for the iteration 10470----->43.23775962819946 :)\n",
            "The loss function for the iteration 10480----->43.237755969173705 :)\n",
            "The loss function for the iteration 10490----->43.23775232494689 :)\n",
            "The loss function for the iteration 10500----->43.23774869545858 :)\n",
            "The loss function for the iteration 10510----->43.23774508064857 :)\n",
            "The loss function for the iteration 10520----->43.237741480456926 :)\n",
            "The loss function for the iteration 10530----->43.23773789482394 :)\n",
            "The loss function for the iteration 10540----->43.237734323690155 :)\n",
            "The loss function for the iteration 10550----->43.237730766996364 :)\n",
            "The loss function for the iteration 10560----->43.237727224683596 :)\n",
            "The loss function for the iteration 10570----->43.23772369669315 :)\n",
            "The loss function for the iteration 10580----->43.23772018296654 :)\n",
            "The loss function for the iteration 10590----->43.23771668344552 :)\n",
            "The loss function for the iteration 10600----->43.2377131980721 :)\n",
            "The loss function for the iteration 10610----->43.23770972678852 :)\n",
            "The loss function for the iteration 10620----->43.237706269537284 :)\n",
            "The loss function for the iteration 10630----->43.23770282626109 :)\n",
            "The loss function for the iteration 10640----->43.2376993969029 :)\n",
            "The loss function for the iteration 10650----->43.23769598140589 :)\n",
            "The loss function for the iteration 10660----->43.23769257971353 :)\n",
            "The loss function for the iteration 10670----->43.23768919176943 :)\n",
            "The loss function for the iteration 10680----->43.237685817517516 :)\n",
            "The loss function for the iteration 10690----->43.23768245690189 :)\n",
            "The loss function for the iteration 10700----->43.23767910986692 :)\n",
            "The loss function for the iteration 10710----->43.237675776357186 :)\n",
            "The loss function for the iteration 10720----->43.23767245631751 :)\n",
            "The loss function for the iteration 10730----->43.23766914969293 :)\n",
            "The loss function for the iteration 10740----->43.23766585642869 :)\n",
            "The loss function for the iteration 10750----->43.237662576470306 :)\n",
            "The loss function for the iteration 10760----->43.237659309763494 :)\n",
            "The loss function for the iteration 10770----->43.23765605625421 :)\n",
            "The loss function for the iteration 10780----->43.23765281588859 :)\n",
            "The loss function for the iteration 10790----->43.23764958861303 :)\n",
            "The loss function for the iteration 10800----->43.23764637437416 :)\n",
            "The loss function for the iteration 10810----->43.23764317311879 :)\n",
            "The loss function for the iteration 10820----->43.23763998479396 :)\n",
            "The loss function for the iteration 10830----->43.23763680934696 :)\n",
            "The loss function for the iteration 10840----->43.23763364672524 :)\n",
            "The loss function for the iteration 10850----->43.23763049687654 :)\n",
            "The loss function for the iteration 10860----->43.23762735974875 :)\n",
            "The loss function for the iteration 10870----->43.23762423529002 :)\n",
            "The loss function for the iteration 10880----->43.23762112344868 :)\n",
            "The loss function for the iteration 10890----->43.23761802417328 :)\n",
            "The loss function for the iteration 10900----->43.237614937412616 :)\n",
            "The loss function for the iteration 10910----->43.237611863115646 :)\n",
            "The loss function for the iteration 10920----->43.23760880123157 :)\n",
            "The loss function for the iteration 10930----->43.2376057517098 :)\n",
            "The loss function for the iteration 10940----->43.23760271449993 :)\n",
            "The loss function for the iteration 10950----->43.23759968955178 :)\n",
            "The loss function for the iteration 10960----->43.23759667681538 :)\n",
            "The loss function for the iteration 10970----->43.23759367624097 :)\n",
            "The loss function for the iteration 10980----->43.23759068777895 :)\n",
            "The loss function for the iteration 10990----->43.237587711379994 :)\n",
            "The loss function for the iteration 11000----->43.237584746994926 :)\n",
            "The loss function for the iteration 11010----->43.237581794574794 :)\n",
            "The loss function for the iteration 11020----->43.237578854070854 :)\n",
            "The loss function for the iteration 11030----->43.23757592543455 :)\n",
            "The loss function for the iteration 11040----->43.237573008617524 :)\n",
            "The loss function for the iteration 11050----->43.237570103571635 :)\n",
            "The loss function for the iteration 11060----->43.23756721024891 :)\n",
            "The loss function for the iteration 11070----->43.23756432860162 :)\n",
            "The loss function for the iteration 11080----->43.23756145858217 :)\n",
            "The loss function for the iteration 11090----->43.23755860014323 :)\n",
            "The loss function for the iteration 11100----->43.23755575323761 :)\n",
            "The loss function for the iteration 11110----->43.237552917818334 :)\n",
            "The loss function for the iteration 11120----->43.23755009383863 :)\n",
            "The loss function for the iteration 11130----->43.23754728125191 :)\n",
            "The loss function for the iteration 11140----->43.23754448001178 :)\n",
            "The loss function for the iteration 11150----->43.23754169007201 :)\n",
            "The loss function for the iteration 11160----->43.23753891138661 :)\n",
            "The loss function for the iteration 11170----->43.237536143909736 :)\n",
            "The loss function for the iteration 11180----->43.23753338759576 :)\n",
            "The loss function for the iteration 11190----->43.23753064239924 :)\n",
            "The loss function for the iteration 11200----->43.237527908274906 :)\n",
            "The loss function for the iteration 11210----->43.23752518517767 :)\n",
            "The loss function for the iteration 11220----->43.237522473062654 :)\n",
            "The loss function for the iteration 11230----->43.23751977188515 :)\n",
            "The loss function for the iteration 11240----->43.23751708160064 :)\n",
            "The loss function for the iteration 11250----->43.23751440216478 :)\n",
            "The loss function for the iteration 11260----->43.23751173353341 :)\n",
            "The loss function for the iteration 11270----->43.23750907566256 :)\n",
            "The loss function for the iteration 11280----->43.23750642850843 :)\n",
            "The loss function for the iteration 11290----->43.23750379202742 :)\n",
            "The loss function for the iteration 11300----->43.23750116617609 :)\n",
            "The loss function for the iteration 11310----->43.23749855091117 :)\n",
            "The loss function for the iteration 11320----->43.23749594618961 :)\n",
            "The loss function for the iteration 11330----->43.237493351968496 :)\n",
            "The loss function for the iteration 11340----->43.23749076820509 :)\n",
            "The loss function for the iteration 11350----->43.237488194856866 :)\n",
            "The loss function for the iteration 11360----->43.23748563188144 :)\n",
            "The loss function for the iteration 11370----->43.23748307923661 :)\n",
            "The loss function for the iteration 11380----->43.23748053688036 :)\n",
            "The loss function for the iteration 11390----->43.237478004770836 :)\n",
            "The loss function for the iteration 11400----->43.237475482866344 :)\n",
            "The loss function for the iteration 11410----->43.23747297112539 :)\n",
            "The loss function for the iteration 11420----->43.237470469506654 :)\n",
            "The loss function for the iteration 11430----->43.23746797796892 :)\n",
            "The loss function for the iteration 11440----->43.237465496471216 :)\n",
            "The loss function for the iteration 11450----->43.237463024972726 :)\n",
            "The loss function for the iteration 11460----->43.237460563432755 :)\n",
            "The loss function for the iteration 11470----->43.23745811181083 :)\n",
            "The loss function for the iteration 11480----->43.23745567006662 :)\n",
            "The loss function for the iteration 11490----->43.237453238159944 :)\n",
            "The loss function for the iteration 11500----->43.23745081605083 :)\n",
            "The loss function for the iteration 11510----->43.237448403699425 :)\n",
            "The loss function for the iteration 11520----->43.23744600106607 :)\n",
            "The loss function for the iteration 11530----->43.23744360811125 :)\n",
            "The loss function for the iteration 11540----->43.23744122479562 :)\n",
            "The loss function for the iteration 11550----->43.23743885108001 :)\n",
            "The loss function for the iteration 11560----->43.237436486925375 :)\n",
            "The loss function for the iteration 11570----->43.23743413229288 :)\n",
            "The loss function for the iteration 11580----->43.237431787143805 :)\n",
            "The loss function for the iteration 11590----->43.2374294514396 :)\n",
            "The loss function for the iteration 11600----->43.2374271251419 :)\n",
            "The loss function for the iteration 11610----->43.237424808212474 :)\n",
            "The loss function for the iteration 11620----->43.23742250061324 :)\n",
            "The loss function for the iteration 11630----->43.23742020230629 :)\n",
            "The loss function for the iteration 11640----->43.23741791325388 :)\n",
            "The loss function for the iteration 11650----->43.23741563341837 :)\n",
            "The loss function for the iteration 11660----->43.23741336276236 :)\n",
            "The loss function for the iteration 11670----->43.23741110124852 :)\n",
            "The loss function for the iteration 11680----->43.23740884883971 :)\n",
            "The loss function for the iteration 11690----->43.23740660549896 :)\n",
            "The loss function for the iteration 11700----->43.23740437118942 :)\n",
            "The loss function for the iteration 11710----->43.237402145874405 :)\n",
            "The loss function for the iteration 11720----->43.23739992951739 :)\n",
            "The loss function for the iteration 11730----->43.23739772208197 :)\n",
            "The loss function for the iteration 11740----->43.23739552353193 :)\n",
            "The loss function for the iteration 11750----->43.23739333383117 :)\n",
            "The loss function for the iteration 11760----->43.23739115294375 :)\n",
            "The loss function for the iteration 11770----->43.23738898083387 :)\n",
            "The loss function for the iteration 11780----->43.23738681746592 :)\n",
            "The loss function for the iteration 11790----->43.23738466280437 :)\n",
            "The loss function for the iteration 11800----->43.23738251681387 :)\n",
            "The loss function for the iteration 11810----->43.2373803794592 :)\n",
            "The loss function for the iteration 11820----->43.23737825070534 :)\n",
            "The loss function for the iteration 11830----->43.237376130517326 :)\n",
            "The loss function for the iteration 11840----->43.237374018860415 :)\n",
            "The loss function for the iteration 11850----->43.23737191569996 :)\n",
            "The loss function for the iteration 11860----->43.237369821001465 :)\n",
            "The loss function for the iteration 11870----->43.237367734730576 :)\n",
            "The loss function for the iteration 11880----->43.23736565685309 :)\n",
            "The loss function for the iteration 11890----->43.23736358733495 :)\n",
            "The loss function for the iteration 11900----->43.23736152614223 :)\n",
            "The loss function for the iteration 11910----->43.2373594732411 :)\n",
            "The loss function for the iteration 11920----->43.23735742859795 :)\n",
            "The loss function for the iteration 11930----->43.237355392179246 :)\n",
            "The loss function for the iteration 11940----->43.23735336395163 :)\n",
            "The loss function for the iteration 11950----->43.23735134388183 :)\n",
            "The loss function for the iteration 11960----->43.237349331936784 :)\n",
            "The loss function for the iteration 11970----->43.2373473280835 :)\n",
            "The loss function for the iteration 11980----->43.23734533228916 :)\n",
            "The loss function for the iteration 11990----->43.23734334452105 :)\n",
            "The loss function for the iteration 12000----->43.23734136474663 :)\n",
            "The loss function for the iteration 12010----->43.23733939293344 :)\n",
            "The loss function for the iteration 12020----->43.2373374290492 :)\n",
            "The loss function for the iteration 12030----->43.237335473061755 :)\n",
            "The loss function for the iteration 12040----->43.23733352493906 :)\n",
            "The loss function for the iteration 12050----->43.237331584649226 :)\n",
            "The loss function for the iteration 12060----->43.23732965216046 :)\n",
            "The loss function for the iteration 12070----->43.23732772744114 :)\n",
            "The loss function for the iteration 12080----->43.23732581045975 :)\n",
            "The loss function for the iteration 12090----->43.23732390118492 :)\n",
            "The loss function for the iteration 12100----->43.237321999585376 :)\n",
            "The loss function for the iteration 12110----->43.237320105630005 :)\n",
            "The loss function for the iteration 12120----->43.23731821928781 :)\n",
            "The loss function for the iteration 12130----->43.23731634052792 :)\n",
            "The loss function for the iteration 12140----->43.23731446931958 :)\n",
            "The loss function for the iteration 12150----->43.2373126056322 :)\n",
            "The loss function for the iteration 12160----->43.237310749435274 :)\n",
            "The loss function for the iteration 12170----->43.23730890069841 :)\n",
            "The loss function for the iteration 12180----->43.23730705939139 :)\n",
            "The loss function for the iteration 12190----->43.23730522548409 :)\n",
            "The loss function for the iteration 12200----->43.23730339894653 :)\n",
            "The loss function for the iteration 12210----->43.2373015797488 :)\n",
            "The loss function for the iteration 12220----->43.237299767861174 :)\n",
            "The loss function for the iteration 12230----->43.23729796325403 :)\n",
            "The loss function for the iteration 12240----->43.237296165897845 :)\n",
            "The loss function for the iteration 12250----->43.23729437576324 :)\n",
            "The loss function for the iteration 12260----->43.23729259282094 :)\n",
            "The loss function for the iteration 12270----->43.237290817041796 :)\n",
            "The loss function for the iteration 12280----->43.2372890483968 :)\n",
            "The loss function for the iteration 12290----->43.23728728685703 :)\n",
            "The loss function for the iteration 12300----->43.23728553239369 :)\n",
            "The loss function for the iteration 12310----->43.23728378497813 :)\n",
            "The loss function for the iteration 12320----->43.237282044581775 :)\n",
            "The loss function for the iteration 12330----->43.23728031117619 :)\n",
            "The loss function for the iteration 12340----->43.23727858473306 :)\n",
            "The loss function for the iteration 12350----->43.23727686522418 :)\n",
            "The loss function for the iteration 12360----->43.237275152621436 :)\n",
            "The loss function for the iteration 12370----->43.237273446896886 :)\n",
            "The loss function for the iteration 12380----->43.23727174802265 :)\n",
            "The loss function for the iteration 12390----->43.23727005597099 :)\n",
            "The loss function for the iteration 12400----->43.237268370714254 :)\n",
            "The loss function for the iteration 12410----->43.23726669222494 :)\n",
            "The loss function for the iteration 12420----->43.237265020475654 :)\n",
            "The loss function for the iteration 12430----->43.23726335543907 :)\n",
            "The loss function for the iteration 12440----->43.23726169708801 :)\n",
            "The loss function for the iteration 12450----->43.23726004539541 :)\n",
            "The loss function for the iteration 12460----->43.23725840033432 :)\n",
            "The loss function for the iteration 12470----->43.23725676187785 :)\n",
            "The loss function for the iteration 12480----->43.2372551299993 :)\n",
            "The loss function for the iteration 12490----->43.23725350467201 :)\n",
            "The loss function for the iteration 12500----->43.23725188586946 :)\n",
            "The loss function for the iteration 12510----->43.23725027356525 :)\n",
            "The loss function for the iteration 12520----->43.237248667733056 :)\n",
            "The loss function for the iteration 12530----->43.23724706834669 :)\n",
            "The loss function for the iteration 12540----->43.23724547538007 :)\n",
            "The loss function for the iteration 12550----->43.23724388880718 :)\n",
            "The loss function for the iteration 12560----->43.237242308602156 :)\n",
            "The loss function for the iteration 12570----->43.23724073473922 :)\n",
            "The loss function for the iteration 12580----->43.23723916719271 :)\n",
            "The loss function for the iteration 12590----->43.23723760593706 :)\n",
            "The loss function for the iteration 12600----->43.23723605094681 :)\n",
            "The loss function for the iteration 12610----->43.237234502196614 :)\n",
            "The loss function for the iteration 12620----->43.23723295966119 :)\n",
            "The loss function for the iteration 12630----->43.23723142331544 :)\n",
            "The loss function for the iteration 12640----->43.237229893134284 :)\n",
            "The loss function for the iteration 12650----->43.23722836909278 :)\n",
            "The loss function for the iteration 12660----->43.237226851166106 :)\n",
            "The loss function for the iteration 12670----->43.2372253393295 :)\n",
            "The loss function for the iteration 12680----->43.23722383355833 :)\n",
            "The loss function for the iteration 12690----->43.23722233382807 :)\n",
            "The loss function for the iteration 12700----->43.23722084011426 :)\n",
            "The loss function for the iteration 12710----->43.23721935239259 :)\n",
            "The loss function for the iteration 12720----->43.23721787063882 :)\n",
            "The loss function for the iteration 12730----->43.23721639482877 :)\n",
            "The loss function for the iteration 12740----->43.23721492493844 :)\n",
            "The loss function for the iteration 12750----->43.23721346094389 :)\n",
            "The loss function for the iteration 12760----->43.23721200282125 :)\n",
            "The loss function for the iteration 12770----->43.237210550546784 :)\n",
            "The loss function for the iteration 12780----->43.23720910409684 :)\n",
            "The loss function for the iteration 12790----->43.23720766344787 :)\n",
            "The loss function for the iteration 12800----->43.23720622857641 :)\n",
            "The loss function for the iteration 12810----->43.23720479945911 :)\n",
            "The loss function for the iteration 12820----->43.23720337607268 :)\n",
            "The loss function for the iteration 12830----->43.23720195839396 :)\n",
            "The loss function for the iteration 12840----->43.2372005463999 :)\n",
            "The loss function for the iteration 12850----->43.23719914006747 :)\n",
            "The loss function for the iteration 12860----->43.23719773937382 :)\n",
            "The loss function for the iteration 12870----->43.23719634429613 :)\n",
            "The loss function for the iteration 12880----->43.23719495481171 :)\n",
            "The loss function for the iteration 12890----->43.23719357089796 :)\n",
            "The loss function for the iteration 12900----->43.23719219253234 :)\n",
            "The loss function for the iteration 12910----->43.237190819692444 :)\n",
            "The loss function for the iteration 12920----->43.23718945235592 :)\n",
            "The loss function for the iteration 12930----->43.237188090500545 :)\n",
            "The loss function for the iteration 12940----->43.23718673410416 :)\n",
            "The loss function for the iteration 12950----->43.23718538314469 :)\n",
            "The loss function for the iteration 12960----->43.23718403760017 :)\n",
            "The loss function for the iteration 12970----->43.237182697448745 :)\n",
            "The loss function for the iteration 12980----->43.237181362668586 :)\n",
            "The loss function for the iteration 12990----->43.237180033238005 :)\n",
            "The loss function for the iteration 13000----->43.23717870913539 :)\n",
            "The loss function for the iteration 13010----->43.23717739033921 :)\n",
            "The loss function for the iteration 13020----->43.23717607682803 :)\n",
            "The loss function for the iteration 13030----->43.23717476858048 :)\n",
            "The loss function for the iteration 13040----->43.23717346557532 :)\n",
            "The loss function for the iteration 13050----->43.23717216779137 :)\n",
            "The loss function for the iteration 13060----->43.23717087520752 :)\n",
            "The loss function for the iteration 13070----->43.237169587802775 :)\n",
            "The loss function for the iteration 13080----->43.23716830555621 :)\n",
            "The loss function for the iteration 13090----->43.23716702844702 :)\n",
            "The loss function for the iteration 13100----->43.2371657564544 :)\n",
            "The loss function for the iteration 13110----->43.23716448955774 :)\n",
            "The loss function for the iteration 13120----->43.23716322773644 :)\n",
            "The loss function for the iteration 13130----->43.23716197096999 :)\n",
            "The loss function for the iteration 13140----->43.237160719238 :)\n",
            "The loss function for the iteration 13150----->43.23715947252011 :)\n",
            "The loss function for the iteration 13160----->43.237158230796105 :)\n",
            "The loss function for the iteration 13170----->43.23715699404579 :)\n",
            "The loss function for the iteration 13180----->43.23715576224911 :)\n",
            "The loss function for the iteration 13190----->43.23715453538605 :)\n",
            "The loss function for the iteration 13200----->43.2371533134367 :)\n",
            "The loss function for the iteration 13210----->43.23715209638121 :)\n",
            "The loss function for the iteration 13220----->43.237150884199835 :)\n",
            "The loss function for the iteration 13230----->43.237149676872896 :)\n",
            "The loss function for the iteration 13240----->43.237148474380795 :)\n",
            "The loss function for the iteration 13250----->43.23714727670401 :)\n",
            "The loss function for the iteration 13260----->43.23714608382312 :)\n",
            "The loss function for the iteration 13270----->43.23714489571875 :)\n",
            "The loss function for the iteration 13280----->43.23714371237163 :)\n",
            "The loss function for the iteration 13290----->43.237142533762565 :)\n",
            "The loss function for the iteration 13300----->43.23714135987242 :)\n",
            "The loss function for the iteration 13310----->43.23714019068217 :)\n",
            "The loss function for the iteration 13320----->43.23713902617282 :)\n",
            "The loss function for the iteration 13330----->43.23713786632551 :)\n",
            "The loss function for the iteration 13340----->43.23713671112141 :)\n",
            "The loss function for the iteration 13350----->43.2371355605418 :)\n",
            "The loss function for the iteration 13360----->43.237134414568 :)\n",
            "The loss function for the iteration 13370----->43.237133273181435 :)\n",
            "The loss function for the iteration 13380----->43.23713213636361 :)\n",
            "The loss function for the iteration 13390----->43.237131004096085 :)\n",
            "The loss function for the iteration 13400----->43.23712987636051 :)\n",
            "The loss function for the iteration 13410----->43.23712875313858 :)\n",
            "The loss function for the iteration 13420----->43.2371276344121 :)\n",
            "The loss function for the iteration 13430----->43.23712652016294 :)\n",
            "The loss function for the iteration 13440----->43.23712541037305 :)\n",
            "The loss function for the iteration 13450----->43.23712430502441 :)\n",
            "The loss function for the iteration 13460----->43.23712320409915 :)\n",
            "The loss function for the iteration 13470----->43.237122107579395 :)\n",
            "The loss function for the iteration 13480----->43.23712101544739 :)\n",
            "The loss function for the iteration 13490----->43.23711992768545 :)\n",
            "The loss function for the iteration 13500----->43.237118844275926 :)\n",
            "The loss function for the iteration 13510----->43.2371177652013 :)\n",
            "The loss function for the iteration 13520----->43.23711669044406 :)\n",
            "The loss function for the iteration 13530----->43.237115619986824 :)\n",
            "The loss function for the iteration 13540----->43.23711455381223 :)\n",
            "The loss function for the iteration 13550----->43.23711349190303 :)\n",
            "The loss function for the iteration 13560----->43.237112434242015 :)\n",
            "The loss function for the iteration 13570----->43.23711138081207 :)\n",
            "The loss function for the iteration 13580----->43.23711033159613 :)\n",
            "The loss function for the iteration 13590----->43.237109286577194 :)\n",
            "The loss function for the iteration 13600----->43.23710824573835 :)\n",
            "The loss function for the iteration 13610----->43.23710720906277 :)\n",
            "The loss function for the iteration 13620----->43.23710617653364 :)\n",
            "The loss function for the iteration 13630----->43.237105148134276 :)\n",
            "The loss function for the iteration 13640----->43.237104123848006 :)\n",
            "The loss function for the iteration 13650----->43.23710310365827 :)\n",
            "The loss function for the iteration 13660----->43.23710208754856 :)\n",
            "The loss function for the iteration 13670----->43.23710107550241 :)\n",
            "The loss function for the iteration 13680----->43.23710006750347 :)\n",
            "The loss function for the iteration 13690----->43.23709906353542 :)\n",
            "The loss function for the iteration 13700----->43.23709806358201 :)\n",
            "The loss function for the iteration 13710----->43.237097067627076 :)\n",
            "The loss function for the iteration 13720----->43.23709607565449 :)\n",
            "The loss function for the iteration 13730----->43.23709508764823 :)\n",
            "The loss function for the iteration 13740----->43.237094103592284 :)\n",
            "The loss function for the iteration 13750----->43.237093123470764 :)\n",
            "The loss function for the iteration 13760----->43.237092147267816 :)\n",
            "The loss function for the iteration 13770----->43.23709117496764 :)\n",
            "The loss function for the iteration 13780----->43.23709020655451 :)\n",
            "The loss function for the iteration 13790----->43.237089242012786 :)\n",
            "The loss function for the iteration 13800----->43.23708828132687 :)\n",
            "The loss function for the iteration 13810----->43.237087324481216 :)\n",
            "The loss function for the iteration 13820----->43.237086371460364 :)\n",
            "The loss function for the iteration 13830----->43.23708542224892 :)\n",
            "The loss function for the iteration 13840----->43.23708447683153 :)\n",
            "The loss function for the iteration 13850----->43.23708353519291 :)\n",
            "The loss function for the iteration 13860----->43.23708259731784 :)\n",
            "The loss function for the iteration 13870----->43.23708166319117 :)\n",
            "The loss function for the iteration 13880----->43.23708073279781 :)\n",
            "The loss function for the iteration 13890----->43.23707980612272 :)\n",
            "The loss function for the iteration 13900----->43.237078883150936 :)\n",
            "The loss function for the iteration 13910----->43.23707796386753 :)\n",
            "The loss function for the iteration 13920----->43.23707704825768 :)\n",
            "The loss function for the iteration 13930----->43.237076136306555 :)\n",
            "The loss function for the iteration 13940----->43.23707522799945 :)\n",
            "The loss function for the iteration 13950----->43.237074323321686 :)\n",
            "The loss function for the iteration 13960----->43.23707342225866 :)\n",
            "The loss function for the iteration 13970----->43.23707252479582 :)\n",
            "The loss function for the iteration 13980----->43.237071630918656 :)\n",
            "The loss function for the iteration 13990----->43.23707074061276 :)\n",
            "The loss function for the iteration 14000----->43.23706985386373 :)\n",
            "The loss function for the iteration 14010----->43.23706897065727 :)\n",
            "The loss function for the iteration 14020----->43.237068090979115 :)\n",
            "The loss function for the iteration 14030----->43.23706721481506 :)\n",
            "The loss function for the iteration 14040----->43.237066342150975 :)\n",
            "The loss function for the iteration 14050----->43.23706547297276 :)\n",
            "The loss function for the iteration 14060----->43.23706460726639 :)\n",
            "The loss function for the iteration 14070----->43.2370637450179 :)\n",
            "The loss function for the iteration 14080----->43.23706288621336 :)\n",
            "The loss function for the iteration 14090----->43.23706203083892 :)\n",
            "The loss function for the iteration 14100----->43.23706117888079 :)\n",
            "The loss function for the iteration 14110----->43.237060330325214 :)\n",
            "The loss function for the iteration 14120----->43.2370594851585 :)\n",
            "The loss function for the iteration 14130----->43.237058643367014 :)\n",
            "The loss function for the iteration 14140----->43.23705780493717 :)\n",
            "The loss function for the iteration 14150----->43.23705696985546 :)\n",
            "The loss function for the iteration 14160----->43.23705613810841 :)\n",
            "The loss function for the iteration 14170----->43.237055309682596 :)\n",
            "The loss function for the iteration 14180----->43.23705448456465 :)\n",
            "The loss function for the iteration 14190----->43.23705366274131 :)\n",
            "The loss function for the iteration 14200----->43.237052844199276 :)\n",
            "The loss function for the iteration 14210----->43.23705202892537 :)\n",
            "The loss function for the iteration 14220----->43.23705121690645 :)\n",
            "The loss function for the iteration 14230----->43.23705040812941 :)\n",
            "The loss function for the iteration 14240----->43.23704960258124 :)\n",
            "The loss function for the iteration 14250----->43.23704880024891 :)\n",
            "The loss function for the iteration 14260----->43.237048001119526 :)\n",
            "The loss function for the iteration 14270----->43.2370472051802 :)\n",
            "The loss function for the iteration 14280----->43.23704641241808 :)\n",
            "The loss function for the iteration 14290----->43.23704562282044 :)\n",
            "The loss function for the iteration 14300----->43.2370448363745 :)\n",
            "The loss function for the iteration 14310----->43.237044053067635 :)\n",
            "The loss function for the iteration 14320----->43.23704327288719 :)\n",
            "The loss function for the iteration 14330----->43.23704249582061 :)\n",
            "The loss function for the iteration 14340----->43.237041721855384 :)\n",
            "The loss function for the iteration 14350----->43.23704095097902 :)\n",
            "The loss function for the iteration 14360----->43.23704018317912 :)\n",
            "The loss function for the iteration 14370----->43.23703941844332 :)\n",
            "The loss function for the iteration 14380----->43.23703865675928 :)\n",
            "The loss function for the iteration 14390----->43.23703789811476 :)\n",
            "The loss function for the iteration 14400----->43.23703714249752 :)\n",
            "The loss function for the iteration 14410----->43.237036389895415 :)\n",
            "The loss function for the iteration 14420----->43.237035640296305 :)\n",
            "The loss function for the iteration 14430----->43.237034893688126 :)\n",
            "The loss function for the iteration 14440----->43.23703415005887 :)\n",
            "The loss function for the iteration 14450----->43.237033409396545 :)\n",
            "The loss function for the iteration 14460----->43.23703267168925 :)\n",
            "The loss function for the iteration 14470----->43.237031936925106 :)\n",
            "The loss function for the iteration 14480----->43.237031205092265 :)\n",
            "The loss function for the iteration 14490----->43.23703047617897 :)\n",
            "The loss function for the iteration 14500----->43.2370297501735 :)\n",
            "The loss function for the iteration 14510----->43.23702902706414 :)\n",
            "The loss function for the iteration 14520----->43.237028306839264 :)\n",
            "The loss function for the iteration 14530----->43.23702758948729 :)\n",
            "The loss function for the iteration 14540----->43.23702687499669 :)\n",
            "The loss function for the iteration 14550----->43.237026163355935 :)\n",
            "The loss function for the iteration 14560----->43.2370254545536 :)\n",
            "The loss function for the iteration 14570----->43.237024748578285 :)\n",
            "The loss function for the iteration 14580----->43.23702404541862 :)\n",
            "The loss function for the iteration 14590----->43.2370233450633 :)\n",
            "The loss function for the iteration 14600----->43.237022647501064 :)\n",
            "The loss function for the iteration 14610----->43.23702195272069 :)\n",
            "The loss function for the iteration 14620----->43.23702126071102 :)\n",
            "The loss function for the iteration 14630----->43.23702057146089 :)\n",
            "The loss function for the iteration 14640----->43.237019884959246 :)\n",
            "The loss function for the iteration 14650----->43.23701920119505 :)\n",
            "The loss function for the iteration 14660----->43.23701852015731 :)\n",
            "The loss function for the iteration 14670----->43.237017841835055 :)\n",
            "The loss function for the iteration 14680----->43.237017166217406 :)\n",
            "The loss function for the iteration 14690----->43.23701649329349 :)\n",
            "The loss function for the iteration 14700----->43.23701582305249 :)\n",
            "The loss function for the iteration 14710----->43.23701515548364 :)\n",
            "The loss function for the iteration 14720----->43.23701449057621 :)\n",
            "The loss function for the iteration 14730----->43.23701382831951 :)\n",
            "The loss function for the iteration 14740----->43.23701316870291 :)\n",
            "The loss function for the iteration 14750----->43.23701251171578 :)\n",
            "The loss function for the iteration 14760----->43.23701185734761 :)\n",
            "The loss function for the iteration 14770----->43.23701120558785 :)\n",
            "The loss function for the iteration 14780----->43.23701055642605 :)\n",
            "The loss function for the iteration 14790----->43.23700990985177 :)\n",
            "The loss function for the iteration 14800----->43.237009265854624 :)\n",
            "The loss function for the iteration 14810----->43.23700862442426 :)\n",
            "The loss function for the iteration 14820----->43.23700798555041 :)\n",
            "The loss function for the iteration 14830----->43.23700734922279 :)\n",
            "The loss function for the iteration 14840----->43.23700671543117 :)\n",
            "The loss function for the iteration 14850----->43.2370060841654 :)\n",
            "The loss function for the iteration 14860----->43.2370054554153 :)\n",
            "The loss function for the iteration 14870----->43.23700482917082 :)\n",
            "The loss function for the iteration 14880----->43.23700420542189 :)\n",
            "The loss function for the iteration 14890----->43.23700358415849 :)\n",
            "The loss function for the iteration 14900----->43.23700296537065 :)\n",
            "The loss function for the iteration 14910----->43.23700234904843 :)\n",
            "The loss function for the iteration 14920----->43.237001735181956 :)\n",
            "The loss function for the iteration 14930----->43.23700112376135 :)\n",
            "The loss function for the iteration 14940----->43.23700051477682 :)\n",
            "The loss function for the iteration 14950----->43.23699990821859 :)\n",
            "The loss function for the iteration 14960----->43.23699930407692 :)\n",
            "The loss function for the iteration 14970----->43.23699870234211 :)\n",
            "The loss function for the iteration 14980----->43.236998103004524 :)\n",
            "The loss function for the iteration 14990----->43.23699750605452 :)\n",
            "The loss function for the iteration 15000----->43.23699691148254 :)\n",
            "The loss function for the iteration 15010----->43.23699631927905 :)\n",
            "The loss function for the iteration 15020----->43.23699572943453 :)\n",
            "The loss function for the iteration 15030----->43.23699514193952 :)\n",
            "The loss function for the iteration 15040----->43.23699455678463 :)\n",
            "The loss function for the iteration 15050----->43.236993973960445 :)\n",
            "The loss function for the iteration 15060----->43.23699339345761 :)\n",
            "The loss function for the iteration 15070----->43.23699281526683 :)\n",
            "The loss function for the iteration 15080----->43.23699223937885 :)\n",
            "The loss function for the iteration 15090----->43.2369916657844 :)\n",
            "The loss function for the iteration 15100----->43.23699109447432 :)\n",
            "The loss function for the iteration 15110----->43.236990525439424 :)\n",
            "The loss function for the iteration 15120----->43.2369899586706 :)\n",
            "The loss function for the iteration 15130----->43.23698939415874 :)\n",
            "The loss function for the iteration 15140----->43.236988831894834 :)\n",
            "The loss function for the iteration 15150----->43.236988271869855 :)\n",
            "The loss function for the iteration 15160----->43.23698771407481 :)\n",
            "The loss function for the iteration 15170----->43.236987158500774 :)\n",
            "The loss function for the iteration 15180----->43.23698660513883 :)\n",
            "The loss function for the iteration 15190----->43.23698605398013 :)\n",
            "The loss function for the iteration 15200----->43.23698550501584 :)\n",
            "The loss function for the iteration 15210----->43.236984958237144 :)\n",
            "The loss function for the iteration 15220----->43.236984413635305 :)\n",
            "The loss function for the iteration 15230----->43.23698387120157 :)\n",
            "The loss function for the iteration 15240----->43.236983330927295 :)\n",
            "The loss function for the iteration 15250----->43.23698279280378 :)\n",
            "The loss function for the iteration 15260----->43.23698225682243 :)\n",
            "The loss function for the iteration 15270----->43.23698172297464 :)\n",
            "The loss function for the iteration 15280----->43.23698119125188 :)\n",
            "The loss function for the iteration 15290----->43.236980661645624 :)\n",
            "The loss function for the iteration 15300----->43.236980134147394 :)\n",
            "The loss function for the iteration 15310----->43.23697960874873 :)\n",
            "The loss function for the iteration 15320----->43.23697908544124 :)\n",
            "The loss function for the iteration 15330----->43.23697856421654 :)\n",
            "The loss function for the iteration 15340----->43.23697804506627 :)\n",
            "The loss function for the iteration 15350----->43.236977527982134 :)\n",
            "The loss function for the iteration 15360----->43.23697701295585 :)\n",
            "The loss function for the iteration 15370----->43.23697649997918 :)\n",
            "The loss function for the iteration 15380----->43.236975989043884 :)\n",
            "The loss function for the iteration 15390----->43.23697548014182 :)\n",
            "The loss function for the iteration 15400----->43.23697497326483 :)\n",
            "The loss function for the iteration 15410----->43.236974468404796 :)\n",
            "The loss function for the iteration 15420----->43.23697396555364 :)\n",
            "The loss function for the iteration 15430----->43.23697346470332 :)\n",
            "The loss function for the iteration 15440----->43.23697296584583 :)\n",
            "The loss function for the iteration 15450----->43.23697246897316 :)\n",
            "The loss function for the iteration 15460----->43.23697197407739 :)\n",
            "The loss function for the iteration 15470----->43.2369714811506 :)\n",
            "The loss function for the iteration 15480----->43.23697099018488 :)\n",
            "The loss function for the iteration 15490----->43.236970501172394 :)\n",
            "The loss function for the iteration 15500----->43.23697001410532 :)\n",
            "The loss function for the iteration 15510----->43.23696952897586 :)\n",
            "The loss function for the iteration 15520----->43.23696904577627 :)\n",
            "The loss function for the iteration 15530----->43.23696856449881 :)\n",
            "The loss function for the iteration 15540----->43.236968085135786 :)\n",
            "The loss function for the iteration 15550----->43.23696760767953 :)\n",
            "The loss function for the iteration 15560----->43.23696713212241 :)\n",
            "The loss function for the iteration 15570----->43.23696665845683 :)\n",
            "The loss function for the iteration 15580----->43.236966186675204 :)\n",
            "The loss function for the iteration 15590----->43.23696571677 :)\n",
            "The loss function for the iteration 15600----->43.23696524873371 :)\n",
            "The loss function for the iteration 15610----->43.23696478255883 :)\n",
            "The loss function for the iteration 15620----->43.236964318237916 :)\n",
            "The loss function for the iteration 15630----->43.236963855763555 :)\n",
            "The loss function for the iteration 15640----->43.236963395128356 :)\n",
            "The loss function for the iteration 15650----->43.236962936324964 :)\n",
            "The loss function for the iteration 15660----->43.23696247934603 :)\n",
            "The loss function for the iteration 15670----->43.23696202418426 :)\n",
            "The loss function for the iteration 15680----->43.23696157083238 :)\n",
            "The loss function for the iteration 15690----->43.23696111928315 :)\n",
            "The loss function for the iteration 15700----->43.236960669529346 :)\n",
            "The loss function for the iteration 15710----->43.236960221563784 :)\n",
            "The loss function for the iteration 15720----->43.236959775379326 :)\n",
            "The loss function for the iteration 15730----->43.236959330968816 :)\n",
            "The loss function for the iteration 15740----->43.236958888325184 :)\n",
            "The loss function for the iteration 15750----->43.236958447441346 :)\n",
            "The loss function for the iteration 15760----->43.23695800831026 :)\n",
            "The loss function for the iteration 15770----->43.23695757092491 :)\n",
            "The loss function for the iteration 15780----->43.23695713527831 :)\n",
            "The loss function for the iteration 15790----->43.23695670136352 :)\n",
            "The loss function for the iteration 15800----->43.2369562691736 :)\n",
            "The loss function for the iteration 15810----->43.23695583870163 :)\n",
            "The loss function for the iteration 15820----->43.23695540994077 :)\n",
            "The loss function for the iteration 15830----->43.236954982884164 :)\n",
            "The loss function for the iteration 15840----->43.23695455752498 :)\n",
            "The loss function for the iteration 15850----->43.23695413385644 :)\n",
            "The loss function for the iteration 15860----->43.23695371187179 :)\n",
            "The loss function for the iteration 15870----->43.23695329156428 :)\n",
            "The loss function for the iteration 15880----->43.236952872927205 :)\n",
            "The loss function for the iteration 15890----->43.23695245595388 :)\n",
            "The loss function for the iteration 15900----->43.23695204063767 :)\n",
            "The loss function for the iteration 15910----->43.23695162697192 :)\n",
            "The loss function for the iteration 15920----->43.236951214950054 :)\n",
            "The loss function for the iteration 15930----->43.236950804565474 :)\n",
            "The loss function for the iteration 15940----->43.236950395811654 :)\n",
            "The loss function for the iteration 15950----->43.23694998868206 :)\n",
            "The loss function for the iteration 15960----->43.2369495831702 :)\n",
            "The loss function for the iteration 15970----->43.23694917926961 :)\n",
            "The loss function for the iteration 15980----->43.23694877697383 :)\n",
            "The loss function for the iteration 15990----->43.23694837627647 :)\n",
            "The loss function for the iteration 16000----->43.236947977171106 :)\n",
            "The loss function for the iteration 16010----->43.23694757965141 :)\n",
            "The loss function for the iteration 16020----->43.23694718371101 :)\n",
            "The loss function for the iteration 16030----->43.23694678934361 :)\n",
            "The loss function for the iteration 16040----->43.236946396542926 :)\n",
            "The loss function for the iteration 16050----->43.23694600530268 :)\n",
            "The loss function for the iteration 16060----->43.23694561561664 :)\n",
            "The loss function for the iteration 16070----->43.23694522747858 :)\n",
            "The loss function for the iteration 16080----->43.23694484088234 :)\n",
            "The loss function for the iteration 16090----->43.23694445582173 :)\n",
            "The loss function for the iteration 16100----->43.23694407229064 :)\n",
            "The loss function for the iteration 16110----->43.23694369028293 :)\n",
            "The loss function for the iteration 16120----->43.236943309792515 :)\n",
            "The loss function for the iteration 16130----->43.23694293081335 :)\n",
            "The loss function for the iteration 16140----->43.23694255333937 :)\n",
            "The loss function for the iteration 16150----->43.236942177364575 :)\n",
            "The loss function for the iteration 16160----->43.23694180288297 :)\n",
            "The loss function for the iteration 16170----->43.236941429888596 :)\n",
            "The loss function for the iteration 16180----->43.23694105837549 :)\n",
            "The loss function for the iteration 16190----->43.236940688337754 :)\n",
            "The loss function for the iteration 16200----->43.23694031976948 :)\n",
            "The loss function for the iteration 16210----->43.2369399526648 :)\n",
            "The loss function for the iteration 16220----->43.23693958701785 :)\n",
            "The loss function for the iteration 16230----->43.23693922282284 :)\n",
            "The loss function for the iteration 16240----->43.23693886007395 :)\n",
            "The loss function for the iteration 16250----->43.23693849876539 :)\n",
            "The loss function for the iteration 16260----->43.236938138891425 :)\n",
            "The loss function for the iteration 16270----->43.23693778044632 :)\n",
            "The loss function for the iteration 16280----->43.236937423424344 :)\n",
            "The loss function for the iteration 16290----->43.236937067819845 :)\n",
            "The loss function for the iteration 16300----->43.23693671362715 :)\n",
            "The loss function for the iteration 16310----->43.23693636084061 :)\n",
            "The loss function for the iteration 16320----->43.23693600945462 :)\n",
            "The loss function for the iteration 16330----->43.23693565946357 :)\n",
            "The loss function for the iteration 16340----->43.2369353108619 :)\n",
            "The loss function for the iteration 16350----->43.23693496364406 :)\n",
            "The loss function for the iteration 16360----->43.23693461780452 :)\n",
            "The loss function for the iteration 16370----->43.23693427333778 :)\n",
            "The loss function for the iteration 16380----->43.23693393023835 :)\n",
            "The loss function for the iteration 16390----->43.236933588500776 :)\n",
            "The loss function for the iteration 16400----->43.23693324811962 :)\n",
            "The loss function for the iteration 16410----->43.23693290908946 :)\n",
            "The loss function for the iteration 16420----->43.23693257140491 :)\n",
            "The loss function for the iteration 16430----->43.236932235060564 :)\n",
            "The loss function for the iteration 16440----->43.236931900051125 :)\n",
            "The loss function for the iteration 16450----->43.23693156637122 :)\n",
            "The loss function for the iteration 16460----->43.23693123401556 :)\n",
            "The loss function for the iteration 16470----->43.23693090297885 :)\n",
            "The loss function for the iteration 16480----->43.236930573255826 :)\n",
            "The loss function for the iteration 16490----->43.23693024484125 :)\n",
            "The loss function for the iteration 16500----->43.23692991772989 :)\n",
            "The loss function for the iteration 16510----->43.23692959191653 :)\n",
            "The loss function for the iteration 16520----->43.23692926739602 :)\n",
            "The loss function for the iteration 16530----->43.23692894416317 :)\n",
            "The loss function for the iteration 16540----->43.23692862221286 :)\n",
            "The loss function for the iteration 16550----->43.236928301539955 :)\n",
            "The loss function for the iteration 16560----->43.23692798213937 :)\n",
            "The loss function for the iteration 16570----->43.23692766400602 :)\n",
            "The loss function for the iteration 16580----->43.236927347134845 :)\n",
            "The loss function for the iteration 16590----->43.23692703152081 :)\n",
            "The loss function for the iteration 16600----->43.23692671715889 :)\n",
            "The loss function for the iteration 16610----->43.236926404044105 :)\n",
            "The loss function for the iteration 16620----->43.23692609217145 :)\n",
            "The loss function for the iteration 16630----->43.236925781536 :)\n",
            "The loss function for the iteration 16640----->43.2369254721328 :)\n",
            "The loss function for the iteration 16650----->43.23692516395693 :)\n",
            "The loss function for the iteration 16660----->43.23692485700351 :)\n",
            "The loss function for the iteration 16670----->43.23692455126765 :)\n",
            "The loss function for the iteration 16680----->43.23692424674447 :)\n",
            "The loss function for the iteration 16690----->43.236923943429176 :)\n",
            "The loss function for the iteration 16700----->43.236923641316906 :)\n",
            "The loss function for the iteration 16710----->43.23692334040289 :)\n",
            "The loss function for the iteration 16720----->43.23692304068233 :)\n",
            "The loss function for the iteration 16730----->43.23692274215047 :)\n",
            "The loss function for the iteration 16740----->43.23692244480257 :)\n",
            "The loss function for the iteration 16750----->43.236922148633916 :)\n",
            "The loss function for the iteration 16760----->43.23692185363977 :)\n",
            "The loss function for the iteration 16770----->43.236921559815485 :)\n",
            "The loss function for the iteration 16780----->43.236921267156376 :)\n",
            "The loss function for the iteration 16790----->43.2369209756578 :)\n",
            "The loss function for the iteration 16800----->43.23692068531512 :)\n",
            "The loss function for the iteration 16810----->43.236920396123736 :)\n",
            "The loss function for the iteration 16820----->43.236920108079055 :)\n",
            "The loss function for the iteration 16830----->43.236919821176485 :)\n",
            "The loss function for the iteration 16840----->43.2369195354115 :)\n",
            "The loss function for the iteration 16850----->43.236919250779536 :)\n",
            "The loss function for the iteration 16860----->43.23691896727608 :)\n",
            "The loss function for the iteration 16870----->43.23691868489666 :)\n",
            "The loss function for the iteration 16880----->43.23691840363675 :)\n",
            "The loss function for the iteration 16890----->43.23691812349192 :)\n",
            "The loss function for the iteration 16900----->43.23691784445771 :)\n",
            "The loss function for the iteration 16910----->43.236917566529684 :)\n",
            "The loss function for the iteration 16920----->43.236917289703435 :)\n",
            "The loss function for the iteration 16930----->43.23691701397458 :)\n",
            "The loss function for the iteration 16940----->43.23691673933874 :)\n",
            "The loss function for the iteration 16950----->43.236916465791545 :)\n",
            "The loss function for the iteration 16960----->43.23691619332866 :)\n",
            "The loss function for the iteration 16970----->43.23691592194577 :)\n",
            "The loss function for the iteration 16980----->43.236915651638576 :)\n",
            "The loss function for the iteration 16990----->43.23691538240277 :)\n",
            "The loss function for the iteration 17000----->43.23691511423409 :)\n",
            "The loss function for the iteration 17010----->43.23691484712826 :)\n",
            "The loss function for the iteration 17020----->43.23691458108109 :)\n",
            "The loss function for the iteration 17030----->43.23691431608832 :)\n",
            "The loss function for the iteration 17040----->43.23691405214576 :)\n",
            "The loss function for the iteration 17050----->43.23691378924923 :)\n",
            "The loss function for the iteration 17060----->43.23691352739454 :)\n",
            "The loss function for the iteration 17070----->43.236913266577574 :)\n",
            "The loss function for the iteration 17080----->43.23691300679416 :)\n",
            "The loss function for the iteration 17090----->43.2369127480402 :)\n",
            "The loss function for the iteration 17100----->43.236912490311575 :)\n",
            "The loss function for the iteration 17110----->43.2369122336042 :)\n",
            "The loss function for the iteration 17120----->43.23691197791403 :)\n",
            "The loss function for the iteration 17130----->43.23691172323699 :)\n",
            "The loss function for the iteration 17140----->43.23691146956905 :)\n",
            "The loss function for the iteration 17150----->43.23691121690618 :)\n",
            "The loss function for the iteration 17160----->43.236910965244384 :)\n",
            "The loss function for the iteration 17170----->43.23691071457966 :)\n",
            "The loss function for the iteration 17180----->43.23691046490806 :)\n",
            "The loss function for the iteration 17190----->43.23691021622559 :)\n",
            "The loss function for the iteration 17200----->43.23690996852835 :)\n",
            "The loss function for the iteration 17210----->43.23690972181239 :)\n",
            "The loss function for the iteration 17220----->43.2369094760738 :)\n",
            "The loss function for the iteration 17230----->43.23690923130869 :)\n",
            "The loss function for the iteration 17240----->43.23690898751319 :)\n",
            "The loss function for the iteration 17250----->43.23690874468342 :)\n",
            "The loss function for the iteration 17260----->43.23690850281556 :)\n",
            "The loss function for the iteration 17270----->43.23690826190574 :)\n",
            "The loss function for the iteration 17280----->43.236908021950185 :)\n",
            "The loss function for the iteration 17290----->43.23690778294507 :)\n",
            "The loss function for the iteration 17300----->43.23690754488661 :)\n",
            "The loss function for the iteration 17310----->43.23690730777103 :)\n",
            "The loss function for the iteration 17320----->43.23690707159457 :)\n",
            "The loss function for the iteration 17330----->43.23690683635353 :)\n",
            "The loss function for the iteration 17340----->43.236906602044144 :)\n",
            "The loss function for the iteration 17350----->43.23690636866272 :)\n",
            "The loss function for the iteration 17360----->43.23690613620554 :)\n",
            "The loss function for the iteration 17370----->43.236905904668944 :)\n",
            "The loss function for the iteration 17380----->43.23690567404926 :)\n",
            "The loss function for the iteration 17390----->43.236905444342845 :)\n",
            "The loss function for the iteration 17400----->43.23690521554605 :)\n",
            "The loss function for the iteration 17410----->43.23690498765526 :)\n",
            "The loss function for the iteration 17420----->43.23690476066686 :)\n",
            "The loss function for the iteration 17430----->43.236904534577256 :)\n",
            "The loss function for the iteration 17440----->43.236904309382886 :)\n",
            "The loss function for the iteration 17450----->43.23690408508016 :)\n",
            "The loss function for the iteration 17460----->43.236903861665546 :)\n",
            "The loss function for the iteration 17470----->43.23690363913551 :)\n",
            "The loss function for the iteration 17480----->43.23690341748652 :)\n",
            "The loss function for the iteration 17490----->43.23690319671508 :)\n",
            "The loss function for the iteration 17500----->43.23690297681769 :)\n",
            "The loss function for the iteration 17510----->43.23690275779085 :)\n",
            "The loss function for the iteration 17520----->43.236902539631124 :)\n",
            "The loss function for the iteration 17530----->43.23690232233506 :)\n",
            "The loss function for the iteration 17540----->43.2369021058992 :)\n",
            "The loss function for the iteration 17550----->43.23690189032012 :)\n",
            "The loss function for the iteration 17560----->43.236901675594424 :)\n",
            "The loss function for the iteration 17570----->43.23690146171872 :)\n",
            "The loss function for the iteration 17580----->43.23690124868962 :)\n",
            "The loss function for the iteration 17590----->43.23690103650374 :)\n",
            "The loss function for the iteration 17600----->43.23690082515773 :)\n",
            "The loss function for the iteration 17610----->43.23690061464826 :)\n",
            "The loss function for the iteration 17620----->43.23690040497198 :)\n",
            "The loss function for the iteration 17630----->43.2369001961256 :)\n",
            "The loss function for the iteration 17640----->43.2368999881058 :)\n",
            "The loss function for the iteration 17650----->43.23689978090929 :)\n",
            "The loss function for the iteration 17660----->43.23689957453279 :)\n",
            "The loss function for the iteration 17670----->43.236899368973056 :)\n",
            "The loss function for the iteration 17680----->43.23689916422683 :)\n",
            "The loss function for the iteration 17690----->43.236898960290866 :)\n",
            "The loss function for the iteration 17700----->43.23689875716195 :)\n",
            "The loss function for the iteration 17710----->43.236898554836856 :)\n",
            "The loss function for the iteration 17720----->43.2368983533124 :)\n",
            "The loss function for the iteration 17730----->43.23689815258539 :)\n",
            "The loss function for the iteration 17740----->43.23689795265266 :)\n",
            "The loss function for the iteration 17750----->43.23689775351104 :)\n",
            "The loss function for the iteration 17760----->43.23689755515741 :)\n",
            "The loss function for the iteration 17770----->43.23689735758859 :)\n",
            "The loss function for the iteration 17780----->43.23689716080149 :)\n",
            "The loss function for the iteration 17790----->43.236896964793 :)\n",
            "The loss function for the iteration 17800----->43.23689676956 :)\n",
            "The loss function for the iteration 17810----->43.236896575099436 :)\n",
            "The loss function for the iteration 17820----->43.23689638140821 :)\n",
            "The loss function for the iteration 17830----->43.236896188483264 :)\n",
            "The loss function for the iteration 17840----->43.23689599632158 :)\n",
            "The loss function for the iteration 17850----->43.23689580492008 :)\n",
            "The loss function for the iteration 17860----->43.236895614275774 :)\n",
            "The loss function for the iteration 17870----->43.23689542438563 :)\n",
            "The loss function for the iteration 17880----->43.23689523524665 :)\n",
            "The loss function for the iteration 17890----->43.23689504685585 :)\n",
            "The loss function for the iteration 17900----->43.23689485921027 :)\n",
            "The loss function for the iteration 17910----->43.23689467230692 :)\n",
            "The loss function for the iteration 17920----->43.23689448614287 :)\n",
            "The loss function for the iteration 17930----->43.23689430071517 :)\n",
            "The loss function for the iteration 17940----->43.2368941160209 :)\n",
            "The loss function for the iteration 17950----->43.23689393205712 :)\n",
            "The loss function for the iteration 17960----->43.23689374882096 :)\n",
            "The loss function for the iteration 17970----->43.23689356630949 :)\n",
            "The loss function for the iteration 17980----->43.23689338451987 :)\n",
            "The loss function for the iteration 17990----->43.2368932034492 :)\n",
            "The loss function for the iteration 18000----->43.23689302309463 :)\n",
            "The loss function for the iteration 18010----->43.2368928434533 :)\n",
            "The loss function for the iteration 18020----->43.2368926645224 :)\n",
            "The loss function for the iteration 18030----->43.23689248629909 :)\n",
            "The loss function for the iteration 18040----->43.236892308780554 :)\n",
            "The loss function for the iteration 18050----->43.23689213196399 :)\n",
            "The loss function for the iteration 18060----->43.23689195584663 :)\n",
            "The loss function for the iteration 18070----->43.23689178042567 :)\n",
            "The loss function for the iteration 18080----->43.23689160569835 :)\n",
            "The loss function for the iteration 18090----->43.23689143166191 :)\n",
            "The loss function for the iteration 18100----->43.236891258313605 :)\n",
            "The loss function for the iteration 18110----->43.236891085650704 :)\n",
            "The loss function for the iteration 18120----->43.236890913670486 :)\n",
            "The loss function for the iteration 18130----->43.23689074237023 :)\n",
            "The loss function for the iteration 18140----->43.23689057174723 :)\n",
            "The loss function for the iteration 18150----->43.2368904017988 :)\n",
            "The loss function for the iteration 18160----->43.236890232522256 :)\n",
            "The loss function for the iteration 18170----->43.23689006391494 :)\n",
            "The loss function for the iteration 18180----->43.23688989597418 :)\n",
            "The loss function for the iteration 18190----->43.23688972869732 :)\n",
            "The loss function for the iteration 18200----->43.23688956208174 :)\n",
            "The loss function for the iteration 18210----->43.2368893961248 :)\n",
            "The loss function for the iteration 18220----->43.23688923082389 :)\n",
            "The loss function for the iteration 18230----->43.23688906617641 :)\n",
            "The loss function for the iteration 18240----->43.23688890217973 :)\n",
            "The loss function for the iteration 18250----->43.236888738831304 :)\n",
            "The loss function for the iteration 18260----->43.236888576128536 :)\n",
            "The loss function for the iteration 18270----->43.236888414068865 :)\n",
            "The loss function for the iteration 18280----->43.23688825264974 :)\n",
            "The loss function for the iteration 18290----->43.23688809186861 :)\n",
            "The loss function for the iteration 18300----->43.23688793172295 :)\n",
            "The loss function for the iteration 18310----->43.23688777221022 :)\n",
            "The loss function for the iteration 18320----->43.236887613327895 :)\n",
            "The loss function for the iteration 18330----->43.23688745507351 :)\n",
            "The loss function for the iteration 18340----->43.236887297444554 :)\n",
            "The loss function for the iteration 18350----->43.23688714043853 :)\n",
            "The loss function for the iteration 18360----->43.23688698405297 :)\n",
            "The loss function for the iteration 18370----->43.23688682828542 :)\n",
            "The loss function for the iteration 18380----->43.236886673133405 :)\n",
            "The loss function for the iteration 18390----->43.23688651859451 :)\n",
            "The loss function for the iteration 18400----->43.23688636466626 :)\n",
            "The loss function for the iteration 18410----->43.236886211346274 :)\n",
            "The loss function for the iteration 18420----->43.236886058632095 :)\n",
            "The loss function for the iteration 18430----->43.236885906521344 :)\n",
            "The loss function for the iteration 18440----->43.23688575501162 :)\n",
            "The loss function for the iteration 18450----->43.236885604100515 :)\n",
            "The loss function for the iteration 18460----->43.23688545378568 :)\n",
            "The loss function for the iteration 18470----->43.236885304064735 :)\n",
            "The loss function for the iteration 18480----->43.23688515493532 :)\n",
            "The loss function for the iteration 18490----->43.23688500639509 :)\n",
            "The loss function for the iteration 18500----->43.23688485844171 :)\n",
            "The loss function for the iteration 18510----->43.23688471107285 :)\n",
            "The loss function for the iteration 18520----->43.236884564286164 :)\n",
            "The loss function for the iteration 18530----->43.23688441807937 :)\n",
            "The loss function for the iteration 18540----->43.23688427245016 :)\n",
            "The loss function for the iteration 18550----->43.23688412739622 :)\n",
            "The loss function for the iteration 18560----->43.2368839829153 :)\n",
            "The loss function for the iteration 18570----->43.2368838390051 :)\n",
            "The loss function for the iteration 18580----->43.23688369566336 :)\n",
            "The loss function for the iteration 18590----->43.23688355288782 :)\n",
            "The loss function for the iteration 18600----->43.236883410676235 :)\n",
            "The loss function for the iteration 18610----->43.23688326902638 :)\n",
            "The loss function for the iteration 18620----->43.236883127936004 :)\n",
            "The loss function for the iteration 18630----->43.23688298740289 :)\n",
            "The loss function for the iteration 18640----->43.23688284742484 :)\n",
            "The loss function for the iteration 18650----->43.23688270799964 :)\n",
            "The loss function for the iteration 18660----->43.2368825691251 :)\n",
            "The loss function for the iteration 18670----->43.23688243079903 :)\n",
            "The loss function for the iteration 18680----->43.23688229301926 :)\n",
            "The loss function for the iteration 18690----->43.23688215578361 :)\n",
            "The loss function for the iteration 18700----->43.23688201908992 :)\n",
            "The loss function for the iteration 18710----->43.23688188293606 :)\n",
            "The loss function for the iteration 18720----->43.23688174731986 :)\n",
            "The loss function for the iteration 18730----->43.23688161223921 :)\n",
            "The loss function for the iteration 18740----->43.236881477691966 :)\n",
            "The loss function for the iteration 18750----->43.236881343676025 :)\n",
            "The loss function for the iteration 18760----->43.23688121018927 :)\n",
            "The loss function for the iteration 18770----->43.23688107722961 :)\n",
            "The loss function for the iteration 18780----->43.23688094479495 :)\n",
            "The loss function for the iteration 18790----->43.23688081288319 :)\n",
            "The loss function for the iteration 18800----->43.23688068149228 :)\n",
            "The loss function for the iteration 18810----->43.23688055062014 :)\n",
            "The loss function for the iteration 18820----->43.23688042026472 :)\n",
            "The loss function for the iteration 18830----->43.23688029042397 :)\n",
            "The loss function for the iteration 18840----->43.23688016109582 :)\n",
            "The loss function for the iteration 18850----->43.23688003227828 :)\n",
            "The loss function for the iteration 18860----->43.23687990396929 :)\n",
            "The loss function for the iteration 18870----->43.23687977616685 :)\n",
            "The loss function for the iteration 18880----->43.23687964886895 :)\n",
            "The loss function for the iteration 18890----->43.23687952207358 :)\n",
            "The loss function for the iteration 18900----->43.23687939577874 :)\n",
            "The loss function for the iteration 18910----->43.236879269982474 :)\n",
            "The loss function for the iteration 18920----->43.23687914468279 :)\n",
            "The loss function for the iteration 18930----->43.23687901987769 :)\n",
            "The loss function for the iteration 18940----->43.236878895565255 :)\n",
            "The loss function for the iteration 18950----->43.2368787717435 :)\n",
            "The loss function for the iteration 18960----->43.236878648410496 :)\n",
            "The loss function for the iteration 18970----->43.2368785255643 :)\n",
            "The loss function for the iteration 18980----->43.23687840320298 :)\n",
            "The loss function for the iteration 18990----->43.23687828132461 :)\n",
            "The loss function for the iteration 19000----->43.23687815992728 :)\n",
            "The loss function for the iteration 19010----->43.23687803900907 :)\n",
            "The loss function for the iteration 19020----->43.23687791856811 :)\n",
            "The loss function for the iteration 19030----->43.236877798602464 :)\n",
            "The loss function for the iteration 19040----->43.236877679110286 :)\n",
            "The loss function for the iteration 19050----->43.23687756008968 :)\n",
            "The loss function for the iteration 19060----->43.23687744153878 :)\n",
            "The loss function for the iteration 19070----->43.236877323455715 :)\n",
            "The loss function for the iteration 19080----->43.236877205838645 :)\n",
            "The loss function for the iteration 19090----->43.236877088685716 :)\n",
            "The loss function for the iteration 19100----->43.236876971995066 :)\n",
            "The loss function for the iteration 19110----->43.2368768557649 :)\n",
            "The loss function for the iteration 19120----->43.23687673999336 :)\n",
            "The loss function for the iteration 19130----->43.236876624678665 :)\n",
            "The loss function for the iteration 19140----->43.236876509818956 :)\n",
            "The loss function for the iteration 19150----->43.236876395412466 :)\n",
            "The loss function for the iteration 19160----->43.236876281457384 :)\n",
            "The loss function for the iteration 19170----->43.23687616795192 :)\n",
            "The loss function for the iteration 19180----->43.23687605489429 :)\n",
            "The loss function for the iteration 19190----->43.23687594228272 :)\n",
            "The loss function for the iteration 19200----->43.23687583011545 :)\n",
            "The loss function for the iteration 19210----->43.23687571839072 :)\n",
            "The loss function for the iteration 19220----->43.23687560710676 :)\n",
            "The loss function for the iteration 19230----->43.23687549626183 :)\n",
            "The loss function for the iteration 19240----->43.236875385854205 :)\n",
            "The loss function for the iteration 19250----->43.23687527588211 :)\n",
            "The loss function for the iteration 19260----->43.236875166343864 :)\n",
            "The loss function for the iteration 19270----->43.23687505723773 :)\n",
            "The loss function for the iteration 19280----->43.236874948562 :)\n",
            "The loss function for the iteration 19290----->43.236874840314954 :)\n",
            "The loss function for the iteration 19300----->43.2368747324949 :)\n",
            "The loss function for the iteration 19310----->43.23687462510016 :)\n",
            "The loss function for the iteration 19320----->43.23687451812903 :)\n",
            "The loss function for the iteration 19330----->43.23687441157983 :)\n",
            "The loss function for the iteration 19340----->43.2368743054509 :)\n",
            "The loss function for the iteration 19350----->43.23687419974057 :)\n",
            "The loss function for the iteration 19360----->43.236874094447195 :)\n",
            "The loss function for the iteration 19370----->43.23687398956909 :)\n",
            "The loss function for the iteration 19380----->43.236873885104636 :)\n",
            "The loss function for the iteration 19390----->43.23687378105219 :)\n",
            "The loss function for the iteration 19400----->43.2368736774101 :)\n",
            "The loss function for the iteration 19410----->43.23687357417677 :)\n",
            "The loss function for the iteration 19420----->43.23687347135057 :)\n",
            "The loss function for the iteration 19430----->43.23687336892988 :)\n",
            "The loss function for the iteration 19440----->43.2368732669131 :)\n",
            "The loss function for the iteration 19450----->43.23687316529861 :)\n",
            "The loss function for the iteration 19460----->43.236873064084854 :)\n",
            "The loss function for the iteration 19470----->43.2368729632702 :)\n",
            "The loss function for the iteration 19480----->43.236872862853104 :)\n",
            "The loss function for the iteration 19490----->43.23687276283198 :)\n",
            "The loss function for the iteration 19500----->43.236872663205254 :)\n",
            "The loss function for the iteration 19510----->43.23687256397136 :)\n",
            "The loss function for the iteration 19520----->43.23687246512875 :)\n",
            "The loss function for the iteration 19530----->43.236872366675875 :)\n",
            "The loss function for the iteration 19540----->43.23687226861119 :)\n",
            "The loss function for the iteration 19550----->43.23687217093314 :)\n",
            "The loss function for the iteration 19560----->43.23687207364022 :)\n",
            "The loss function for the iteration 19570----->43.2368719767309 :)\n",
            "The loss function for the iteration 19580----->43.23687188020365 :)\n",
            "The loss function for the iteration 19590----->43.23687178405695 :)\n",
            "The loss function for the iteration 19600----->43.23687168828931 :)\n",
            "The loss function for the iteration 19610----->43.23687159289923 :)\n",
            "The loss function for the iteration 19620----->43.2368714978852 :)\n",
            "The loss function for the iteration 19630----->43.23687140324574 :)\n",
            "The loss function for the iteration 19640----->43.23687130897936 :)\n",
            "The loss function for the iteration 19650----->43.23687121508459 :)\n",
            "The loss function for the iteration 19660----->43.23687112155996 :)\n",
            "The loss function for the iteration 19670----->43.23687102840398 :)\n",
            "The loss function for the iteration 19680----->43.23687093561524 :)\n",
            "The loss function for the iteration 19690----->43.236870843192236 :)\n",
            "The loss function for the iteration 19700----->43.23687075113354 :)\n",
            "The loss function for the iteration 19710----->43.23687065943772 :)\n",
            "The loss function for the iteration 19720----->43.23687056810331 :)\n",
            "The loss function for the iteration 19730----->43.23687047712891 :)\n",
            "The loss function for the iteration 19740----->43.236870386513075 :)\n",
            "The loss function for the iteration 19750----->43.236870296254416 :)\n",
            "The loss function for the iteration 19760----->43.23687020635146 :)\n",
            "The loss function for the iteration 19770----->43.23687011680285 :)\n",
            "The loss function for the iteration 19780----->43.23687002760717 :)\n",
            "The loss function for the iteration 19790----->43.23686993876302 :)\n",
            "The loss function for the iteration 19800----->43.236869850268995 :)\n",
            "The loss function for the iteration 19810----->43.236869762123725 :)\n",
            "The loss function for the iteration 19820----->43.23686967432584 :)\n",
            "The loss function for the iteration 19830----->43.23686958687392 :)\n",
            "The loss function for the iteration 19840----->43.23686949976667 :)\n",
            "The loss function for the iteration 19850----->43.236869413002665 :)\n",
            "The loss function for the iteration 19860----->43.236869326580546 :)\n",
            "The loss function for the iteration 19870----->43.23686924049899 :)\n",
            "The loss function for the iteration 19880----->43.23686915475664 :)\n",
            "The loss function for the iteration 19890----->43.23686906935214 :)\n",
            "The loss function for the iteration 19900----->43.23686898428417 :)\n",
            "The loss function for the iteration 19910----->43.236868899551396 :)\n",
            "The loss function for the iteration 19920----->43.23686881515246 :)\n",
            "The loss function for the iteration 19930----->43.23686873108609 :)\n",
            "The loss function for the iteration 19940----->43.236868647350946 :)\n",
            "The loss function for the iteration 19950----->43.236868563945706 :)\n",
            "The loss function for the iteration 19960----->43.23686848086909 :)\n",
            "The loss function for the iteration 19970----->43.236868398119775 :)\n",
            "The loss function for the iteration 19980----->43.236868315696476 :)\n",
            "The loss function for the iteration 19990----->43.2368682335979 :)\n",
            "The loss function for the iteration 20000----->43.23686815182276 :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_regression_custom_kaggle = LassoRegression(0.0001, 0.9, 20000)\n",
        "y_train_kaggle_reshaped = y_train_kaggle.reshape(-1, 1)\n",
        "lasso_regression_custom_kaggle.train(X_train_kaggle, y_train_kaggle_reshaped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wywrZWKPd4qb",
        "outputId": "df4eca0c-fc6f-4c63-d063-d945f062cb5b"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss function for the iteration 10----->69.50557213810906 :)\n",
            "The loss function for the iteration 20----->48.593104618967644 :)\n",
            "The loss function for the iteration 30----->46.23495265740718 :)\n",
            "The loss function for the iteration 40----->44.96442509014272 :)\n",
            "The loss function for the iteration 50----->44.25561276710471 :)\n",
            "The loss function for the iteration 60----->43.85626936305618 :)\n",
            "The loss function for the iteration 70----->43.62773560447448 :)\n",
            "The loss function for the iteration 80----->43.493692215891436 :)\n",
            "The loss function for the iteration 90----->43.412117465621435 :)\n",
            "The loss function for the iteration 100----->43.35986038358039 :)\n",
            "The loss function for the iteration 110----->43.32415391966733 :)\n",
            "The loss function for the iteration 120----->43.297945663853845 :)\n",
            "The loss function for the iteration 130----->43.27732918739833 :)\n",
            "The loss function for the iteration 140----->43.26014985690398 :)\n",
            "The loss function for the iteration 150----->43.24517144884872 :)\n",
            "The loss function for the iteration 160----->43.231700085209695 :)\n",
            "The loss function for the iteration 170----->43.21933505326276 :)\n",
            "The loss function for the iteration 180----->43.20783742387334 :)\n",
            "The loss function for the iteration 190----->43.19705867651262 :)\n",
            "The loss function for the iteration 200----->43.186901110605554 :)\n",
            "The loss function for the iteration 210----->43.1772962059675 :)\n",
            "The loss function for the iteration 220----->43.16819248620458 :)\n",
            "The loss function for the iteration 230----->43.15954884306267 :)\n",
            "The loss function for the iteration 240----->43.15133084583402 :)\n",
            "The loss function for the iteration 250----->43.14350842645543 :)\n",
            "The loss function for the iteration 260----->43.136054791951736 :)\n",
            "The loss function for the iteration 270----->43.12894563598274 :)\n",
            "The loss function for the iteration 280----->43.12215873323341 :)\n",
            "The loss function for the iteration 290----->43.11567356364167 :)\n",
            "The loss function for the iteration 300----->43.109471153384035 :)\n",
            "The loss function for the iteration 310----->43.10353389972239 :)\n",
            "The loss function for the iteration 320----->43.097845459762496 :)\n",
            "The loss function for the iteration 330----->43.092390666635815 :)\n",
            "The loss function for the iteration 340----->43.087155364356825 :)\n",
            "The loss function for the iteration 350----->43.08212658629915 :)\n",
            "The loss function for the iteration 360----->43.077292047777455 :)\n",
            "The loss function for the iteration 370----->43.0726402439347 :)\n",
            "The loss function for the iteration 380----->43.06816051548694 :)\n",
            "The loss function for the iteration 390----->43.06384303911259 :)\n",
            "The loss function for the iteration 400----->43.0596785935867 :)\n",
            "The loss function for the iteration 410----->43.05565850286182 :)\n",
            "The loss function for the iteration 420----->43.051774749555165 :)\n",
            "The loss function for the iteration 430----->43.048019902701384 :)\n",
            "The loss function for the iteration 440----->43.04438693372151 :)\n",
            "The loss function for the iteration 450----->43.040869349258436 :)\n",
            "The loss function for the iteration 460----->43.037461106750435 :)\n",
            "The loss function for the iteration 470----->43.0341564546354 :)\n",
            "The loss function for the iteration 480----->43.030950141999845 :)\n",
            "The loss function for the iteration 490----->43.02783714711354 :)\n",
            "The loss function for the iteration 500----->43.0248128873195 :)\n",
            "The loss function for the iteration 510----->43.021872984939876 :)\n",
            "The loss function for the iteration 520----->43.01901339842534 :)\n",
            "The loss function for the iteration 530----->43.01623029518541 :)\n",
            "The loss function for the iteration 540----->43.013520148889945 :)\n",
            "The loss function for the iteration 550----->43.01087959430285 :)\n",
            "The loss function for the iteration 560----->43.008305492273664 :)\n",
            "The loss function for the iteration 570----->43.00579497384852 :)\n",
            "The loss function for the iteration 580----->43.00334519321764 :)\n",
            "The loss function for the iteration 590----->43.000953604334356 :)\n",
            "The loss function for the iteration 600----->42.99861776038063 :)\n",
            "The loss function for the iteration 610----->42.99633536182996 :)\n",
            "The loss function for the iteration 620----->42.994104250418275 :)\n",
            "The loss function for the iteration 630----->42.99192244392331 :)\n",
            "The loss function for the iteration 640----->42.989788029418015 :)\n",
            "The loss function for the iteration 650----->42.9876991522145 :)\n",
            "The loss function for the iteration 660----->42.98565425267846 :)\n",
            "The loss function for the iteration 670----->42.983652051511555 :)\n",
            "The loss function for the iteration 680----->42.98169057403633 :)\n",
            "The loss function for the iteration 690----->42.979768390541125 :)\n",
            "The loss function for the iteration 700----->42.977884175627764 :)\n",
            "The loss function for the iteration 710----->42.97603661570458 :)\n",
            "The loss function for the iteration 720----->42.97422451296734 :)\n",
            "The loss function for the iteration 730----->42.972446734263244 :)\n",
            "The loss function for the iteration 740----->42.97070215312332 :)\n",
            "The loss function for the iteration 750----->42.968989750801775 :)\n",
            "The loss function for the iteration 760----->42.96730853603151 :)\n",
            "The loss function for the iteration 770----->42.96565759581725 :)\n",
            "The loss function for the iteration 780----->42.964035996486366 :)\n",
            "The loss function for the iteration 790----->42.96244291406585 :)\n",
            "The loss function for the iteration 800----->42.960877513443734 :)\n",
            "The loss function for the iteration 810----->42.95933904333295 :)\n",
            "The loss function for the iteration 820----->42.95782674249377 :)\n",
            "The loss function for the iteration 830----->42.956339953482434 :)\n",
            "The loss function for the iteration 840----->42.95487790912804 :)\n",
            "The loss function for the iteration 850----->42.9534400557074 :)\n",
            "The loss function for the iteration 860----->42.95202571567217 :)\n",
            "The loss function for the iteration 870----->42.95063431366381 :)\n",
            "The loss function for the iteration 880----->42.94926527557269 :)\n",
            "The loss function for the iteration 890----->42.94791803011145 :)\n",
            "The loss function for the iteration 900----->42.94659207248707 :)\n",
            "The loss function for the iteration 910----->42.94528689262661 :)\n",
            "The loss function for the iteration 920----->42.94400200858414 :)\n",
            "The loss function for the iteration 930----->42.94273692119861 :)\n",
            "The loss function for the iteration 940----->42.941491211031966 :)\n",
            "The loss function for the iteration 950----->42.94026441576489 :)\n",
            "The loss function for the iteration 960----->42.939056133087306 :)\n",
            "The loss function for the iteration 970----->42.93786592556716 :)\n",
            "The loss function for the iteration 980----->42.936693438007225 :)\n",
            "The loss function for the iteration 990----->42.93553821977972 :)\n",
            "The loss function for the iteration 1000----->42.93439998185534 :)\n",
            "The loss function for the iteration 1010----->42.93327829811813 :)\n",
            "The loss function for the iteration 1020----->42.9321728332176 :)\n",
            "The loss function for the iteration 1030----->42.93108326806502 :)\n",
            "The loss function for the iteration 1040----->42.93000928282652 :)\n",
            "The loss function for the iteration 1050----->42.92895053831137 :)\n",
            "The loss function for the iteration 1060----->42.92790675444881 :)\n",
            "The loss function for the iteration 1070----->42.92687757802873 :)\n",
            "The loss function for the iteration 1080----->42.92586274818884 :)\n",
            "The loss function for the iteration 1090----->42.924861990483926 :)\n",
            "The loss function for the iteration 1100----->42.92387499701916 :)\n",
            "The loss function for the iteration 1110----->42.92290153474646 :)\n",
            "The loss function for the iteration 1120----->42.921941319557845 :)\n",
            "The loss function for the iteration 1130----->42.92099408726309 :)\n",
            "The loss function for the iteration 1140----->42.92005963884572 :)\n",
            "The loss function for the iteration 1150----->42.919137666273684 :)\n",
            "The loss function for the iteration 1160----->42.9182279759661 :)\n",
            "The loss function for the iteration 1170----->42.91733032535359 :)\n",
            "The loss function for the iteration 1180----->42.91644447967069 :)\n",
            "The loss function for the iteration 1190----->42.91557022282804 :)\n",
            "The loss function for the iteration 1200----->42.91470733200854 :)\n",
            "The loss function for the iteration 1210----->42.91385558050195 :)\n",
            "The loss function for the iteration 1220----->42.91301480085391 :)\n",
            "The loss function for the iteration 1230----->42.912184771041076 :)\n",
            "The loss function for the iteration 1240----->42.91136529384765 :)\n",
            "The loss function for the iteration 1250----->42.91055617490148 :)\n",
            "The loss function for the iteration 1260----->42.90975721369234 :)\n",
            "The loss function for the iteration 1270----->42.90896824392643 :)\n",
            "The loss function for the iteration 1280----->42.908189085526814 :)\n",
            "The loss function for the iteration 1290----->42.90741957232319 :)\n",
            "The loss function for the iteration 1300----->42.90665949069217 :)\n",
            "The loss function for the iteration 1310----->42.905908706014735 :)\n",
            "The loss function for the iteration 1320----->42.90516702688549 :)\n",
            "The loss function for the iteration 1330----->42.90443429893239 :)\n",
            "The loss function for the iteration 1340----->42.90371035343513 :)\n",
            "The loss function for the iteration 1350----->42.90299505572363 :)\n",
            "The loss function for the iteration 1360----->42.90228822726519 :)\n",
            "The loss function for the iteration 1370----->42.901589756438995 :)\n",
            "The loss function for the iteration 1380----->42.90089946186654 :)\n",
            "The loss function for the iteration 1390----->42.900217201287845 :)\n",
            "The loss function for the iteration 1400----->42.899542851376914 :)\n",
            "The loss function for the iteration 1410----->42.89887623954037 :)\n",
            "The loss function for the iteration 1420----->42.8982172809036 :)\n",
            "The loss function for the iteration 1430----->42.89756579334629 :)\n",
            "The loss function for the iteration 1440----->42.896921650360255 :)\n",
            "The loss function for the iteration 1450----->42.89628475683433 :)\n",
            "The loss function for the iteration 1460----->42.895654935965084 :)\n",
            "The loss function for the iteration 1470----->42.89503211721418 :)\n",
            "The loss function for the iteration 1480----->42.89441614694749 :)\n",
            "The loss function for the iteration 1490----->42.89380690238313 :)\n",
            "The loss function for the iteration 1500----->42.893204295537736 :)\n",
            "The loss function for the iteration 1510----->42.89260820316091 :)\n",
            "The loss function for the iteration 1520----->42.89201847939122 :)\n",
            "The loss function for the iteration 1530----->42.89143506148143 :)\n",
            "The loss function for the iteration 1540----->42.89085779299558 :)\n",
            "The loss function for the iteration 1550----->42.89028660108132 :)\n",
            "The loss function for the iteration 1560----->42.889721390124606 :)\n",
            "The loss function for the iteration 1570----->42.889162004419745 :)\n",
            "The loss function for the iteration 1580----->42.8886083921846 :)\n",
            "The loss function for the iteration 1590----->42.88806042190469 :)\n",
            "The loss function for the iteration 1600----->42.887518021047576 :)\n",
            "The loss function for the iteration 1610----->42.88698109780898 :)\n",
            "The loss function for the iteration 1620----->42.88644953063544 :)\n",
            "The loss function for the iteration 1630----->42.88592323612884 :)\n",
            "The loss function for the iteration 1640----->42.88540213522095 :)\n",
            "The loss function for the iteration 1650----->42.884886114337625 :)\n",
            "The loss function for the iteration 1660----->42.88437512891455 :)\n",
            "The loss function for the iteration 1670----->42.88386907583747 :)\n",
            "The loss function for the iteration 1680----->42.88336783425408 :)\n",
            "The loss function for the iteration 1690----->42.8828713831968 :)\n",
            "The loss function for the iteration 1700----->42.882379577315426 :)\n",
            "The loss function for the iteration 1710----->42.8818923838101 :)\n",
            "The loss function for the iteration 1720----->42.881409714449646 :)\n",
            "The loss function for the iteration 1730----->42.880931455548065 :)\n",
            "The loss function for the iteration 1740----->42.88045761032011 :)\n",
            "The loss function for the iteration 1750----->42.879988021174896 :)\n",
            "The loss function for the iteration 1760----->42.87952265998815 :)\n",
            "The loss function for the iteration 1770----->42.8790614591608 :)\n",
            "The loss function for the iteration 1780----->42.87860429644242 :)\n",
            "The loss function for the iteration 1790----->42.878151180809255 :)\n",
            "The loss function for the iteration 1800----->42.87770198848705 :)\n",
            "The loss function for the iteration 1810----->42.87725665951975 :)\n",
            "The loss function for the iteration 1820----->42.87681513976111 :)\n",
            "The loss function for the iteration 1830----->42.87637734982955 :)\n",
            "The loss function for the iteration 1840----->42.87594324976519 :)\n",
            "The loss function for the iteration 1850----->42.87551277797711 :)\n",
            "The loss function for the iteration 1860----->42.87508586683803 :)\n",
            "The loss function for the iteration 1870----->42.874662434190384 :)\n",
            "The loss function for the iteration 1880----->42.87424244935691 :)\n",
            "The loss function for the iteration 1890----->42.87382583020093 :)\n",
            "The loss function for the iteration 1900----->42.87341253115332 :)\n",
            "The loss function for the iteration 1910----->42.873002508674915 :)\n",
            "The loss function for the iteration 1920----->42.87259566550463 :)\n",
            "The loss function for the iteration 1930----->42.87219200911295 :)\n",
            "The loss function for the iteration 1940----->42.8717914590274 :)\n",
            "The loss function for the iteration 1950----->42.871393958302605 :)\n",
            "The loss function for the iteration 1960----->42.87099947253853 :)\n",
            "The loss function for the iteration 1970----->42.87060789874399 :)\n",
            "The loss function for the iteration 1980----->42.87021927126947 :)\n",
            "The loss function for the iteration 1990----->42.869833478965965 :)\n",
            "The loss function for the iteration 2000----->42.869450482841486 :)\n",
            "The loss function for the iteration 2010----->42.86907026583343 :)\n",
            "The loss function for the iteration 2020----->42.868692710989095 :)\n",
            "The loss function for the iteration 2030----->42.86831786532961 :)\n",
            "The loss function for the iteration 2040----->42.867945660713545 :)\n",
            "The loss function for the iteration 2050----->42.86757600561519 :)\n",
            "The loss function for the iteration 2060----->42.86720890937516 :)\n",
            "The loss function for the iteration 2070----->42.866844283799054 :)\n",
            "The loss function for the iteration 2080----->42.86648213278422 :)\n",
            "The loss function for the iteration 2090----->42.86612240711187 :)\n",
            "The loss function for the iteration 2100----->42.86576500270471 :)\n",
            "The loss function for the iteration 2110----->42.865409977967076 :)\n",
            "The loss function for the iteration 2120----->42.86505725036785 :)\n",
            "The loss function for the iteration 2130----->42.86470676549126 :)\n",
            "The loss function for the iteration 2140----->42.86435850045332 :)\n",
            "The loss function for the iteration 2150----->42.86401240056333 :)\n",
            "The loss function for the iteration 2160----->42.863668470283976 :)\n",
            "The loss function for the iteration 2170----->42.86332666164177 :)\n",
            "The loss function for the iteration 2180----->42.86298690661244 :)\n",
            "The loss function for the iteration 2190----->42.86264921700652 :)\n",
            "The loss function for the iteration 2200----->42.86231353623692 :)\n",
            "The loss function for the iteration 2210----->42.86197981785449 :)\n",
            "The loss function for the iteration 2220----->42.861648052342915 :)\n",
            "The loss function for the iteration 2230----->42.861318200403225 :)\n",
            "The loss function for the iteration 2240----->42.860990235479065 :)\n",
            "The loss function for the iteration 2250----->42.86066412401943 :)\n",
            "The loss function for the iteration 2260----->42.86033980394255 :)\n",
            "The loss function for the iteration 2270----->42.86001731308706 :)\n",
            "The loss function for the iteration 2280----->42.859696583075355 :)\n",
            "The loss function for the iteration 2290----->42.85937754903832 :)\n",
            "The loss function for the iteration 2300----->42.85906025002167 :)\n",
            "The loss function for the iteration 2310----->42.858744625651624 :)\n",
            "The loss function for the iteration 2320----->42.8584306631716 :)\n",
            "The loss function for the iteration 2330----->42.858118331246025 :)\n",
            "The loss function for the iteration 2340----->42.857807549676416 :)\n",
            "The loss function for the iteration 2350----->42.85749838784771 :)\n",
            "The loss function for the iteration 2360----->42.8571907542045 :)\n",
            "The loss function for the iteration 2370----->42.85688463091004 :)\n",
            "The loss function for the iteration 2380----->42.856580056307415 :)\n",
            "The loss function for the iteration 2390----->42.85627691912227 :)\n",
            "The loss function for the iteration 2400----->42.85597525101377 :)\n",
            "The loss function for the iteration 2410----->42.855674999134145 :)\n",
            "The loss function for the iteration 2420----->42.85537615748427 :)\n",
            "The loss function for the iteration 2430----->42.85507873770115 :)\n",
            "The loss function for the iteration 2440----->42.85478262163782 :)\n",
            "The loss function for the iteration 2450----->42.85448786723128 :)\n",
            "The loss function for the iteration 2460----->42.85419444773108 :)\n",
            "The loss function for the iteration 2470----->42.85390230894305 :)\n",
            "The loss function for the iteration 2480----->42.85361144739332 :)\n",
            "The loss function for the iteration 2490----->42.853321835323385 :)\n",
            "The loss function for the iteration 2500----->42.853033489654415 :)\n",
            "The loss function for the iteration 2510----->42.85274635139401 :)\n",
            "The loss function for the iteration 2520----->42.852460411028574 :)\n",
            "The loss function for the iteration 2530----->42.85217567598429 :)\n",
            "The loss function for the iteration 2540----->42.85189206576061 :)\n",
            "The loss function for the iteration 2550----->42.851609610893014 :)\n",
            "The loss function for the iteration 2560----->42.851328308913146 :)\n",
            "The loss function for the iteration 2570----->42.85104809169174 :)\n",
            "The loss function for the iteration 2580----->42.85076895757908 :)\n",
            "The loss function for the iteration 2590----->42.850490899328214 :)\n",
            "The loss function for the iteration 2600----->42.85021390699502 :)\n",
            "The loss function for the iteration 2610----->42.84993795597636 :)\n",
            "The loss function for the iteration 2620----->42.84966303567313 :)\n",
            "The loss function for the iteration 2630----->42.84938914334814 :)\n",
            "The loss function for the iteration 2640----->42.84911623518411 :)\n",
            "The loss function for the iteration 2650----->42.84884432285924 :)\n",
            "The loss function for the iteration 2660----->42.84857335790563 :)\n",
            "The loss function for the iteration 2670----->42.84830336644836 :)\n",
            "The loss function for the iteration 2680----->42.84803426651864 :)\n",
            "The loss function for the iteration 2690----->42.84776611508904 :)\n",
            "The loss function for the iteration 2700----->42.84749886481959 :)\n",
            "The loss function for the iteration 2710----->42.847232520289815 :)\n",
            "The loss function for the iteration 2720----->42.84696705489303 :)\n",
            "The loss function for the iteration 2730----->42.846702461476646 :)\n",
            "The loss function for the iteration 2740----->42.84643872009884 :)\n",
            "The loss function for the iteration 2750----->42.846175829684974 :)\n",
            "The loss function for the iteration 2760----->42.84591376453932 :)\n",
            "The loss function for the iteration 2770----->42.845652526555334 :)\n",
            "The loss function for the iteration 2780----->42.845392055947585 :)\n",
            "The loss function for the iteration 2790----->42.84513241639395 :)\n",
            "The loss function for the iteration 2800----->42.84487354506903 :)\n",
            "The loss function for the iteration 2810----->42.84461544775432 :)\n",
            "The loss function for the iteration 2820----->42.84435813404625 :)\n",
            "The loss function for the iteration 2830----->42.84410154434356 :)\n",
            "The loss function for the iteration 2840----->42.843845712512646 :)\n",
            "The loss function for the iteration 2850----->42.843590593325104 :)\n",
            "The loss function for the iteration 2860----->42.843336195209524 :)\n",
            "The loss function for the iteration 2870----->42.84308249607157 :)\n",
            "The loss function for the iteration 2880----->42.84282948160506 :)\n",
            "The loss function for the iteration 2890----->42.842577174741386 :)\n",
            "The loss function for the iteration 2900----->42.842325544821094 :)\n",
            "The loss function for the iteration 2910----->42.842074579534916 :)\n",
            "The loss function for the iteration 2920----->42.8418242709617 :)\n",
            "The loss function for the iteration 2930----->42.84157461141657 :)\n",
            "The loss function for the iteration 2940----->42.84132557467576 :)\n",
            "The loss function for the iteration 2950----->42.841077200504415 :)\n",
            "The loss function for the iteration 2960----->42.84082943104506 :)\n",
            "The loss function for the iteration 2970----->42.84058225315439 :)\n",
            "The loss function for the iteration 2980----->42.84033571482024 :)\n",
            "The loss function for the iteration 2990----->42.84008974737639 :)\n",
            "The loss function for the iteration 3000----->42.8398443853651 :)\n",
            "The loss function for the iteration 3010----->42.83959960691221 :)\n",
            "The loss function for the iteration 3020----->42.83935537013117 :)\n",
            "The loss function for the iteration 3030----->42.83911172979646 :)\n",
            "The loss function for the iteration 3040----->42.83886861136199 :)\n",
            "The loss function for the iteration 3050----->42.838626055330174 :)\n",
            "The loss function for the iteration 3060----->42.83838406237617 :)\n",
            "The loss function for the iteration 3070----->42.838142568154105 :)\n",
            "The loss function for the iteration 3080----->42.837901632591205 :)\n",
            "The loss function for the iteration 3090----->42.83766122404242 :)\n",
            "The loss function for the iteration 3100----->42.83742132091389 :)\n",
            "The loss function for the iteration 3110----->42.83718192138594 :)\n",
            "The loss function for the iteration 3120----->42.83694300388627 :)\n",
            "The loss function for the iteration 3130----->42.83670458139716 :)\n",
            "The loss function for the iteration 3140----->42.836466676606065 :)\n",
            "The loss function for the iteration 3150----->42.836229232258084 :)\n",
            "The loss function for the iteration 3160----->42.835992259344216 :)\n",
            "The loss function for the iteration 3170----->42.83575575629056 :)\n",
            "The loss function for the iteration 3180----->42.83551968864749 :)\n",
            "The loss function for the iteration 3190----->42.83528411684001 :)\n",
            "The loss function for the iteration 3200----->42.83504896184056 :)\n",
            "The loss function for the iteration 3210----->42.83481425913837 :)\n",
            "The loss function for the iteration 3220----->42.83458001908366 :)\n",
            "The loss function for the iteration 3230----->42.83434617625722 :)\n",
            "The loss function for the iteration 3240----->42.83411280924483 :)\n",
            "The loss function for the iteration 3250----->42.83387984753114 :)\n",
            "The loss function for the iteration 3260----->42.833647281521216 :)\n",
            "The loss function for the iteration 3270----->42.83341515946859 :)\n",
            "The loss function for the iteration 3280----->42.83318342024032 :)\n",
            "The loss function for the iteration 3290----->42.832952105189115 :)\n",
            "The loss function for the iteration 3300----->42.832721185178855 :)\n",
            "The loss function for the iteration 3310----->42.83249063206987 :)\n",
            "The loss function for the iteration 3320----->42.83226047312653 :)\n",
            "The loss function for the iteration 3330----->42.83203069970161 :)\n",
            "The loss function for the iteration 3340----->42.83180130912085 :)\n",
            "The loss function for the iteration 3350----->42.83157231363504 :)\n",
            "The loss function for the iteration 3360----->42.831343679027434 :)\n",
            "The loss function for the iteration 3370----->42.83111540263388 :)\n",
            "The loss function for the iteration 3380----->42.83088750894031 :)\n",
            "The loss function for the iteration 3390----->42.83065994681264 :)\n",
            "The loss function for the iteration 3400----->42.83043275252305 :)\n",
            "The loss function for the iteration 3410----->42.83020589709266 :)\n",
            "The loss function for the iteration 3420----->42.829979377430675 :)\n",
            "The loss function for the iteration 3430----->42.82975322814746 :)\n",
            "The loss function for the iteration 3440----->42.82952739008743 :)\n",
            "The loss function for the iteration 3450----->42.829301894277286 :)\n",
            "The loss function for the iteration 3460----->42.82907671995431 :)\n",
            "The loss function for the iteration 3470----->42.828851876008926 :)\n",
            "The loss function for the iteration 3480----->42.828627370407744 :)\n",
            "The loss function for the iteration 3490----->42.82840317075593 :)\n",
            "The loss function for the iteration 3500----->42.82817930202966 :)\n",
            "The loss function for the iteration 3510----->42.82795572366177 :)\n",
            "The loss function for the iteration 3520----->42.82773245735268 :)\n",
            "The loss function for the iteration 3530----->42.82750950112652 :)\n",
            "The loss function for the iteration 3540----->42.8272868509319 :)\n",
            "The loss function for the iteration 3550----->42.82706450953818 :)\n",
            "The loss function for the iteration 3560----->42.826842446174204 :)\n",
            "The loss function for the iteration 3570----->42.82662068578415 :)\n",
            "The loss function for the iteration 3580----->42.82639921320116 :)\n",
            "The loss function for the iteration 3590----->42.82617803005062 :)\n",
            "The loss function for the iteration 3600----->42.825957131467696 :)\n",
            "The loss function for the iteration 3610----->42.825736515874254 :)\n",
            "The loss function for the iteration 3620----->42.82551617142811 :)\n",
            "The loss function for the iteration 3630----->42.825296112119254 :)\n",
            "The loss function for the iteration 3640----->42.825076315961006 :)\n",
            "The loss function for the iteration 3650----->42.824856777265644 :)\n",
            "The loss function for the iteration 3660----->42.8246375247849 :)\n",
            "The loss function for the iteration 3670----->42.82441851921773 :)\n",
            "The loss function for the iteration 3680----->42.82419980987694 :)\n",
            "The loss function for the iteration 3690----->42.82398135570688 :)\n",
            "The loss function for the iteration 3700----->42.82376314448504 :)\n",
            "The loss function for the iteration 3710----->42.82354520441954 :)\n",
            "The loss function for the iteration 3720----->42.823327481633605 :)\n",
            "The loss function for the iteration 3730----->42.82311004672685 :)\n",
            "The loss function for the iteration 3740----->42.82289285760546 :)\n",
            "The loss function for the iteration 3750----->42.82267590047405 :)\n",
            "The loss function for the iteration 3760----->42.82245918839532 :)\n",
            "The loss function for the iteration 3770----->42.8222426965437 :)\n",
            "The loss function for the iteration 3780----->42.82202646343674 :)\n",
            "The loss function for the iteration 3790----->42.82181047206043 :)\n",
            "The loss function for the iteration 3800----->42.82159471515518 :)\n",
            "The loss function for the iteration 3810----->42.82137919005219 :)\n",
            "The loss function for the iteration 3820----->42.82116390922042 :)\n",
            "The loss function for the iteration 3830----->42.8209488348154 :)\n",
            "The loss function for the iteration 3840----->42.820734001048855 :)\n",
            "The loss function for the iteration 3850----->42.820519379650975 :)\n",
            "The loss function for the iteration 3860----->42.820304989374485 :)\n",
            "The loss function for the iteration 3870----->42.82009083721259 :)\n",
            "The loss function for the iteration 3880----->42.819876876617556 :)\n",
            "The loss function for the iteration 3890----->42.81966316056996 :)\n",
            "The loss function for the iteration 3900----->42.819449631710945 :)\n",
            "The loss function for the iteration 3910----->42.81923632122979 :)\n",
            "The loss function for the iteration 3920----->42.819023239216484 :)\n",
            "The loss function for the iteration 3930----->42.81881035268131 :)\n",
            "The loss function for the iteration 3940----->42.81859770508269 :)\n",
            "The loss function for the iteration 3950----->42.81838524045171 :)\n",
            "The loss function for the iteration 3960----->42.81817297822673 :)\n",
            "The loss function for the iteration 3970----->42.81796093066906 :)\n",
            "The loss function for the iteration 3980----->42.81774908255937 :)\n",
            "The loss function for the iteration 3990----->42.817537452262414 :)\n",
            "The loss function for the iteration 4000----->42.817326025229036 :)\n",
            "The loss function for the iteration 4010----->42.81711476727892 :)\n",
            "The loss function for the iteration 4020----->42.81690372253145 :)\n",
            "The loss function for the iteration 4030----->42.81669287550446 :)\n",
            "The loss function for the iteration 4040----->42.81648220885135 :)\n",
            "The loss function for the iteration 4050----->42.81627176123455 :)\n",
            "The loss function for the iteration 4060----->42.81606146973464 :)\n",
            "The loss function for the iteration 4070----->42.81585138806082 :)\n",
            "The loss function for the iteration 4080----->42.81564149986733 :)\n",
            "The loss function for the iteration 4090----->42.81543178654131 :)\n",
            "The loss function for the iteration 4100----->42.81522227761573 :)\n",
            "The loss function for the iteration 4110----->42.8150129285586 :)\n",
            "The loss function for the iteration 4120----->42.81480378339946 :)\n",
            "The loss function for the iteration 4130----->42.81459481017717 :)\n",
            "The loss function for the iteration 4140----->42.81438603574466 :)\n",
            "The loss function for the iteration 4150----->42.81417743682506 :)\n",
            "The loss function for the iteration 4160----->42.81396900063421 :)\n",
            "The loss function for the iteration 4170----->42.81376075740214 :)\n",
            "The loss function for the iteration 4180----->42.813552678390174 :)\n",
            "The loss function for the iteration 4190----->42.8133447805499 :)\n",
            "The loss function for the iteration 4200----->42.81313705890484 :)\n",
            "The loss function for the iteration 4210----->42.81292950136124 :)\n",
            "The loss function for the iteration 4220----->42.812722117503384 :)\n",
            "The loss function for the iteration 4230----->42.812514922580164 :)\n",
            "The loss function for the iteration 4240----->42.81230787155701 :)\n",
            "The loss function for the iteration 4250----->42.81210102151066 :)\n",
            "The loss function for the iteration 4260----->42.811894327543975 :)\n",
            "The loss function for the iteration 4270----->42.81168779265473 :)\n",
            "The loss function for the iteration 4280----->42.81148145178527 :)\n",
            "The loss function for the iteration 4290----->42.81127523227905 :)\n",
            "The loss function for the iteration 4300----->42.81106919665906 :)\n",
            "The loss function for the iteration 4310----->42.81086332446757 :)\n",
            "The loss function for the iteration 4320----->42.81065762122777 :)\n",
            "The loss function for the iteration 4330----->42.810452094348065 :)\n",
            "The loss function for the iteration 4340----->42.81024673847832 :)\n",
            "The loss function for the iteration 4350----->42.810041506900426 :)\n",
            "The loss function for the iteration 4360----->42.80983643643638 :)\n",
            "The loss function for the iteration 4370----->42.80963152977425 :)\n",
            "The loss function for the iteration 4380----->42.80942675204638 :)\n",
            "The loss function for the iteration 4390----->42.809222183931325 :)\n",
            "The loss function for the iteration 4400----->42.80901773737768 :)\n",
            "The loss function for the iteration 4410----->42.808813442898355 :)\n",
            "The loss function for the iteration 4420----->42.80860932668445 :)\n",
            "The loss function for the iteration 4430----->42.808405354373654 :)\n",
            "The loss function for the iteration 4440----->42.80820153733395 :)\n",
            "The loss function for the iteration 4450----->42.80799787879604 :)\n",
            "The loss function for the iteration 4460----->42.807794344858365 :)\n",
            "The loss function for the iteration 4470----->42.80759097013544 :)\n",
            "The loss function for the iteration 4480----->42.80738777642007 :)\n",
            "The loss function for the iteration 4490----->42.80718469051061 :)\n",
            "The loss function for the iteration 4500----->42.80698178425505 :)\n",
            "The loss function for the iteration 4510----->42.80677901171194 :)\n",
            "The loss function for the iteration 4520----->42.8065763648968 :)\n",
            "The loss function for the iteration 4530----->42.80637389988855 :)\n",
            "The loss function for the iteration 4540----->42.80617156521562 :)\n",
            "The loss function for the iteration 4550----->42.80596936305011 :)\n",
            "The loss function for the iteration 4560----->42.80576733011052 :)\n",
            "The loss function for the iteration 4570----->42.805565414888235 :)\n",
            "The loss function for the iteration 4580----->42.80536366531986 :)\n",
            "The loss function for the iteration 4590----->42.80516208740748 :)\n",
            "The loss function for the iteration 4600----->42.80496059785304 :)\n",
            "The loss function for the iteration 4610----->42.80475928258315 :)\n",
            "The loss function for the iteration 4620----->42.804558070938704 :)\n",
            "The loss function for the iteration 4630----->42.80435701448316 :)\n",
            "The loss function for the iteration 4640----->42.80415613229707 :)\n",
            "The loss function for the iteration 4650----->42.803955350432716 :)\n",
            "The loss function for the iteration 4660----->42.803754740816714 :)\n",
            "The loss function for the iteration 4670----->42.80355424400151 :)\n",
            "The loss function for the iteration 4680----->42.80335389777922 :)\n",
            "The loss function for the iteration 4690----->42.803153703196436 :)\n",
            "The loss function for the iteration 4700----->42.80295363336992 :)\n",
            "The loss function for the iteration 4710----->42.8027536927475 :)\n",
            "The loss function for the iteration 4720----->42.80255389024193 :)\n",
            "The loss function for the iteration 4730----->42.802354200931546 :)\n",
            "The loss function for the iteration 4740----->42.802154676871375 :)\n",
            "The loss function for the iteration 4750----->42.801955291190595 :)\n",
            "The loss function for the iteration 4760----->42.80175600966225 :)\n",
            "The loss function for the iteration 4770----->42.80155690253085 :)\n",
            "The loss function for the iteration 4780----->42.801357885095875 :)\n",
            "The loss function for the iteration 4790----->42.801159020302826 :)\n",
            "The loss function for the iteration 4800----->42.800960297215255 :)\n",
            "The loss function for the iteration 4810----->42.800761674033005 :)\n",
            "The loss function for the iteration 4820----->42.800563203395555 :)\n",
            "The loss function for the iteration 4830----->42.80036486860849 :)\n",
            "The loss function for the iteration 4840----->42.80016665471562 :)\n",
            "The loss function for the iteration 4850----->42.79996860282787 :)\n",
            "The loss function for the iteration 4860----->42.799770660896975 :)\n",
            "The loss function for the iteration 4870----->42.7995728359759 :)\n",
            "The loss function for the iteration 4880----->42.79937514834041 :)\n",
            "The loss function for the iteration 4890----->42.799177577992246 :)\n",
            "The loss function for the iteration 4900----->42.79898014388855 :)\n",
            "The loss function for the iteration 4910----->42.798782841552786 :)\n",
            "The loss function for the iteration 4920----->42.79858567012244 :)\n",
            "The loss function for the iteration 4930----->42.798388611807546 :)\n",
            "The loss function for the iteration 4940----->42.7981917055428 :)\n",
            "The loss function for the iteration 4950----->42.79799489567858 :)\n",
            "The loss function for the iteration 4960----->42.7977982133244 :)\n",
            "The loss function for the iteration 4970----->42.79760166180874 :)\n",
            "The loss function for the iteration 4980----->42.797405222792776 :)\n",
            "The loss function for the iteration 4990----->42.797208927325094 :)\n",
            "The loss function for the iteration 5000----->42.79701276444967 :)\n",
            "The loss function for the iteration 5010----->42.79681670340618 :)\n",
            "The loss function for the iteration 5020----->42.79662076753839 :)\n",
            "The loss function for the iteration 5030----->42.79642496738396 :)\n",
            "The loss function for the iteration 5040----->42.79622926658633 :)\n",
            "The loss function for the iteration 5050----->42.79603373335762 :)\n",
            "The loss function for the iteration 5060----->42.79583828438124 :)\n",
            "The loss function for the iteration 5070----->42.7956429674469 :)\n",
            "The loss function for the iteration 5080----->42.79544777077204 :)\n",
            "The loss function for the iteration 5090----->42.79525270924762 :)\n",
            "The loss function for the iteration 5100----->42.79505778234547 :)\n",
            "The loss function for the iteration 5110----->42.79486296234588 :)\n",
            "The loss function for the iteration 5120----->42.79466826920562 :)\n",
            "The loss function for the iteration 5130----->42.794473657042225 :)\n",
            "The loss function for the iteration 5140----->42.794279195317486 :)\n",
            "The loss function for the iteration 5150----->42.79408484228451 :)\n",
            "The loss function for the iteration 5160----->42.79389062775507 :)\n",
            "The loss function for the iteration 5170----->42.79369652468266 :)\n",
            "The loss function for the iteration 5180----->42.79350254669529 :)\n",
            "The loss function for the iteration 5190----->42.79330866946804 :)\n",
            "The loss function for the iteration 5200----->42.79311492950955 :)\n",
            "The loss function for the iteration 5210----->42.792921300988915 :)\n",
            "The loss function for the iteration 5220----->42.792727774989 :)\n",
            "The loss function for the iteration 5230----->42.79253437528997 :)\n",
            "The loss function for the iteration 5240----->42.792341075789246 :)\n",
            "The loss function for the iteration 5250----->42.79214791193289 :)\n",
            "The loss function for the iteration 5260----->42.79195485298398 :)\n",
            "The loss function for the iteration 5270----->42.79176192437672 :)\n",
            "The loss function for the iteration 5280----->42.79156908131429 :)\n",
            "The loss function for the iteration 5290----->42.79137639352102 :)\n",
            "The loss function for the iteration 5300----->42.791183802976974 :)\n",
            "The loss function for the iteration 5310----->42.79099134357805 :)\n",
            "The loss function for the iteration 5320----->42.79079900525348 :)\n",
            "The loss function for the iteration 5330----->42.79060675690499 :)\n",
            "The loss function for the iteration 5340----->42.790414636209974 :)\n",
            "The loss function for the iteration 5350----->42.790222630083406 :)\n",
            "The loss function for the iteration 5360----->42.7900307417145 :)\n",
            "The loss function for the iteration 5370----->42.789838965530144 :)\n",
            "The loss function for the iteration 5380----->42.78964730272804 :)\n",
            "The loss function for the iteration 5390----->42.78945574400812 :)\n",
            "The loss function for the iteration 5400----->42.7892643162041 :)\n",
            "The loss function for the iteration 5410----->42.78907298653073 :)\n",
            "The loss function for the iteration 5420----->42.78888179485255 :)\n",
            "The loss function for the iteration 5430----->42.78869068123708 :)\n",
            "The loss function for the iteration 5440----->42.78849971829737 :)\n",
            "The loss function for the iteration 5450----->42.788308845652566 :)\n",
            "The loss function for the iteration 5460----->42.788118070247656 :)\n",
            "The loss function for the iteration 5470----->42.78792743803986 :)\n",
            "The loss function for the iteration 5480----->42.787736874349065 :)\n",
            "The loss function for the iteration 5490----->42.78754645488558 :)\n",
            "The loss function for the iteration 5500----->42.78735616397286 :)\n",
            "The loss function for the iteration 5510----->42.78716594578998 :)\n",
            "The loss function for the iteration 5520----->42.786975863173545 :)\n",
            "The loss function for the iteration 5530----->42.786785891287515 :)\n",
            "The loss function for the iteration 5540----->42.78659600194886 :)\n",
            "The loss function for the iteration 5550----->42.78640629005586 :)\n",
            "The loss function for the iteration 5560----->42.78621663597894 :)\n",
            "The loss function for the iteration 5570----->42.78602709813305 :)\n",
            "The loss function for the iteration 5580----->42.785837679353655 :)\n",
            "The loss function for the iteration 5590----->42.78564834560383 :)\n",
            "The loss function for the iteration 5600----->42.78545914253778 :)\n",
            "The loss function for the iteration 5610----->42.78527004785376 :)\n",
            "The loss function for the iteration 5620----->42.78508104863577 :)\n",
            "The loss function for the iteration 5630----->42.78489216947984 :)\n",
            "The loss function for the iteration 5640----->42.78470340194376 :)\n",
            "The loss function for the iteration 5650----->42.78451474651249 :)\n",
            "The loss function for the iteration 5660----->42.78432619353342 :)\n",
            "The loss function for the iteration 5670----->42.784137734195454 :)\n",
            "The loss function for the iteration 5680----->42.78394938312479 :)\n",
            "The loss function for the iteration 5690----->42.78376114152491 :)\n",
            "The loss function for the iteration 5700----->42.783573036381384 :)\n",
            "The loss function for the iteration 5710----->42.783385015101786 :)\n",
            "The loss function for the iteration 5720----->42.78319710487476 :)\n",
            "The loss function for the iteration 5730----->42.783009285151024 :)\n",
            "The loss function for the iteration 5740----->42.78282158530749 :)\n",
            "The loss function for the iteration 5750----->42.7826340043904 :)\n",
            "The loss function for the iteration 5760----->42.78244651604296 :)\n",
            "The loss function for the iteration 5770----->42.782259133860066 :)\n",
            "The loss function for the iteration 5780----->42.782071843293544 :)\n",
            "The loss function for the iteration 5790----->42.78188466598944 :)\n",
            "The loss function for the iteration 5800----->42.78169763311085 :)\n",
            "The loss function for the iteration 5810----->42.781510670538054 :)\n",
            "The loss function for the iteration 5820----->42.781323835122805 :)\n",
            "The loss function for the iteration 5830----->42.78113710018926 :)\n",
            "The loss function for the iteration 5840----->42.78095043832445 :)\n",
            "The loss function for the iteration 5850----->42.780763936545114 :)\n",
            "The loss function for the iteration 5860----->42.78057750103049 :)\n",
            "The loss function for the iteration 5870----->42.78039117980401 :)\n",
            "The loss function for the iteration 5880----->42.7802049844383 :)\n",
            "The loss function for the iteration 5890----->42.78001887140952 :)\n",
            "The loss function for the iteration 5900----->42.77983287898488 :)\n",
            "The loss function for the iteration 5910----->42.77964700368067 :)\n",
            "The loss function for the iteration 5920----->42.7794611839768 :)\n",
            "The loss function for the iteration 5930----->42.77927549126619 :)\n",
            "The loss function for the iteration 5940----->42.77908990552552 :)\n",
            "The loss function for the iteration 5950----->42.7789044128896 :)\n",
            "The loss function for the iteration 5960----->42.77871904705316 :)\n",
            "The loss function for the iteration 5970----->42.77853378844873 :)\n",
            "The loss function for the iteration 5980----->42.778348600782586 :)\n",
            "The loss function for the iteration 5990----->42.77816354645129 :)\n",
            "The loss function for the iteration 6000----->42.7779785583385 :)\n",
            "The loss function for the iteration 6010----->42.777793685841985 :)\n",
            "The loss function for the iteration 6020----->42.777608934792546 :)\n",
            "The loss function for the iteration 6030----->42.77742426585648 :)\n",
            "The loss function for the iteration 6040----->42.77723971130206 :)\n",
            "The loss function for the iteration 6050----->42.77705524728969 :)\n",
            "The loss function for the iteration 6060----->42.77687089491346 :)\n",
            "The loss function for the iteration 6070----->42.77668666881237 :)\n",
            "The loss function for the iteration 6080----->42.776502521030416 :)\n",
            "The loss function for the iteration 6090----->42.77631847722184 :)\n",
            "The loss function for the iteration 6100----->42.77613452090162 :)\n",
            "The loss function for the iteration 6110----->42.77595064575648 :)\n",
            "The loss function for the iteration 6120----->42.775766930823494 :)\n",
            "The loss function for the iteration 6130----->42.77558327906824 :)\n",
            "The loss function for the iteration 6140----->42.77539975983776 :)\n",
            "The loss function for the iteration 6150----->42.77521632185615 :)\n",
            "The loss function for the iteration 6160----->42.77503298708137 :)\n",
            "The loss function for the iteration 6170----->42.77484975872181 :)\n",
            "The loss function for the iteration 6180----->42.77466664154965 :)\n",
            "The loss function for the iteration 6190----->42.77448359113313 :)\n",
            "The loss function for the iteration 6200----->42.77430066954623 :)\n",
            "The loss function for the iteration 6210----->42.774117826050784 :)\n",
            "The loss function for the iteration 6220----->42.773935119096535 :)\n",
            "The loss function for the iteration 6230----->42.773752475377734 :)\n",
            "The loss function for the iteration 6240----->42.77356996324367 :)\n",
            "The loss function for the iteration 6250----->42.77338750186121 :)\n",
            "The loss function for the iteration 6260----->42.773205167363734 :)\n",
            "The loss function for the iteration 6270----->42.77302294491578 :)\n",
            "The loss function for the iteration 6280----->42.77284080804785 :)\n",
            "The loss function for the iteration 6290----->42.77265877669093 :)\n",
            "The loss function for the iteration 6300----->42.772476854271545 :)\n",
            "The loss function for the iteration 6310----->42.772295010623175 :)\n",
            "The loss function for the iteration 6320----->42.77211330207128 :)\n",
            "The loss function for the iteration 6330----->42.77193166618317 :)\n",
            "The loss function for the iteration 6340----->42.77175012950151 :)\n",
            "The loss function for the iteration 6350----->42.77156868812503 :)\n",
            "The loss function for the iteration 6360----->42.771387357242574 :)\n",
            "The loss function for the iteration 6370----->42.77120612051793 :)\n",
            "The loss function for the iteration 6380----->42.77102497946645 :)\n",
            "The loss function for the iteration 6390----->42.770843947447254 :)\n",
            "The loss function for the iteration 6400----->42.770663003867185 :)\n",
            "The loss function for the iteration 6410----->42.770482172022 :)\n",
            "The loss function for the iteration 6420----->42.770301443030554 :)\n",
            "The loss function for the iteration 6430----->42.77012078304895 :)\n",
            "The loss function for the iteration 6440----->42.76994022582091 :)\n",
            "The loss function for the iteration 6450----->42.76975976490993 :)\n",
            "The loss function for the iteration 6460----->42.76957940414254 :)\n",
            "The loss function for the iteration 6470----->42.76939915500677 :)\n",
            "The loss function for the iteration 6480----->42.76921900483625 :)\n",
            "The loss function for the iteration 6490----->42.769038942215445 :)\n",
            "The loss function for the iteration 6500----->42.76885897052139 :)\n",
            "The loss function for the iteration 6510----->42.76867911318548 :)\n",
            "The loss function for the iteration 6520----->42.768499331746746 :)\n",
            "The loss function for the iteration 6530----->42.76831967620534 :)\n",
            "The loss function for the iteration 6540----->42.768140105216794 :)\n",
            "The loss function for the iteration 6550----->42.76796062630701 :)\n",
            "The loss function for the iteration 6560----->42.76778123681591 :)\n",
            "The loss function for the iteration 6570----->42.76760193165234 :)\n",
            "The loss function for the iteration 6580----->42.76742273724059 :)\n",
            "The loss function for the iteration 6590----->42.76724364924107 :)\n",
            "The loss function for the iteration 6600----->42.76706466163305 :)\n",
            "The loss function for the iteration 6610----->42.7668857674502 :)\n",
            "The loss function for the iteration 6620----->42.76670691943861 :)\n",
            "The loss function for the iteration 6630----->42.76652822496242 :)\n",
            "The loss function for the iteration 6640----->42.766349596724204 :)\n",
            "The loss function for the iteration 6650----->42.766171070285374 :)\n",
            "The loss function for the iteration 6660----->42.765992668389906 :)\n",
            "The loss function for the iteration 6670----->42.76581430640327 :)\n",
            "The loss function for the iteration 6680----->42.76563607854772 :)\n",
            "The loss function for the iteration 6690----->42.765457952541915 :)\n",
            "The loss function for the iteration 6700----->42.765279873850346 :)\n",
            "The loss function for the iteration 6710----->42.76510193440045 :)\n",
            "The loss function for the iteration 6720----->42.764924075469466 :)\n",
            "The loss function for the iteration 6730----->42.76474631090952 :)\n",
            "The loss function for the iteration 6740----->42.76456866792 :)\n",
            "The loss function for the iteration 6750----->42.76439108407866 :)\n",
            "The loss function for the iteration 6760----->42.76421360039818 :)\n",
            "The loss function for the iteration 6770----->42.7640362284095 :)\n",
            "The loss function for the iteration 6780----->42.76385895352008 :)\n",
            "The loss function for the iteration 6790----->42.76368177098024 :)\n",
            "The loss function for the iteration 6800----->42.76350468443911 :)\n",
            "The loss function for the iteration 6810----->42.76332767145101 :)\n",
            "The loss function for the iteration 6820----->42.7631507662331 :)\n",
            "The loss function for the iteration 6830----->42.76297395269838 :)\n",
            "The loss function for the iteration 6840----->42.76279723454614 :)\n",
            "The loss function for the iteration 6850----->42.762620629686694 :)\n",
            "The loss function for the iteration 6860----->42.7624441027305 :)\n",
            "The loss function for the iteration 6870----->42.76226767389719 :)\n",
            "The loss function for the iteration 6880----->42.76209132316193 :)\n",
            "The loss function for the iteration 6890----->42.76191504655583 :)\n",
            "The loss function for the iteration 6900----->42.76173889821057 :)\n",
            "The loss function for the iteration 6910----->42.761562843959936 :)\n",
            "The loss function for the iteration 6920----->42.761386873915754 :)\n",
            "The loss function for the iteration 6930----->42.761211021758164 :)\n",
            "The loss function for the iteration 6940----->42.761035224453494 :)\n",
            "The loss function for the iteration 6950----->42.76085953839775 :)\n",
            "The loss function for the iteration 6960----->42.760683939559684 :)\n",
            "The loss function for the iteration 6970----->42.76050842991019 :)\n",
            "The loss function for the iteration 6980----->42.76033299409218 :)\n",
            "The loss function for the iteration 6990----->42.76015767782218 :)\n",
            "The loss function for the iteration 7000----->42.75998244311034 :)\n",
            "The loss function for the iteration 7010----->42.759807298481796 :)\n",
            "The loss function for the iteration 7020----->42.75963226163397 :)\n",
            "The loss function for the iteration 7030----->42.75945728166729 :)\n",
            "The loss function for the iteration 7040----->42.75928241853319 :)\n",
            "The loss function for the iteration 7050----->42.75910766797023 :)\n",
            "The loss function for the iteration 7060----->42.758932967319254 :)\n",
            "The loss function for the iteration 7070----->42.75875838094771 :)\n",
            "The loss function for the iteration 7080----->42.75858385702506 :)\n",
            "The loss function for the iteration 7090----->42.758409430861086 :)\n",
            "The loss function for the iteration 7100----->42.75823512955362 :)\n",
            "The loss function for the iteration 7110----->42.758060888904765 :)\n",
            "The loss function for the iteration 7120----->42.757886770333215 :)\n",
            "The loss function for the iteration 7130----->42.757712710926434 :)\n",
            "The loss function for the iteration 7140----->42.75753875097248 :)\n",
            "The loss function for the iteration 7150----->42.757364889670015 :)\n",
            "The loss function for the iteration 7160----->42.75719111254724 :)\n",
            "The loss function for the iteration 7170----->42.75701743932507 :)\n",
            "The loss function for the iteration 7180----->42.756843880543684 :)\n",
            "The loss function for the iteration 7190----->42.75667037402439 :)\n",
            "The loss function for the iteration 7200----->42.75649697413515 :)\n",
            "The loss function for the iteration 7210----->42.75632365393196 :)\n",
            "The loss function for the iteration 7220----->42.75615040928119 :)\n",
            "The loss function for the iteration 7230----->42.75597727602163 :)\n",
            "The loss function for the iteration 7240----->42.75580422576869 :)\n",
            "The loss function for the iteration 7250----->42.75563124589042 :)\n",
            "The loss function for the iteration 7260----->42.755458402257005 :)\n",
            "The loss function for the iteration 7270----->42.75528562744776 :)\n",
            "The loss function for the iteration 7280----->42.75511293746486 :)\n",
            "The loss function for the iteration 7290----->42.75494034763862 :)\n",
            "The loss function for the iteration 7300----->42.75476780937617 :)\n",
            "The loss function for the iteration 7310----->42.75459539596244 :)\n",
            "The loss function for the iteration 7320----->42.754423058161684 :)\n",
            "The loss function for the iteration 7330----->42.75425082313568 :)\n",
            "The loss function for the iteration 7340----->42.754078659420074 :)\n",
            "The loss function for the iteration 7350----->42.753906591148585 :)\n",
            "The loss function for the iteration 7360----->42.75373463922894 :)\n",
            "The loss function for the iteration 7370----->42.75356275917159 :)\n",
            "The loss function for the iteration 7380----->42.753390972610525 :)\n",
            "The loss function for the iteration 7390----->42.753219265277714 :)\n",
            "The loss function for the iteration 7400----->42.75304762941611 :)\n",
            "The loss function for the iteration 7410----->42.75287611659616 :)\n",
            "The loss function for the iteration 7420----->42.75270468083516 :)\n",
            "The loss function for the iteration 7430----->42.75253333086144 :)\n",
            "The loss function for the iteration 7440----->42.75236207526954 :)\n",
            "The loss function for the iteration 7450----->42.75219088662669 :)\n",
            "The loss function for the iteration 7460----->42.75201979403609 :)\n",
            "The loss function for the iteration 7470----->42.751848780093326 :)\n",
            "The loss function for the iteration 7480----->42.75167787325652 :)\n",
            "The loss function for the iteration 7490----->42.751507060164435 :)\n",
            "The loss function for the iteration 7500----->42.75133632912495 :)\n",
            "The loss function for the iteration 7510----->42.75116569602652 :)\n",
            "The loss function for the iteration 7520----->42.75099512410202 :)\n",
            "The loss function for the iteration 7530----->42.75082465674029 :)\n",
            "The loss function for the iteration 7540----->42.75065426789618 :)\n",
            "The loss function for the iteration 7550----->42.75048398128805 :)\n",
            "The loss function for the iteration 7560----->42.75031374521907 :)\n",
            "The loss function for the iteration 7570----->42.75014363672422 :)\n",
            "The loss function for the iteration 7580----->42.74997359555788 :)\n",
            "The loss function for the iteration 7590----->42.74980365317973 :)\n",
            "The loss function for the iteration 7600----->42.74963378962039 :)\n",
            "The loss function for the iteration 7610----->42.74946400488215 :)\n",
            "The loss function for the iteration 7620----->42.7492943098042 :)\n",
            "The loss function for the iteration 7630----->42.74912473279457 :)\n",
            "The loss function for the iteration 7640----->42.748955217118265 :)\n",
            "The loss function for the iteration 7650----->42.74878579601258 :)\n",
            "The loss function for the iteration 7660----->42.74861644747127 :)\n",
            "The loss function for the iteration 7670----->42.748447208006915 :)\n",
            "The loss function for the iteration 7680----->42.74827804423391 :)\n",
            "The loss function for the iteration 7690----->42.74810898422973 :)\n",
            "The loss function for the iteration 7700----->42.74793999623167 :)\n",
            "The loss function for the iteration 7710----->42.747771081107196 :)\n",
            "The loss function for the iteration 7720----->42.74760227458047 :)\n",
            "The loss function for the iteration 7730----->42.747433534333815 :)\n",
            "The loss function for the iteration 7740----->42.74726487948619 :)\n",
            "The loss function for the iteration 7750----->42.74709633953492 :)\n",
            "The loss function for the iteration 7760----->42.74692785136819 :)\n",
            "The loss function for the iteration 7770----->42.74675948202162 :)\n",
            "The loss function for the iteration 7780----->42.74659115080442 :)\n",
            "The loss function for the iteration 7790----->42.74642294873134 :)\n",
            "The loss function for the iteration 7800----->42.74625482298125 :)\n",
            "The loss function for the iteration 7810----->42.7460867817904 :)\n",
            "The loss function for the iteration 7820----->42.74591883009434 :)\n",
            "The loss function for the iteration 7830----->42.74575093544658 :)\n",
            "The loss function for the iteration 7840----->42.74558312916367 :)\n",
            "The loss function for the iteration 7850----->42.745415426179655 :)\n",
            "The loss function for the iteration 7860----->42.74524778501376 :)\n",
            "The loss function for the iteration 7870----->42.745080233846735 :)\n",
            "The loss function for the iteration 7880----->42.7449127998989 :)\n",
            "The loss function for the iteration 7890----->42.744745409517385 :)\n",
            "The loss function for the iteration 7900----->42.744578136233095 :)\n",
            "The loss function for the iteration 7910----->42.7444109427235 :)\n",
            "The loss function for the iteration 7920----->42.74424378686768 :)\n",
            "The loss function for the iteration 7930----->42.744076762755064 :)\n",
            "The loss function for the iteration 7940----->42.74390981305581 :)\n",
            "The loss function for the iteration 7950----->42.74374293363026 :)\n",
            "The loss function for the iteration 7960----->42.74357618538939 :)\n",
            "The loss function for the iteration 7970----->42.743409479572925 :)\n",
            "The loss function for the iteration 7980----->42.74324288128175 :)\n",
            "The loss function for the iteration 7990----->42.74307636153534 :)\n",
            "The loss function for the iteration 8000----->42.74290990795458 :)\n",
            "The loss function for the iteration 8010----->42.74274355995318 :)\n",
            "The loss function for the iteration 8020----->42.74257729290865 :)\n",
            "The loss function for the iteration 8030----->42.74241109405734 :)\n",
            "The loss function for the iteration 8040----->42.74224500317698 :)\n",
            "The loss function for the iteration 8050----->42.74207894828366 :)\n",
            "The loss function for the iteration 8060----->42.74191304045779 :)\n",
            "The loss function for the iteration 8070----->42.741747188157525 :)\n",
            "The loss function for the iteration 8080----->42.74158141288895 :)\n",
            "The loss function for the iteration 8090----->42.74141571216194 :)\n",
            "The loss function for the iteration 8100----->42.741250098361675 :)\n",
            "The loss function for the iteration 8110----->42.74108456459156 :)\n",
            "The loss function for the iteration 8120----->42.740919146336424 :)\n",
            "The loss function for the iteration 8130----->42.74075380611745 :)\n",
            "The loss function for the iteration 8140----->42.74058851549279 :)\n",
            "The loss function for the iteration 8150----->42.74042335297602 :)\n",
            "The loss function for the iteration 8160----->42.740258240440404 :)\n",
            "The loss function for the iteration 8170----->42.74009320192021 :)\n",
            "The loss function for the iteration 8180----->42.73992826256862 :)\n",
            "The loss function for the iteration 8190----->42.73976339274325 :)\n",
            "The loss function for the iteration 8200----->42.73959861808009 :)\n",
            "The loss function for the iteration 8210----->42.73943392400484 :)\n",
            "The loss function for the iteration 8220----->42.73926931546069 :)\n",
            "The loss function for the iteration 8230----->42.739104787096906 :)\n",
            "The loss function for the iteration 8240----->42.7389403417488 :)\n",
            "The loss function for the iteration 8250----->42.738775984269594 :)\n",
            "The loss function for the iteration 8260----->42.73861167782548 :)\n",
            "The loss function for the iteration 8270----->42.73844749759407 :)\n",
            "The loss function for the iteration 8280----->42.73828336813615 :)\n",
            "The loss function for the iteration 8290----->42.738119349216674 :)\n",
            "The loss function for the iteration 8300----->42.7379553662844 :)\n",
            "The loss function for the iteration 8310----->42.737791487570526 :)\n",
            "The loss function for the iteration 8320----->42.737627700766744 :)\n",
            "The loss function for the iteration 8330----->42.73746397687435 :)\n",
            "The loss function for the iteration 8340----->42.73730036796427 :)\n",
            "The loss function for the iteration 8350----->42.737136803096384 :)\n",
            "The loss function for the iteration 8360----->42.73697331789446 :)\n",
            "The loss function for the iteration 8370----->42.73680994980948 :)\n",
            "The loss function for the iteration 8380----->42.736646635905 :)\n",
            "The loss function for the iteration 8390----->42.73648341786694 :)\n",
            "The loss function for the iteration 8400----->42.73632027993205 :)\n",
            "The loss function for the iteration 8410----->42.736157210285846 :)\n",
            "The loss function for the iteration 8420----->42.735994221680755 :)\n",
            "The loss function for the iteration 8430----->42.73583131727654 :)\n",
            "The loss function for the iteration 8440----->42.73566849284055 :)\n",
            "The loss function for the iteration 8450----->42.73550575501139 :)\n",
            "The loss function for the iteration 8460----->42.73534312390092 :)\n",
            "The loss function for the iteration 8470----->42.735180533352995 :)\n",
            "The loss function for the iteration 8480----->42.73501804586327 :)\n",
            "The loss function for the iteration 8490----->42.73485561618955 :)\n",
            "The loss function for the iteration 8500----->42.73469329764928 :)\n",
            "The loss function for the iteration 8510----->42.73453104096833 :)\n",
            "The loss function for the iteration 8520----->42.73436885503112 :)\n",
            "The loss function for the iteration 8530----->42.73420677187993 :)\n",
            "The loss function for the iteration 8540----->42.734044739033465 :)\n",
            "The loss function for the iteration 8550----->42.73388280735945 :)\n",
            "The loss function for the iteration 8560----->42.733720949801935 :)\n",
            "The loss function for the iteration 8570----->42.73355914804877 :)\n",
            "The loss function for the iteration 8580----->42.73339746982941 :)\n",
            "The loss function for the iteration 8590----->42.733235851692044 :)\n",
            "The loss function for the iteration 8600----->42.73307432810498 :)\n",
            "The loss function for the iteration 8610----->42.73291286890853 :)\n",
            "The loss function for the iteration 8620----->42.73275148245115 :)\n",
            "The loss function for the iteration 8630----->42.732590192907054 :)\n",
            "The loss function for the iteration 8640----->42.73242894736608 :)\n",
            "The loss function for the iteration 8650----->42.73226782035842 :)\n",
            "The loss function for the iteration 8660----->42.73210674678955 :)\n",
            "The loss function for the iteration 8670----->42.73194576626305 :)\n",
            "The loss function for the iteration 8680----->42.73178485818438 :)\n",
            "The loss function for the iteration 8690----->42.73162402609626 :)\n",
            "The loss function for the iteration 8700----->42.731463258485824 :)\n",
            "The loss function for the iteration 8710----->42.731302604836074 :)\n",
            "The loss function for the iteration 8720----->42.73114201940156 :)\n",
            "The loss function for the iteration 8730----->42.73098147813044 :)\n",
            "The loss function for the iteration 8740----->42.730821050794326 :)\n",
            "The loss function for the iteration 8750----->42.730660679666 :)\n",
            "The loss function for the iteration 8760----->42.73050040465256 :)\n",
            "The loss function for the iteration 8770----->42.73034022945259 :)\n",
            "The loss function for the iteration 8780----->42.73018009524885 :)\n",
            "The loss function for the iteration 8790----->42.730020059816326 :)\n",
            "The loss function for the iteration 8800----->42.72986008948176 :)\n",
            "The loss function for the iteration 8810----->42.72970018856411 :)\n",
            "The loss function for the iteration 8820----->42.72954038906505 :)\n",
            "The loss function for the iteration 8830----->42.729380632139254 :)\n",
            "The loss function for the iteration 8840----->42.72922101106313 :)\n",
            "The loss function for the iteration 8850----->42.72906142774614 :)\n",
            "The loss function for the iteration 8860----->42.728901925180274 :)\n",
            "The loss function for the iteration 8870----->42.72874250351321 :)\n",
            "The loss function for the iteration 8880----->42.72858315286884 :)\n",
            "The loss function for the iteration 8890----->42.728423894844596 :)\n",
            "The loss function for the iteration 8900----->42.72826470309238 :)\n",
            "The loss function for the iteration 8910----->42.72810560043562 :)\n",
            "The loss function for the iteration 8920----->42.72794656205543 :)\n",
            "The loss function for the iteration 8930----->42.72778763292904 :)\n",
            "The loss function for the iteration 8940----->42.727628760648905 :)\n",
            "The loss function for the iteration 8950----->42.72746994610435 :)\n",
            "The loss function for the iteration 8960----->42.72731123780508 :)\n",
            "The loss function for the iteration 8970----->42.727152587314414 :)\n",
            "The loss function for the iteration 8980----->42.72699401248752 :)\n",
            "The loss function for the iteration 8990----->42.72683553973933 :)\n",
            "The loss function for the iteration 9000----->42.726677116369196 :)\n",
            "The loss function for the iteration 9010----->42.72651879949654 :)\n",
            "The loss function for the iteration 9020----->42.726360552096104 :)\n",
            "The loss function for the iteration 9030----->42.726202379530186 :)\n",
            "The loss function for the iteration 9040----->42.72604427411278 :)\n",
            "The loss function for the iteration 9050----->42.7258862516807 :)\n",
            "The loss function for the iteration 9060----->42.725728296799765 :)\n",
            "The loss function for the iteration 9070----->42.725570411925055 :)\n",
            "The loss function for the iteration 9080----->42.725412613371326 :)\n",
            "The loss function for the iteration 9090----->42.72525489535486 :)\n",
            "The loss function for the iteration 9100----->42.72509728465323 :)\n",
            "The loss function for the iteration 9110----->42.72493968825859 :)\n",
            "The loss function for the iteration 9120----->42.72478220363026 :)\n",
            "The loss function for the iteration 9130----->42.724624769102 :)\n",
            "The loss function for the iteration 9140----->42.7244674065911 :)\n",
            "The loss function for the iteration 9150----->42.72431015847372 :)\n",
            "The loss function for the iteration 9160----->42.72415294839375 :)\n",
            "The loss function for the iteration 9170----->42.72399581240355 :)\n",
            "The loss function for the iteration 9180----->42.72383878291306 :)\n",
            "The loss function for the iteration 9190----->42.72368179264276 :)\n",
            "The loss function for the iteration 9200----->42.72352488465936 :)\n",
            "The loss function for the iteration 9210----->42.72336808528006 :)\n",
            "The loss function for the iteration 9220----->42.72321132605121 :)\n",
            "The loss function for the iteration 9230----->42.723054677774364 :)\n",
            "The loss function for the iteration 9240----->42.72289807911401 :)\n",
            "The loss function for the iteration 9250----->42.722741560145955 :)\n",
            "The loss function for the iteration 9260----->42.72258514264357 :)\n",
            "The loss function for the iteration 9270----->42.72242877613879 :)\n",
            "The loss function for the iteration 9280----->42.722272482141435 :)\n",
            "The loss function for the iteration 9290----->42.72211627212362 :)\n",
            "The loss function for the iteration 9300----->42.7219601050125 :)\n",
            "The loss function for the iteration 9310----->42.72180406976575 :)\n",
            "The loss function for the iteration 9320----->42.721648072467765 :)\n",
            "The loss function for the iteration 9330----->42.721492139700175 :)\n",
            "The loss function for the iteration 9340----->42.72133633001999 :)\n",
            "The loss function for the iteration 9350----->42.72118053944918 :)\n",
            "The loss function for the iteration 9360----->42.72102487356877 :)\n",
            "The loss function for the iteration 9370----->42.72086925099167 :)\n",
            "The loss function for the iteration 9380----->42.72071370493312 :)\n",
            "The loss function for the iteration 9390----->42.72055823274545 :)\n",
            "The loss function for the iteration 9400----->42.72040284276256 :)\n",
            "The loss function for the iteration 9410----->42.72024752477831 :)\n",
            "The loss function for the iteration 9420----->42.720092266240385 :)\n",
            "The loss function for the iteration 9430----->42.71993710580054 :)\n",
            "The loss function for the iteration 9440----->42.71978200370998 :)\n",
            "The loss function for the iteration 9450----->42.71962697130559 :)\n",
            "The loss function for the iteration 9460----->42.71947201807467 :)\n",
            "The loss function for the iteration 9470----->42.71931715961877 :)\n",
            "The loss function for the iteration 9480----->42.719162356388 :)\n",
            "The loss function for the iteration 9490----->42.71900763426148 :)\n",
            "The loss function for the iteration 9500----->42.718852968320085 :)\n",
            "The loss function for the iteration 9510----->42.718698377438464 :)\n",
            "The loss function for the iteration 9520----->42.71854387203614 :)\n",
            "The loss function for the iteration 9530----->42.71838944325216 :)\n",
            "The loss function for the iteration 9540----->42.71823508613773 :)\n",
            "The loss function for the iteration 9550----->42.71808080417398 :)\n",
            "The loss function for the iteration 9560----->42.717926595396946 :)\n",
            "The loss function for the iteration 9570----->42.717772457234126 :)\n",
            "The loss function for the iteration 9580----->42.717618383374884 :)\n",
            "The loss function for the iteration 9590----->42.717464395617746 :)\n",
            "The loss function for the iteration 9600----->42.71731048729774 :)\n",
            "The loss function for the iteration 9610----->42.717156614125386 :)\n",
            "The loss function for the iteration 9620----->42.71700285297435 :)\n",
            "The loss function for the iteration 9630----->42.716849136485756 :)\n",
            "The loss function for the iteration 9640----->42.71669551477373 :)\n",
            "The loss function for the iteration 9650----->42.71654197191296 :)\n",
            "The loss function for the iteration 9660----->42.71638847608867 :)\n",
            "The loss function for the iteration 9670----->42.716235069733955 :)\n",
            "The loss function for the iteration 9680----->42.71608171871637 :)\n",
            "The loss function for the iteration 9690----->42.715928474096245 :)\n",
            "The loss function for the iteration 9700----->42.71577527302291 :)\n",
            "The loss function for the iteration 9710----->42.71562215934467 :)\n",
            "The loss function for the iteration 9720----->42.715469120422334 :)\n",
            "The loss function for the iteration 9730----->42.71531614788932 :)\n",
            "The loss function for the iteration 9740----->42.7151632395529 :)\n",
            "The loss function for the iteration 9750----->42.715010395949356 :)\n",
            "The loss function for the iteration 9760----->42.714857629552206 :)\n",
            "The loss function for the iteration 9770----->42.71470492276183 :)\n",
            "The loss function for the iteration 9780----->42.71455231793023 :)\n",
            "The loss function for the iteration 9790----->42.71439975549996 :)\n",
            "The loss function for the iteration 9800----->42.714247292915516 :)\n",
            "The loss function for the iteration 9810----->42.71409489742355 :)\n",
            "The loss function for the iteration 9820----->42.71394256916323 :)\n",
            "The loss function for the iteration 9830----->42.713790300982375 :)\n",
            "The loss function for the iteration 9840----->42.713638098590856 :)\n",
            "The loss function for the iteration 9850----->42.713486008996384 :)\n",
            "The loss function for the iteration 9860----->42.71333396646788 :)\n",
            "The loss function for the iteration 9870----->42.71318199123591 :)\n",
            "The loss function for the iteration 9880----->42.71303010357126 :)\n",
            "The loss function for the iteration 9890----->42.712878259706436 :)\n",
            "The loss function for the iteration 9900----->42.712726503424896 :)\n",
            "The loss function for the iteration 9910----->42.71257482847786 :)\n",
            "The loss function for the iteration 9920----->42.71242318338588 :)\n",
            "The loss function for the iteration 9930----->42.7122716499549 :)\n",
            "The loss function for the iteration 9940----->42.71212016569915 :)\n",
            "The loss function for the iteration 9950----->42.71196874191023 :)\n",
            "The loss function for the iteration 9960----->42.71181741264487 :)\n",
            "The loss function for the iteration 9970----->42.71166615203815 :)\n",
            "The loss function for the iteration 9980----->42.71151498066297 :)\n",
            "The loss function for the iteration 9990----->42.71136386275828 :)\n",
            "The loss function for the iteration 10000----->42.71121279424884 :)\n",
            "The loss function for the iteration 10010----->42.711061816834004 :)\n",
            "The loss function for the iteration 10020----->42.71091091419071 :)\n",
            "The loss function for the iteration 10030----->42.71076005931745 :)\n",
            "The loss function for the iteration 10040----->42.710609320672596 :)\n",
            "The loss function for the iteration 10050----->42.71045860320086 :)\n",
            "The loss function for the iteration 10060----->42.71030796747627 :)\n",
            "The loss function for the iteration 10070----->42.71015743781915 :)\n",
            "The loss function for the iteration 10080----->42.71000690245492 :)\n",
            "The loss function for the iteration 10090----->42.70985650471418 :)\n",
            "The loss function for the iteration 10100----->42.709706163301206 :)\n",
            "The loss function for the iteration 10110----->42.7095558770618 :)\n",
            "The loss function for the iteration 10120----->42.70940568105321 :)\n",
            "The loss function for the iteration 10130----->42.709255537667794 :)\n",
            "The loss function for the iteration 10140----->42.70910548030253 :)\n",
            "The loss function for the iteration 10150----->42.70895548505463 :)\n",
            "The loss function for the iteration 10160----->42.70880555342134 :)\n",
            "The loss function for the iteration 10170----->42.70865568336845 :)\n",
            "The loss function for the iteration 10180----->42.70850590610107 :)\n",
            "The loss function for the iteration 10190----->42.708356170833255 :)\n",
            "The loss function for the iteration 10200----->42.70820653367226 :)\n",
            "The loss function for the iteration 10210----->42.708056938645605 :)\n",
            "The loss function for the iteration 10220----->42.70790740815961 :)\n",
            "The loss function for the iteration 10230----->42.70775799737835 :)\n",
            "The loss function for the iteration 10240----->42.707608597289756 :)\n",
            "The loss function for the iteration 10250----->42.707459299630194 :)\n",
            "The loss function for the iteration 10260----->42.70731006682382 :)\n",
            "The loss function for the iteration 10270----->42.70716088333965 :)\n",
            "The loss function for the iteration 10280----->42.707011785547806 :)\n",
            "The loss function for the iteration 10290----->42.70686274861535 :)\n",
            "The loss function for the iteration 10300----->42.706713790962276 :)\n",
            "The loss function for the iteration 10310----->42.70656490546209 :)\n",
            "The loss function for the iteration 10320----->42.70641610267305 :)\n",
            "The loss function for the iteration 10330----->42.70626733661277 :)\n",
            "The loss function for the iteration 10340----->42.706118659132606 :)\n",
            "The loss function for the iteration 10350----->42.70597005872506 :)\n",
            "The loss function for the iteration 10360----->42.705821519802306 :)\n",
            "The loss function for the iteration 10370----->42.705673039934474 :)\n",
            "The loss function for the iteration 10380----->42.705524601240185 :)\n",
            "The loss function for the iteration 10390----->42.70537626335159 :)\n",
            "The loss function for the iteration 10400----->42.705227978482654 :)\n",
            "The loss function for the iteration 10410----->42.70507976707345 :)\n",
            "The loss function for the iteration 10420----->42.70493163796283 :)\n",
            "The loss function for the iteration 10430----->42.70478358216868 :)\n",
            "The loss function for the iteration 10440----->42.70463557476655 :)\n",
            "The loss function for the iteration 10450----->42.70448764191145 :)\n",
            "The loss function for the iteration 10460----->42.704339771169735 :)\n",
            "The loss function for the iteration 10470----->42.70419196596325 :)\n",
            "The loss function for the iteration 10480----->42.70404426552306 :)\n",
            "The loss function for the iteration 10490----->42.703896592227125 :)\n",
            "The loss function for the iteration 10500----->42.70374898678091 :)\n",
            "The loss function for the iteration 10510----->42.70360147081179 :)\n",
            "The loss function for the iteration 10520----->42.7034539990178 :)\n",
            "The loss function for the iteration 10530----->42.7033066010451 :)\n",
            "The loss function for the iteration 10540----->42.703159274919166 :)\n",
            "The loss function for the iteration 10550----->42.70301200611412 :)\n",
            "The loss function for the iteration 10560----->42.70286483127023 :)\n",
            "The loss function for the iteration 10570----->42.70271770089954 :)\n",
            "The loss function for the iteration 10580----->42.70257064617898 :)\n",
            "The loss function for the iteration 10590----->42.70242368160737 :)\n",
            "The loss function for the iteration 10600----->42.70227675440073 :)\n",
            "The loss function for the iteration 10610----->42.70212991005451 :)\n",
            "The loss function for the iteration 10620----->42.70198311104688 :)\n",
            "The loss function for the iteration 10630----->42.70183636511945 :)\n",
            "The loss function for the iteration 10640----->42.70168972914754 :)\n",
            "The loss function for the iteration 10650----->42.701543141241295 :)\n",
            "The loss function for the iteration 10660----->42.7013966105036 :)\n",
            "The loss function for the iteration 10670----->42.70125017725726 :)\n",
            "The loss function for the iteration 10680----->42.70110379945603 :)\n",
            "The loss function for the iteration 10690----->42.700957477167854 :)\n",
            "The loss function for the iteration 10700----->42.700811231361286 :)\n",
            "The loss function for the iteration 10710----->42.700665032195744 :)\n",
            "The loss function for the iteration 10720----->42.70051891216397 :)\n",
            "The loss function for the iteration 10730----->42.70037286296344 :)\n",
            "The loss function for the iteration 10740----->42.700226880536384 :)\n",
            "The loss function for the iteration 10750----->42.70008098383969 :)\n",
            "The loss function for the iteration 10760----->42.69993512679927 :)\n",
            "The loss function for the iteration 10770----->42.69978936121657 :)\n",
            "The loss function for the iteration 10780----->42.69964363916307 :)\n",
            "The loss function for the iteration 10790----->42.699497971841836 :)\n",
            "The loss function for the iteration 10800----->42.69935240806258 :)\n",
            "The loss function for the iteration 10810----->42.69920689329304 :)\n",
            "The loss function for the iteration 10820----->42.69906143242725 :)\n",
            "The loss function for the iteration 10830----->42.69891602526515 :)\n",
            "The loss function for the iteration 10840----->42.69877070606029 :)\n",
            "The loss function for the iteration 10850----->42.69862544445895 :)\n",
            "The loss function for the iteration 10860----->42.69848023775648 :)\n",
            "The loss function for the iteration 10870----->42.6983351187683 :)\n",
            "The loss function for the iteration 10880----->42.69819005674001 :)\n",
            "The loss function for the iteration 10890----->42.69804505779634 :)\n",
            "The loss function for the iteration 10900----->42.697900133213295 :)\n",
            "The loss function for the iteration 10910----->42.69775527895227 :)\n",
            "The loss function for the iteration 10920----->42.697610484804066 :)\n",
            "The loss function for the iteration 10930----->42.69746578268668 :)\n",
            "The loss function for the iteration 10940----->42.69732110521351 :)\n",
            "The loss function for the iteration 10950----->42.69717647867415 :)\n",
            "The loss function for the iteration 10960----->42.69703195403621 :)\n",
            "The loss function for the iteration 10970----->42.69688746741054 :)\n",
            "The loss function for the iteration 10980----->42.696743072838075 :)\n",
            "The loss function for the iteration 10990----->42.69659873077463 :)\n",
            "The loss function for the iteration 11000----->42.69645445044659 :)\n",
            "The loss function for the iteration 11010----->42.69631025485961 :)\n",
            "The loss function for the iteration 11020----->42.69616608889554 :)\n",
            "The loss function for the iteration 11030----->42.69602201312325 :)\n",
            "The loss function for the iteration 11040----->42.695878016473394 :)\n",
            "The loss function for the iteration 11050----->42.695734063731486 :)\n",
            "The loss function for the iteration 11060----->42.695590180835154 :)\n",
            "The loss function for the iteration 11070----->42.695446362109735 :)\n",
            "The loss function for the iteration 11080----->42.69530259452864 :)\n",
            "The loss function for the iteration 11090----->42.69515891423991 :)\n",
            "The loss function for the iteration 11100----->42.69501530809936 :)\n",
            "The loss function for the iteration 11110----->42.694871723848465 :)\n",
            "The loss function for the iteration 11120----->42.694728240213344 :)\n",
            "The loss function for the iteration 11130----->42.69458481137961 :)\n",
            "The loss function for the iteration 11140----->42.69444142451491 :)\n",
            "The loss function for the iteration 11150----->42.694298113828516 :)\n",
            "The loss function for the iteration 11160----->42.694154864210574 :)\n",
            "The loss function for the iteration 11170----->42.69401168480936 :)\n",
            "The loss function for the iteration 11180----->42.69386856682855 :)\n",
            "The loss function for the iteration 11190----->42.693725528664935 :)\n",
            "The loss function for the iteration 11200----->42.69358255177313 :)\n",
            "The loss function for the iteration 11210----->42.69343964142648 :)\n",
            "The loss function for the iteration 11220----->42.69329679233731 :)\n",
            "The loss function for the iteration 11230----->42.69315399250145 :)\n",
            "The loss function for the iteration 11240----->42.693011259645296 :)\n",
            "The loss function for the iteration 11250----->42.69286858867431 :)\n",
            "The loss function for the iteration 11260----->42.69272599270898 :)\n",
            "The loss function for the iteration 11270----->42.692583436407396 :)\n",
            "The loss function for the iteration 11280----->42.692440944308984 :)\n",
            "The loss function for the iteration 11290----->42.69229855709692 :)\n",
            "The loss function for the iteration 11300----->42.6921562008023 :)\n",
            "The loss function for the iteration 11310----->42.69201391321849 :)\n",
            "The loss function for the iteration 11320----->42.69187170027151 :)\n",
            "The loss function for the iteration 11330----->42.69172953536437 :)\n",
            "The loss function for the iteration 11340----->42.691587444194454 :)\n",
            "The loss function for the iteration 11350----->42.691445419922346 :)\n",
            "The loss function for the iteration 11360----->42.6913034593837 :)\n",
            "The loss function for the iteration 11370----->42.69116158547833 :)\n",
            "The loss function for the iteration 11380----->42.69101975324361 :)\n",
            "The loss function for the iteration 11390----->42.69087796924038 :)\n",
            "The loss function for the iteration 11400----->42.690736249564154 :)\n",
            "The loss function for the iteration 11410----->42.690594569439455 :)\n",
            "The loss function for the iteration 11420----->42.69045299368346 :)\n",
            "The loss function for the iteration 11430----->42.690311476993074 :)\n",
            "The loss function for the iteration 11440----->42.69016999855204 :)\n",
            "The loss function for the iteration 11450----->42.69002862399876 :)\n",
            "The loss function for the iteration 11460----->42.68988729189254 :)\n",
            "The loss function for the iteration 11470----->42.68974599056327 :)\n",
            "The loss function for the iteration 11480----->42.68960479671669 :)\n",
            "The loss function for the iteration 11490----->42.68946364335651 :)\n",
            "The loss function for the iteration 11500----->42.68932254872287 :)\n",
            "The loss function for the iteration 11510----->42.68918153385347 :)\n",
            "The loss function for the iteration 11520----->42.68904055270013 :)\n",
            "The loss function for the iteration 11530----->42.688899674784714 :)\n",
            "The loss function for the iteration 11540----->42.68875886662106 :)\n",
            "The loss function for the iteration 11550----->42.68861808957709 :)\n",
            "The loss function for the iteration 11560----->42.68847739090577 :)\n",
            "The loss function for the iteration 11570----->42.68833671738864 :)\n",
            "The loss function for the iteration 11580----->42.68819613712256 :)\n",
            "The loss function for the iteration 11590----->42.68805561085489 :)\n",
            "The loss function for the iteration 11600----->42.68791512638232 :)\n",
            "The loss function for the iteration 11610----->42.68777475638847 :)\n",
            "The loss function for the iteration 11620----->42.687634418704945 :)\n",
            "The loss function for the iteration 11630----->42.68749412510268 :)\n",
            "The loss function for the iteration 11640----->42.68735390273807 :)\n",
            "The loss function for the iteration 11650----->42.68721375890767 :)\n",
            "The loss function for the iteration 11660----->42.68707366301713 :)\n",
            "The loss function for the iteration 11670----->42.68693365145646 :)\n",
            "The loss function for the iteration 11680----->42.68679368842598 :)\n",
            "The loss function for the iteration 11690----->42.68665377566473 :)\n",
            "The loss function for the iteration 11700----->42.686513951068264 :)\n",
            "The loss function for the iteration 11710----->42.6863741455759 :)\n",
            "The loss function for the iteration 11720----->42.686234419473074 :)\n",
            "The loss function for the iteration 11730----->42.686094754954006 :)\n",
            "The loss function for the iteration 11740----->42.685955148951855 :)\n",
            "The loss function for the iteration 11750----->42.685815619319385 :)\n",
            "The loss function for the iteration 11760----->42.68567611609055 :)\n",
            "The loss function for the iteration 11770----->42.68553671548494 :)\n",
            "The loss function for the iteration 11780----->42.68539736017479 :)\n",
            "The loss function for the iteration 11790----->42.68525807251543 :)\n",
            "The loss function for the iteration 11800----->42.685118842352914 :)\n",
            "The loss function for the iteration 11810----->42.6849796738008 :)\n",
            "The loss function for the iteration 11820----->42.68484059552212 :)\n",
            "The loss function for the iteration 11830----->42.684701516771426 :)\n",
            "The loss function for the iteration 11840----->42.68456253408125 :)\n",
            "The loss function for the iteration 11850----->42.68442360513562 :)\n",
            "The loss function for the iteration 11860----->42.68428472950533 :)\n",
            "The loss function for the iteration 11870----->42.68414593306854 :)\n",
            "The loss function for the iteration 11880----->42.68400718005222 :)\n",
            "The loss function for the iteration 11890----->42.683868502374125 :)\n",
            "The loss function for the iteration 11900----->42.683729872516714 :)\n",
            "The loss function for the iteration 11910----->42.683591311602996 :)\n",
            "The loss function for the iteration 11920----->42.68345280470891 :)\n",
            "The loss function for the iteration 11930----->42.6833143642498 :)\n",
            "The loss function for the iteration 11940----->42.68317599832481 :)\n",
            "The loss function for the iteration 11950----->42.683037678149034 :)\n",
            "The loss function for the iteration 11960----->42.68289940732135 :)\n",
            "The loss function for the iteration 11970----->42.682761194395376 :)\n",
            "The loss function for the iteration 11980----->42.68262307740449 :)\n",
            "The loss function for the iteration 11990----->42.68248500200281 :)\n",
            "The loss function for the iteration 12000----->42.68234698915211 :)\n",
            "The loss function for the iteration 12010----->42.68220906095706 :)\n",
            "The loss function for the iteration 12020----->42.68207114571483 :)\n",
            "The loss function for the iteration 12030----->42.68193330124505 :)\n",
            "The loss function for the iteration 12040----->42.68179552463365 :)\n",
            "The loss function for the iteration 12050----->42.68165780814719 :)\n",
            "The loss function for the iteration 12060----->42.68152016315518 :)\n",
            "The loss function for the iteration 12070----->42.68138257861582 :)\n",
            "The loss function for the iteration 12080----->42.68124502811747 :)\n",
            "The loss function for the iteration 12090----->42.681107536746424 :)\n",
            "The loss function for the iteration 12100----->42.68097012850558 :)\n",
            "The loss function for the iteration 12110----->42.680832786790894 :)\n",
            "The loss function for the iteration 12120----->42.68069549006521 :)\n",
            "The loss function for the iteration 12130----->42.68055826005559 :)\n",
            "The loss function for the iteration 12140----->42.68042109959797 :)\n",
            "The loss function for the iteration 12150----->42.68028396693391 :)\n",
            "The loss function for the iteration 12160----->42.68014691115159 :)\n",
            "The loss function for the iteration 12170----->42.680009933428884 :)\n",
            "The loss function for the iteration 12180----->42.67987299814617 :)\n",
            "The loss function for the iteration 12190----->42.67973611189432 :)\n",
            "The loss function for the iteration 12200----->42.67959926531914 :)\n",
            "The loss function for the iteration 12210----->42.67946250773253 :)\n",
            "The loss function for the iteration 12220----->42.67932579139285 :)\n",
            "The loss function for the iteration 12230----->42.67918916588384 :)\n",
            "The loss function for the iteration 12240----->42.67905260507309 :)\n",
            "The loss function for the iteration 12250----->42.678916052152026 :)\n",
            "The loss function for the iteration 12260----->42.67877960463864 :)\n",
            "The loss function for the iteration 12270----->42.678643177853814 :)\n",
            "The loss function for the iteration 12280----->42.678506812032374 :)\n",
            "The loss function for the iteration 12290----->42.6783705324084 :)\n",
            "The loss function for the iteration 12300----->42.67823430408217 :)\n",
            "The loss function for the iteration 12310----->42.67809813539589 :)\n",
            "The loss function for the iteration 12320----->42.67796199680988 :)\n",
            "The loss function for the iteration 12330----->42.677825939367864 :)\n",
            "The loss function for the iteration 12340----->42.67768994684805 :)\n",
            "The loss function for the iteration 12350----->42.677554004795056 :)\n",
            "The loss function for the iteration 12360----->42.677418106422955 :)\n",
            "The loss function for the iteration 12370----->42.67728228603547 :)\n",
            "The loss function for the iteration 12380----->42.67714651504063 :)\n",
            "The loss function for the iteration 12390----->42.67701079826725 :)\n",
            "The loss function for the iteration 12400----->42.67687518501241 :)\n",
            "The loss function for the iteration 12410----->42.67673958393659 :)\n",
            "The loss function for the iteration 12420----->42.676604056421354 :)\n",
            "The loss function for the iteration 12430----->42.676468607527745 :)\n",
            "The loss function for the iteration 12440----->42.676333166185664 :)\n",
            "The loss function for the iteration 12450----->42.67619782445426 :)\n",
            "The loss function for the iteration 12460----->42.67606253930413 :)\n",
            "The loss function for the iteration 12470----->42.67592729155415 :)\n",
            "The loss function for the iteration 12480----->42.675792113663185 :)\n",
            "The loss function for the iteration 12490----->42.67565697881068 :)\n",
            "The loss function for the iteration 12500----->42.675521919912626 :)\n",
            "The loss function for the iteration 12510----->42.67538692560473 :)\n",
            "The loss function for the iteration 12520----->42.67525193607346 :)\n",
            "The loss function for the iteration 12530----->42.675117057661645 :)\n",
            "The loss function for the iteration 12540----->42.674982233742554 :)\n",
            "The loss function for the iteration 12550----->42.67484743387253 :)\n",
            "The loss function for the iteration 12560----->42.674712750759745 :)\n",
            "The loss function for the iteration 12570----->42.674578075688835 :)\n",
            "The loss function for the iteration 12580----->42.67444346841314 :)\n",
            "The loss function for the iteration 12590----->42.67430893733331 :)\n",
            "The loss function for the iteration 12600----->42.67417443281195 :)\n",
            "The loss function for the iteration 12610----->42.674040017191786 :)\n",
            "The loss function for the iteration 12620----->42.673905632437126 :)\n",
            "The loss function for the iteration 12630----->42.67377132405904 :)\n",
            "The loss function for the iteration 12640----->42.67363706617043 :)\n",
            "The loss function for the iteration 12650----->42.67350285652081 :)\n",
            "The loss function for the iteration 12660----->42.67336873241027 :)\n",
            "The loss function for the iteration 12670----->42.67323464340483 :)\n",
            "The loss function for the iteration 12680----->42.67310061352813 :)\n",
            "The loss function for the iteration 12690----->42.672966651623575 :)\n",
            "The loss function for the iteration 12700----->42.67283276327014 :)\n",
            "The loss function for the iteration 12710----->42.67269889046502 :)\n",
            "The loss function for the iteration 12720----->42.67256506710474 :)\n",
            "The loss function for the iteration 12730----->42.672431331256185 :)\n",
            "The loss function for the iteration 12740----->42.67229763855118 :)\n",
            "The loss function for the iteration 12750----->42.67216401264101 :)\n",
            "The loss function for the iteration 12760----->42.67203044509368 :)\n",
            "The loss function for the iteration 12770----->42.67189692116665 :)\n",
            "The loss function for the iteration 12780----->42.67176345803731 :)\n",
            "The loss function for the iteration 12790----->42.67163006332317 :)\n",
            "The loss function for the iteration 12800----->42.671496717755126 :)\n",
            "The loss function for the iteration 12810----->42.67136342762469 :)\n",
            "The loss function for the iteration 12820----->42.67123018058169 :)\n",
            "The loss function for the iteration 12830----->42.67109699413355 :)\n",
            "The loss function for the iteration 12840----->42.67096387581122 :)\n",
            "The loss function for the iteration 12850----->42.67083079694224 :)\n",
            "The loss function for the iteration 12860----->42.670697815554675 :)\n",
            "The loss function for the iteration 12870----->42.67056487367996 :)\n",
            "The loss function for the iteration 12880----->42.67043196588976 :)\n",
            "The loss function for the iteration 12890----->42.67029914197139 :)\n",
            "The loss function for the iteration 12900----->42.670166353841005 :)\n",
            "The loss function for the iteration 12910----->42.67003362065881 :)\n",
            "The loss function for the iteration 12920----->42.66990095129851 :)\n",
            "The loss function for the iteration 12930----->42.66976836183575 :)\n",
            "The loss function for the iteration 12940----->42.66963580061521 :)\n",
            "The loss function for the iteration 12950----->42.669503282433226 :)\n",
            "The loss function for the iteration 12960----->42.669370832958556 :)\n",
            "The loss function for the iteration 12970----->42.669238434664955 :)\n",
            "The loss function for the iteration 12980----->42.66910611228809 :)\n",
            "The loss function for the iteration 12990----->42.668973819447984 :)\n",
            "The loss function for the iteration 13000----->42.66884160595324 :)\n",
            "The loss function for the iteration 13010----->42.668709444520815 :)\n",
            "The loss function for the iteration 13020----->42.66857732588847 :)\n",
            "The loss function for the iteration 13030----->42.66844529501444 :)\n",
            "The loss function for the iteration 13040----->42.668313285276916 :)\n",
            "The loss function for the iteration 13050----->42.668181337187065 :)\n",
            "The loss function for the iteration 13060----->42.668049463614125 :)\n",
            "The loss function for the iteration 13070----->42.667917615682 :)\n",
            "The loss function for the iteration 13080----->42.66778581550749 :)\n",
            "The loss function for the iteration 13090----->42.667654102075076 :)\n",
            "The loss function for the iteration 13100----->42.66752246111223 :)\n",
            "The loss function for the iteration 13110----->42.66739086202383 :)\n",
            "The loss function for the iteration 13120----->42.667259300272214 :)\n",
            "The loss function for the iteration 13130----->42.66712779379693 :)\n",
            "The loss function for the iteration 13140----->42.6669963669467 :)\n",
            "The loss function for the iteration 13150----->42.66686496002353 :)\n",
            "The loss function for the iteration 13160----->42.66673363616615 :)\n",
            "The loss function for the iteration 13170----->42.666602371165496 :)\n",
            "The loss function for the iteration 13180----->42.666471138925466 :)\n",
            "The loss function for the iteration 13190----->42.66633998696831 :)\n",
            "The loss function for the iteration 13200----->42.66620884865328 :)\n",
            "The loss function for the iteration 13210----->42.666077800993534 :)\n",
            "The loss function for the iteration 13220----->42.66594680282995 :)\n",
            "The loss function for the iteration 13230----->42.665815851183254 :)\n",
            "The loss function for the iteration 13240----->42.66568497976544 :)\n",
            "The loss function for the iteration 13250----->42.665554121441566 :)\n",
            "The loss function for the iteration 13260----->42.6654233573255 :)\n",
            "The loss function for the iteration 13270----->42.66529263477739 :)\n",
            "The loss function for the iteration 13280----->42.66516194299844 :)\n",
            "The loss function for the iteration 13290----->42.6650313377859 :)\n",
            "The loss function for the iteration 13300----->42.664900774503394 :)\n",
            "The loss function for the iteration 13310----->42.66477025181966 :)\n",
            "The loss function for the iteration 13320----->42.66463980161459 :)\n",
            "The loss function for the iteration 13330----->42.664509394579234 :)\n",
            "The loss function for the iteration 13340----->42.66437905002047 :)\n",
            "The loss function for the iteration 13350----->42.66424877686044 :)\n",
            "The loss function for the iteration 13360----->42.66411851578026 :)\n",
            "The loss function for the iteration 13370----->42.66398835108379 :)\n",
            "The loss function for the iteration 13380----->42.663858215627656 :)\n",
            "The loss function for the iteration 13390----->42.663728133800056 :)\n",
            "The loss function for the iteration 13400----->42.66359814485241 :)\n",
            "The loss function for the iteration 13410----->42.663468144877015 :)\n",
            "The loss function for the iteration 13420----->42.66333825284624 :)\n",
            "The loss function for the iteration 13430----->42.66320839687622 :)\n",
            "The loss function for the iteration 13440----->42.663078566888665 :)\n",
            "The loss function for the iteration 13450----->42.66294883615058 :)\n",
            "The loss function for the iteration 13460----->42.66281914426488 :)\n",
            "The loss function for the iteration 13470----->42.6626895074589 :)\n",
            "The loss function for the iteration 13480----->42.662559928138286 :)\n",
            "The loss function for the iteration 13490----->42.66243039715206 :)\n",
            "The loss function for the iteration 13500----->42.66230093006385 :)\n",
            "The loss function for the iteration 13510----->42.662171510263626 :)\n",
            "The loss function for the iteration 13520----->42.66204211376687 :)\n",
            "The loss function for the iteration 13530----->42.6619128168916 :)\n",
            "The loss function for the iteration 13540----->42.66178355886942 :)\n",
            "The loss function for the iteration 13550----->42.661654331505694 :)\n",
            "The loss function for the iteration 13560----->42.661525208928225 :)\n",
            "The loss function for the iteration 13570----->42.66139610173597 :)\n",
            "The loss function for the iteration 13580----->42.661267057044675 :)\n",
            "The loss function for the iteration 13590----->42.66113806297917 :)\n",
            "The loss function for the iteration 13600----->42.66100911396916 :)\n",
            "The loss function for the iteration 13610----->42.660880226121456 :)\n",
            "The loss function for the iteration 13620----->42.66075138686645 :)\n",
            "The loss function for the iteration 13630----->42.66062261072123 :)\n",
            "The loss function for the iteration 13640----->42.66049388035359 :)\n",
            "The loss function for the iteration 13650----->42.66036520862116 :)\n",
            "The loss function for the iteration 13660----->42.66023658903342 :)\n",
            "The loss function for the iteration 13670----->42.66010801950377 :)\n",
            "The loss function for the iteration 13680----->42.65997950285 :)\n",
            "The loss function for the iteration 13690----->42.659851039195786 :)\n",
            "The loss function for the iteration 13700----->42.659722647068726 :)\n",
            "The loss function for the iteration 13710----->42.65959427847139 :)\n",
            "The loss function for the iteration 13720----->42.65946597521147 :)\n",
            "The loss function for the iteration 13730----->42.65933774674958 :)\n",
            "The loss function for the iteration 13740----->42.65920955966646 :)\n",
            "The loss function for the iteration 13750----->42.65908139190082 :)\n",
            "The loss function for the iteration 13760----->42.658953304177786 :)\n",
            "The loss function for the iteration 13770----->42.65882526393203 :)\n",
            "The loss function for the iteration 13780----->42.6586972562664 :)\n",
            "The loss function for the iteration 13790----->42.658569329183074 :)\n",
            "The loss function for the iteration 13800----->42.65844144887784 :)\n",
            "The loss function for the iteration 13810----->42.65831362821539 :)\n",
            "The loss function for the iteration 13820----->42.658185848840866 :)\n",
            "The loss function for the iteration 13830----->42.65805809661106 :)\n",
            "The loss function for the iteration 13840----->42.65793045124702 :)\n",
            "The loss function for the iteration 13850----->42.65780281514408 :)\n",
            "The loss function for the iteration 13860----->42.657675254860315 :)\n",
            "The loss function for the iteration 13870----->42.65754776261767 :)\n",
            "The loss function for the iteration 13880----->42.65742029288012 :)\n",
            "The loss function for the iteration 13890----->42.65729290118936 :)\n",
            "The loss function for the iteration 13900----->42.65716555756753 :)\n",
            "The loss function for the iteration 13910----->42.65703824334537 :)\n",
            "The loss function for the iteration 13920----->42.656911002409544 :)\n",
            "The loss function for the iteration 13930----->42.6567838210025 :)\n",
            "The loss function for the iteration 13940----->42.65665665659539 :)\n",
            "The loss function for the iteration 13950----->42.65652956655062 :)\n",
            "The loss function for the iteration 13960----->42.65640250473005 :)\n",
            "The loss function for the iteration 13970----->42.65627553692818 :)\n",
            "The loss function for the iteration 13980----->42.65614861921659 :)\n",
            "The loss function for the iteration 13990----->42.65602169608397 :)\n",
            "The loss function for the iteration 14000----->42.65589489208918 :)\n",
            "The loss function for the iteration 14010----->42.655768110005326 :)\n",
            "The loss function for the iteration 14020----->42.65564134941196 :)\n",
            "The loss function for the iteration 14030----->42.65551468677036 :)\n",
            "The loss function for the iteration 14040----->42.65538806249083 :)\n",
            "The loss function for the iteration 14050----->42.655261489761564 :)\n",
            "The loss function for the iteration 14060----->42.65513494867548 :)\n",
            "The loss function for the iteration 14070----->42.65500844759637 :)\n",
            "The loss function for the iteration 14080----->42.65488203160247 :)\n",
            "The loss function for the iteration 14090----->42.65475566466762 :)\n",
            "The loss function for the iteration 14100----->42.654629328646365 :)\n",
            "The loss function for the iteration 14110----->42.654503086727274 :)\n",
            "The loss function for the iteration 14120----->42.65437686928563 :)\n",
            "The loss function for the iteration 14130----->42.6542506868029 :)\n",
            "The loss function for the iteration 14140----->42.654124593393774 :)\n",
            "The loss function for the iteration 14150----->42.65399851222808 :)\n",
            "The loss function for the iteration 14160----->42.65387252411466 :)\n",
            "The loss function for the iteration 14170----->42.65374658305927 :)\n",
            "The loss function for the iteration 14180----->42.65362066343308 :)\n",
            "The loss function for the iteration 14190----->42.65349481419228 :)\n",
            "The loss function for the iteration 14200----->42.65336900319607 :)\n",
            "The loss function for the iteration 14210----->42.65324326527662 :)\n",
            "The loss function for the iteration 14220----->42.65311755921412 :)\n",
            "The loss function for the iteration 14230----->42.652991904016105 :)\n",
            "The loss function for the iteration 14240----->42.65286632400261 :)\n",
            "The loss function for the iteration 14250----->42.6527407701297 :)\n",
            "The loss function for the iteration 14260----->42.65261524837948 :)\n",
            "The loss function for the iteration 14270----->42.65248982319971 :)\n",
            "The loss function for the iteration 14280----->42.65236443453286 :)\n",
            "The loss function for the iteration 14290----->42.65223906735925 :)\n",
            "The loss function for the iteration 14300----->42.65211377591967 :)\n",
            "The loss function for the iteration 14310----->42.65198851890329 :)\n",
            "The loss function for the iteration 14320----->42.6518633237141 :)\n",
            "The loss function for the iteration 14330----->42.65173818914729 :)\n",
            "The loss function for the iteration 14340----->42.65161310108381 :)\n",
            "The loss function for the iteration 14350----->42.651488100600254 :)\n",
            "The loss function for the iteration 14360----->42.651363093494375 :)\n",
            "The loss function for the iteration 14370----->42.65123816430422 :)\n",
            "The loss function for the iteration 14380----->42.65111327316317 :)\n",
            "The loss function for the iteration 14390----->42.65098841385148 :)\n",
            "The loss function for the iteration 14400----->42.65086365381007 :)\n",
            "The loss function for the iteration 14410----->42.65073892411169 :)\n",
            "The loss function for the iteration 14420----->42.65061422041397 :)\n",
            "The loss function for the iteration 14430----->42.65048960205654 :)\n",
            "The loss function for the iteration 14440----->42.65036502240369 :)\n",
            "The loss function for the iteration 14450----->42.650240478072 :)\n",
            "The loss function for the iteration 14460----->42.65011601476327 :)\n",
            "The loss function for the iteration 14470----->42.649991595275715 :)\n",
            "The loss function for the iteration 14480----->42.64986722782356 :)\n",
            "The loss function for the iteration 14490----->42.6497429007112 :)\n",
            "The loss function for the iteration 14500----->42.64961860334169 :)\n",
            "The loss function for the iteration 14510----->42.64949440759118 :)\n",
            "The loss function for the iteration 14520----->42.649370218012905 :)\n",
            "The loss function for the iteration 14530----->42.649246080634825 :)\n",
            "The loss function for the iteration 14540----->42.64912200798016 :)\n",
            "The loss function for the iteration 14550----->42.648997954433824 :)\n",
            "The loss function for the iteration 14560----->42.64887398019713 :)\n",
            "The loss function for the iteration 14570----->42.648750043314685 :)\n",
            "The loss function for the iteration 14580----->42.64862617116626 :)\n",
            "The loss function for the iteration 14590----->42.64850236668955 :)\n",
            "The loss function for the iteration 14600----->42.648378584455344 :)\n",
            "The loss function for the iteration 14610----->42.648254837979835 :)\n",
            "The loss function for the iteration 14620----->42.64813117116474 :)\n",
            "The loss function for the iteration 14630----->42.64800752261447 :)\n",
            "The loss function for the iteration 14640----->42.64788393819465 :)\n",
            "The loss function for the iteration 14650----->42.647760430307436 :)\n",
            "The loss function for the iteration 14660----->42.647636919525965 :)\n",
            "The loss function for the iteration 14670----->42.64751350439838 :)\n",
            "The loss function for the iteration 14680----->42.64739012880709 :)\n",
            "The loss function for the iteration 14690----->42.64726676578066 :)\n",
            "The loss function for the iteration 14700----->42.64714349978755 :)\n",
            "The loss function for the iteration 14710----->42.64702027999708 :)\n",
            "The loss function for the iteration 14720----->42.64689708917407 :)\n",
            "The loss function for the iteration 14730----->42.646773942659344 :)\n",
            "The loss function for the iteration 14740----->42.64665085950162 :)\n",
            "The loss function for the iteration 14750----->42.64652781993105 :)\n",
            "The loss function for the iteration 14760----->42.64640483407138 :)\n",
            "The loss function for the iteration 14770----->42.646281891141705 :)\n",
            "The loss function for the iteration 14780----->42.646159003370926 :)\n",
            "The loss function for the iteration 14790----->42.646036190234966 :)\n",
            "The loss function for the iteration 14800----->42.64591337333512 :)\n",
            "The loss function for the iteration 14810----->42.645790640745446 :)\n",
            "The loss function for the iteration 14820----->42.64566797109211 :)\n",
            "The loss function for the iteration 14830----->42.64554532968205 :)\n",
            "The loss function for the iteration 14840----->42.64542273877942 :)\n",
            "The loss function for the iteration 14850----->42.64530018908236 :)\n",
            "The loss function for the iteration 14860----->42.645177686781615 :)\n",
            "The loss function for the iteration 14870----->42.645055243635994 :)\n",
            "The loss function for the iteration 14880----->42.644932850644246 :)\n",
            "The loss function for the iteration 14890----->42.64481049329997 :)\n",
            "The loss function for the iteration 14900----->42.64468820532832 :)\n",
            "The loss function for the iteration 14910----->42.64456595386671 :)\n",
            "The loss function for the iteration 14920----->42.64444374019842 :)\n",
            "The loss function for the iteration 14930----->42.64432159226836 :)\n",
            "The loss function for the iteration 14940----->42.644199474686346 :)\n",
            "The loss function for the iteration 14950----->42.644077462376856 :)\n",
            "The loss function for the iteration 14960----->42.64395545706523 :)\n",
            "The loss function for the iteration 14970----->42.64383348758391 :)\n",
            "The loss function for the iteration 14980----->42.6437115964726 :)\n",
            "The loss function for the iteration 14990----->42.6435897285314 :)\n",
            "The loss function for the iteration 15000----->42.643467897832 :)\n",
            "The loss function for the iteration 15010----->42.64334613046323 :)\n",
            "The loss function for the iteration 15020----->42.64322441512709 :)\n",
            "The loss function for the iteration 15030----->42.64310275930285 :)\n",
            "The loss function for the iteration 15040----->42.64298113391336 :)\n",
            "The loss function for the iteration 15050----->42.642859552068266 :)\n",
            "The loss function for the iteration 15060----->42.64273804880544 :)\n",
            "The loss function for the iteration 15070----->42.64261657269677 :)\n",
            "The loss function for the iteration 15080----->42.642495124237605 :)\n",
            "The loss function for the iteration 15090----->42.64237376424906 :)\n",
            "The loss function for the iteration 15100----->42.64225240745994 :)\n",
            "The loss function for the iteration 15110----->42.64213113166888 :)\n",
            "The loss function for the iteration 15120----->42.64200991110587 :)\n",
            "The loss function for the iteration 15130----->42.641888717619835 :)\n",
            "The loss function for the iteration 15140----->42.64176759925039 :)\n",
            "The loss function for the iteration 15150----->42.6416465020166 :)\n",
            "The loss function for the iteration 15160----->42.64152545249615 :)\n",
            "The loss function for the iteration 15170----->42.64140445220995 :)\n",
            "The loss function for the iteration 15180----->42.64128350108793 :)\n",
            "The loss function for the iteration 15190----->42.6411626114333 :)\n",
            "The loss function for the iteration 15200----->42.641041772315354 :)\n",
            "The loss function for the iteration 15210----->42.64092096269 :)\n",
            "The loss function for the iteration 15220----->42.64080021588184 :)\n",
            "The loss function for the iteration 15230----->42.64067951275225 :)\n",
            "The loss function for the iteration 15240----->42.64055883707624 :)\n",
            "The loss function for the iteration 15250----->42.64043823881689 :)\n",
            "The loss function for the iteration 15260----->42.640317680898285 :)\n",
            "The loss function for the iteration 15270----->42.64019716215313 :)\n",
            "The loss function for the iteration 15280----->42.640076699675525 :)\n",
            "The loss function for the iteration 15290----->42.639956276756315 :)\n",
            "The loss function for the iteration 15300----->42.63983592501598 :)\n",
            "The loss function for the iteration 15310----->42.639715577025214 :)\n",
            "The loss function for the iteration 15320----->42.63959530700073 :)\n",
            "The loss function for the iteration 15330----->42.63947508317413 :)\n",
            "The loss function for the iteration 15340----->42.63935488625463 :)\n",
            "The loss function for the iteration 15350----->42.63923474739662 :)\n",
            "The loss function for the iteration 15360----->42.63911465253215 :)\n",
            "The loss function for the iteration 15370----->42.63899462611327 :)\n",
            "The loss function for the iteration 15380----->42.63887463598777 :)\n",
            "The loss function for the iteration 15390----->42.638754680705574 :)\n",
            "The loss function for the iteration 15400----->42.6386347856557 :)\n",
            "The loss function for the iteration 15410----->42.6385149405447 :)\n",
            "The loss function for the iteration 15420----->42.638395131385046 :)\n",
            "The loss function for the iteration 15430----->42.63827538421572 :)\n",
            "The loss function for the iteration 15440----->42.63815568203267 :)\n",
            "The loss function for the iteration 15450----->42.638036007532165 :)\n",
            "The loss function for the iteration 15460----->42.63791642009723 :)\n",
            "The loss function for the iteration 15470----->42.63779683954946 :)\n",
            "The loss function for the iteration 15480----->42.637677310995684 :)\n",
            "The loss function for the iteration 15490----->42.63755785060647 :)\n",
            "The loss function for the iteration 15500----->42.637438425284124 :)\n",
            "The loss function for the iteration 15510----->42.637319043854426 :)\n",
            "The loss function for the iteration 15520----->42.6371996905701 :)\n",
            "The loss function for the iteration 15530----->42.6370804037992 :)\n",
            "The loss function for the iteration 15540----->42.63696116895693 :)\n",
            "The loss function for the iteration 15550----->42.6368419674091 :)\n",
            "The loss function for the iteration 15560----->42.63672282336697 :)\n",
            "The loss function for the iteration 15570----->42.636603737110235 :)\n",
            "The loss function for the iteration 15580----->42.63648469691415 :)\n",
            "The loss function for the iteration 15590----->42.63636566955348 :)\n",
            "The loss function for the iteration 15600----->42.63624673038498 :)\n",
            "The loss function for the iteration 15610----->42.63612781696886 :)\n",
            "The loss function for the iteration 15620----->42.63600894373323 :)\n",
            "The loss function for the iteration 15630----->42.635890132377476 :)\n",
            "The loss function for the iteration 15640----->42.63577135550987 :)\n",
            "The loss function for the iteration 15650----->42.63565263174212 :)\n",
            "The loss function for the iteration 15660----->42.63553394277613 :)\n",
            "The loss function for the iteration 15670----->42.6354153222359 :)\n",
            "The loss function for the iteration 15680----->42.635296745900334 :)\n",
            "The loss function for the iteration 15690----->42.63517820300786 :)\n",
            "The loss function for the iteration 15700----->42.63505970734954 :)\n",
            "The loss function for the iteration 15710----->42.634941262984896 :)\n",
            "The loss function for the iteration 15720----->42.63482286182117 :)\n",
            "The loss function for the iteration 15730----->42.63470451724826 :)\n",
            "The loss function for the iteration 15740----->42.63458621128612 :)\n",
            "The loss function for the iteration 15750----->42.63446794338797 :)\n",
            "The loss function for the iteration 15760----->42.6343497431147 :)\n",
            "The loss function for the iteration 15770----->42.63423154935904 :)\n",
            "The loss function for the iteration 15780----->42.634113428323616 :)\n",
            "The loss function for the iteration 15790----->42.63399533483463 :)\n",
            "The loss function for the iteration 15800----->42.63387729699411 :)\n",
            "The loss function for the iteration 15810----->42.63375935177886 :)\n",
            "The loss function for the iteration 15820----->42.6336413844715 :)\n",
            "The loss function for the iteration 15830----->42.63352349736225 :)\n",
            "The loss function for the iteration 15840----->42.633405664543005 :)\n",
            "The loss function for the iteration 15850----->42.633287857276436 :)\n",
            "The loss function for the iteration 15860----->42.63317009472628 :)\n",
            "The loss function for the iteration 15870----->42.63305237861545 :)\n",
            "The loss function for the iteration 15880----->42.63293472027465 :)\n",
            "The loss function for the iteration 15890----->42.63281709696873 :)\n",
            "The loss function for the iteration 15900----->42.632699528431594 :)\n",
            "The loss function for the iteration 15910----->42.63258200086251 :)\n",
            "The loss function for the iteration 15920----->42.63246453939704 :)\n",
            "The loss function for the iteration 15930----->42.63234711352467 :)\n",
            "The loss function for the iteration 15940----->42.63222969889552 :)\n",
            "The loss function for the iteration 15950----->42.63211238092892 :)\n",
            "The loss function for the iteration 15960----->42.63199506422594 :)\n",
            "The loss function for the iteration 15970----->42.63187779182962 :)\n",
            "The loss function for the iteration 15980----->42.631760594247716 :)\n",
            "The loss function for the iteration 15990----->42.631643431094645 :)\n",
            "The loss function for the iteration 16000----->42.63152630781571 :)\n",
            "The loss function for the iteration 16010----->42.63140922858183 :)\n",
            "The loss function for the iteration 16020----->42.63129218914612 :)\n",
            "The loss function for the iteration 16030----->42.63117519497402 :)\n",
            "The loss function for the iteration 16040----->42.631058274405234 :)\n",
            "The loss function for the iteration 16050----->42.63094137657728 :)\n",
            "The loss function for the iteration 16060----->42.63082452258051 :)\n",
            "The loss function for the iteration 16070----->42.63070774657753 :)\n",
            "The loss function for the iteration 16080----->42.63059096814022 :)\n",
            "The loss function for the iteration 16090----->42.63047426670383 :)\n",
            "The loss function for the iteration 16100----->42.63035758577982 :)\n",
            "The loss function for the iteration 16110----->42.630240957832726 :)\n",
            "The loss function for the iteration 16120----->42.63012441105573 :)\n",
            "The loss function for the iteration 16130----->42.63000786119563 :)\n",
            "The loss function for the iteration 16140----->42.62989136205701 :)\n",
            "The loss function for the iteration 16150----->42.62977493821341 :)\n",
            "The loss function for the iteration 16160----->42.62965854832975 :)\n",
            "The loss function for the iteration 16170----->42.62954219189036 :)\n",
            "The loss function for the iteration 16180----->42.629425898107534 :)\n",
            "The loss function for the iteration 16190----->42.62930964057789 :)\n",
            "The loss function for the iteration 16200----->42.62919342184678 :)\n",
            "The loss function for the iteration 16210----->42.629077245810954 :)\n",
            "The loss function for the iteration 16220----->42.628961109944434 :)\n",
            "The loss function for the iteration 16230----->42.62884505314082 :)\n",
            "The loss function for the iteration 16240----->42.62872898791097 :)\n",
            "The loss function for the iteration 16250----->42.6286129990385 :)\n",
            "The loss function for the iteration 16260----->42.62849706750621 :)\n",
            "The loss function for the iteration 16270----->42.62838115043695 :)\n",
            "The loss function for the iteration 16280----->42.62826531045537 :)\n",
            "The loss function for the iteration 16290----->42.62814948751965 :)\n",
            "The loss function for the iteration 16300----->42.628033734811574 :)\n",
            "The loss function for the iteration 16310----->42.6279180224211 :)\n",
            "The loss function for the iteration 16320----->42.62780232174537 :)\n",
            "The loss function for the iteration 16330----->42.62768667319322 :)\n",
            "The loss function for the iteration 16340----->42.627571097705285 :)\n",
            "The loss function for the iteration 16350----->42.62745553973212 :)\n",
            "The loss function for the iteration 16360----->42.627340050182035 :)\n",
            "The loss function for the iteration 16370----->42.627224616584535 :)\n",
            "The loss function for the iteration 16380----->42.62710916494271 :)\n",
            "The loss function for the iteration 16390----->42.62699382145214 :)\n",
            "The loss function for the iteration 16400----->42.62687849510863 :)\n",
            "The loss function for the iteration 16410----->42.62676320279597 :)\n",
            "The loss function for the iteration 16420----->42.62664799972203 :)\n",
            "The loss function for the iteration 16430----->42.62653280647028 :)\n",
            "The loss function for the iteration 16440----->42.62641766749865 :)\n",
            "The loss function for the iteration 16450----->42.62630255855526 :)\n",
            "The loss function for the iteration 16460----->42.62618749939892 :)\n",
            "The loss function for the iteration 16470----->42.62607247079646 :)\n",
            "The loss function for the iteration 16480----->42.62595750399681 :)\n",
            "The loss function for the iteration 16490----->42.625842558295226 :)\n",
            "The loss function for the iteration 16500----->42.62572769061144 :)\n",
            "The loss function for the iteration 16510----->42.6256128499247 :)\n",
            "The loss function for the iteration 16520----->42.62549803903586 :)\n",
            "The loss function for the iteration 16530----->42.62538331067399 :)\n",
            "The loss function for the iteration 16540----->42.62526860437534 :)\n",
            "The loss function for the iteration 16550----->42.625153954811914 :)\n",
            "The loss function for the iteration 16560----->42.625039304227734 :)\n",
            "The loss function for the iteration 16570----->42.62492472973513 :)\n",
            "The loss function for the iteration 16580----->42.62481020645285 :)\n",
            "The loss function for the iteration 16590----->42.6246957084455 :)\n",
            "The loss function for the iteration 16600----->42.62458127992423 :)\n",
            "The loss function for the iteration 16610----->42.62446689779959 :)\n",
            "The loss function for the iteration 16620----->42.62435253517495 :)\n",
            "The loss function for the iteration 16630----->42.62423821309672 :)\n",
            "The loss function for the iteration 16640----->42.62412394949149 :)\n",
            "The loss function for the iteration 16650----->42.62400971115854 :)\n",
            "The loss function for the iteration 16660----->42.623895538815134 :)\n",
            "The loss function for the iteration 16670----->42.623781397716 :)\n",
            "The loss function for the iteration 16680----->42.62366730579334 :)\n",
            "The loss function for the iteration 16690----->42.623553268403114 :)\n",
            "The loss function for the iteration 16700----->42.62343925676245 :)\n",
            "The loss function for the iteration 16710----->42.623325288015934 :)\n",
            "The loss function for the iteration 16720----->42.62321138240937 :)\n",
            "The loss function for the iteration 16730----->42.62309750481952 :)\n",
            "The loss function for the iteration 16740----->42.62298369128466 :)\n",
            "The loss function for the iteration 16750----->42.622869883306755 :)\n",
            "The loss function for the iteration 16760----->42.62275613611755 :)\n",
            "The loss function for the iteration 16770----->42.62264242098603 :)\n",
            "The loss function for the iteration 16780----->42.62252876344442 :)\n",
            "The loss function for the iteration 16790----->42.62241516350946 :)\n",
            "The loss function for the iteration 16800----->42.62230155646149 :)\n",
            "The loss function for the iteration 16810----->42.622188028610246 :)\n",
            "The loss function for the iteration 16820----->42.622074521686045 :)\n",
            "The loss function for the iteration 16830----->42.62196105810745 :)\n",
            "The loss function for the iteration 16840----->42.62184765006567 :)\n",
            "The loss function for the iteration 16850----->42.62173431571389 :)\n",
            "The loss function for the iteration 16860----->42.62162097809658 :)\n",
            "The loss function for the iteration 16870----->42.6215077015306 :)\n",
            "The loss function for the iteration 16880----->42.621394472762084 :)\n",
            "The loss function for the iteration 16890----->42.62128127842373 :)\n",
            "The loss function for the iteration 16900----->42.62116814276501 :)\n",
            "The loss function for the iteration 16910----->42.62105502844568 :)\n",
            "The loss function for the iteration 16920----->42.620941974080395 :)\n",
            "The loss function for the iteration 16930----->42.62082896860318 :)\n",
            "The loss function for the iteration 16940----->42.6207159689364 :)\n",
            "The loss function for the iteration 16950----->42.62060303158549 :)\n",
            "The loss function for the iteration 16960----->42.620490146228505 :)\n",
            "The loss function for the iteration 16970----->42.620377315528394 :)\n",
            "The loss function for the iteration 16980----->42.62026451935123 :)\n",
            "The loss function for the iteration 16990----->42.620151755083945 :)\n",
            "The loss function for the iteration 17000----->42.620039047633156 :)\n",
            "The loss function for the iteration 17010----->42.61992635968087 :)\n",
            "The loss function for the iteration 17020----->42.619813723834135 :)\n",
            "The loss function for the iteration 17030----->42.61970114114439 :)\n",
            "The loss function for the iteration 17040----->42.61958862148621 :)\n",
            "The loss function for the iteration 17050----->42.619476099252914 :)\n",
            "The loss function for the iteration 17060----->42.61936365873144 :)\n",
            "The loss function for the iteration 17070----->42.61925122795828 :)\n",
            "The loss function for the iteration 17080----->42.61913884758671 :)\n",
            "The loss function for the iteration 17090----->42.61902652458382 :)\n",
            "The loss function for the iteration 17100----->42.61891422159295 :)\n",
            "The loss function for the iteration 17110----->42.618801950862725 :)\n",
            "The loss function for the iteration 17120----->42.61868974699072 :)\n",
            "The loss function for the iteration 17130----->42.61857757074368 :)\n",
            "The loss function for the iteration 17140----->42.61846545304168 :)\n",
            "The loss function for the iteration 17150----->42.618353374701755 :)\n",
            "The loss function for the iteration 17160----->42.618241359681384 :)\n",
            "The loss function for the iteration 17170----->42.61812935340708 :)\n",
            "The loss function for the iteration 17180----->42.61801740700344 :)\n",
            "The loss function for the iteration 17190----->42.61790547229922 :)\n",
            "The loss function for the iteration 17200----->42.617793612539934 :)\n",
            "The loss function for the iteration 17210----->42.617681792281985 :)\n",
            "The loss function for the iteration 17220----->42.617570005782575 :)\n",
            "The loss function for the iteration 17230----->42.61745826773305 :)\n",
            "The loss function for the iteration 17240----->42.61734657506289 :)\n",
            "The loss function for the iteration 17250----->42.61723489640428 :)\n",
            "The loss function for the iteration 17260----->42.617123255805126 :)\n",
            "The loss function for the iteration 17270----->42.61701167652958 :)\n",
            "The loss function for the iteration 17280----->42.61690016079699 :)\n",
            "The loss function for the iteration 17290----->42.61678866317026 :)\n",
            "The loss function for the iteration 17300----->42.61667721229414 :)\n",
            "The loss function for the iteration 17310----->42.61656580265636 :)\n",
            "The loss function for the iteration 17320----->42.61645444647817 :)\n",
            "The loss function for the iteration 17330----->42.616343088525085 :)\n",
            "The loss function for the iteration 17340----->42.616231834660965 :)\n",
            "The loss function for the iteration 17350----->42.61612059300684 :)\n",
            "The loss function for the iteration 17360----->42.61600938195021 :)\n",
            "The loss function for the iteration 17370----->42.61589822725166 :)\n",
            "The loss function for the iteration 17380----->42.615787102461084 :)\n",
            "The loss function for the iteration 17390----->42.61567603119636 :)\n",
            "The loss function for the iteration 17400----->42.61556502419839 :)\n",
            "The loss function for the iteration 17410----->42.61545402158033 :)\n",
            "The loss function for the iteration 17420----->42.615343084095976 :)\n",
            "The loss function for the iteration 17430----->42.61523217145585 :)\n",
            "The loss function for the iteration 17440----->42.615121298927974 :)\n",
            "The loss function for the iteration 17450----->42.61501047065088 :)\n",
            "The loss function for the iteration 17460----->42.61489971279883 :)\n",
            "The loss function for the iteration 17470----->42.614788947649764 :)\n",
            "The loss function for the iteration 17480----->42.61467826556631 :)\n",
            "The loss function for the iteration 17490----->42.614567586154784 :)\n",
            "The loss function for the iteration 17500----->42.614456973632606 :)\n",
            "The loss function for the iteration 17510----->42.614346413741394 :)\n",
            "The loss function for the iteration 17520----->42.61423586742796 :)\n",
            "The loss function for the iteration 17530----->42.61412539179565 :)\n",
            "The loss function for the iteration 17540----->42.614014938457096 :)\n",
            "The loss function for the iteration 17550----->42.613904516831184 :)\n",
            "The loss function for the iteration 17560----->42.613794163290265 :)\n",
            "The loss function for the iteration 17570----->42.61368381168191 :)\n",
            "The loss function for the iteration 17580----->42.613573517370206 :)\n",
            "The loss function for the iteration 17590----->42.61346330385854 :)\n",
            "The loss function for the iteration 17600----->42.613353094160715 :)\n",
            "The loss function for the iteration 17610----->42.61324291975444 :)\n",
            "The loss function for the iteration 17620----->42.61313281884041 :)\n",
            "The loss function for the iteration 17630----->42.61302272224473 :)\n",
            "The loss function for the iteration 17640----->42.61291267366325 :)\n",
            "The loss function for the iteration 17650----->42.61280269474145 :)\n",
            "The loss function for the iteration 17660----->42.61269273351827 :)\n",
            "The loss function for the iteration 17670----->42.61258284533887 :)\n",
            "The loss function for the iteration 17680----->42.61247293909092 :)\n",
            "The loss function for the iteration 17690----->42.612363114366545 :)\n",
            "The loss function for the iteration 17700----->42.61225332631324 :)\n",
            "The loss function for the iteration 17710----->42.6121435634244 :)\n",
            "The loss function for the iteration 17720----->42.61203384445632 :)\n",
            "The loss function for the iteration 17730----->42.611924175976526 :)\n",
            "The loss function for the iteration 17740----->42.61181454072632 :)\n",
            "The loss function for the iteration 17750----->42.61170496215813 :)\n",
            "The loss function for the iteration 17760----->42.611595410858776 :)\n",
            "The loss function for the iteration 17770----->42.611485911897724 :)\n",
            "The loss function for the iteration 17780----->42.611376463873576 :)\n",
            "The loss function for the iteration 17790----->42.61126701211715 :)\n",
            "The loss function for the iteration 17800----->42.61115760749803 :)\n",
            "The loss function for the iteration 17810----->42.61104827769387 :)\n",
            "The loss function for the iteration 17820----->42.61093895973552 :)\n",
            "The loss function for the iteration 17830----->42.61082969349801 :)\n",
            "The loss function for the iteration 17840----->42.610720482357706 :)\n",
            "The loss function for the iteration 17850----->42.61061129587541 :)\n",
            "The loss function for the iteration 17860----->42.61050216831045 :)\n",
            "The loss function for the iteration 17870----->42.61039304873001 :)\n",
            "The loss function for the iteration 17880----->42.610284013228046 :)\n",
            "The loss function for the iteration 17890----->42.61017497446508 :)\n",
            "The loss function for the iteration 17900----->42.61006599431684 :)\n",
            "The loss function for the iteration 17910----->42.60995705232874 :)\n",
            "The loss function for the iteration 17920----->42.609848143173046 :)\n",
            "The loss function for the iteration 17930----->42.609739279574605 :)\n",
            "The loss function for the iteration 17940----->42.60963045791471 :)\n",
            "The loss function for the iteration 17950----->42.609521667992745 :)\n",
            "The loss function for the iteration 17960----->42.609412891191425 :)\n",
            "The loss function for the iteration 17970----->42.6093042042901 :)\n",
            "The loss function for the iteration 17980----->42.60919553191555 :)\n",
            "The loss function for the iteration 17990----->42.60908692348007 :)\n",
            "The loss function for the iteration 18000----->42.60897833992771 :)\n",
            "The loss function for the iteration 18010----->42.60886980814744 :)\n",
            "The loss function for the iteration 18020----->42.60876131092818 :)\n",
            "The loss function for the iteration 18030----->42.60865282635471 :)\n",
            "The loss function for the iteration 18040----->42.60854441428021 :)\n",
            "The loss function for the iteration 18050----->42.60843603353984 :)\n",
            "The loss function for the iteration 18060----->42.60832768083217 :)\n",
            "The loss function for the iteration 18070----->42.608219375451164 :)\n",
            "The loss function for the iteration 18080----->42.6081111080011 :)\n",
            "The loss function for the iteration 18090----->42.60800288338896 :)\n",
            "The loss function for the iteration 18100----->42.60789471055893 :)\n",
            "The loss function for the iteration 18110----->42.60778655058835 :)\n",
            "The loss function for the iteration 18120----->42.60767845946561 :)\n",
            "The loss function for the iteration 18130----->42.60757041181652 :)\n",
            "The loss function for the iteration 18140----->42.607462372278896 :)\n",
            "The loss function for the iteration 18150----->42.60735437874634 :)\n",
            "The loss function for the iteration 18160----->42.60724644796687 :)\n",
            "The loss function for the iteration 18170----->42.607138541490556 :)\n",
            "The loss function for the iteration 18180----->42.60703067356938 :)\n",
            "The loss function for the iteration 18190----->42.60692284552007 :)\n",
            "The loss function for the iteration 18200----->42.606815063465895 :)\n",
            "The loss function for the iteration 18210----->42.60670732757766 :)\n",
            "The loss function for the iteration 18220----->42.606599595025116 :)\n",
            "The loss function for the iteration 18230----->42.606491957535574 :)\n",
            "The loss function for the iteration 18240----->42.60638433748536 :)\n",
            "The loss function for the iteration 18250----->42.60627672784322 :)\n",
            "The loss function for the iteration 18260----->42.606169190221955 :)\n",
            "The loss function for the iteration 18270----->42.60606167467287 :)\n",
            "The loss function for the iteration 18280----->42.605954203522565 :)\n",
            "The loss function for the iteration 18290----->42.60584677730398 :)\n",
            "The loss function for the iteration 18300----->42.60573940118631 :)\n",
            "The loss function for the iteration 18310----->42.60563206069272 :)\n",
            "The loss function for the iteration 18320----->42.605524759653015 :)\n",
            "The loss function for the iteration 18330----->42.60541748693946 :)\n",
            "The loss function for the iteration 18340----->42.60531025542089 :)\n",
            "The loss function for the iteration 18350----->42.60520307313023 :)\n",
            "The loss function for the iteration 18360----->42.60509591530432 :)\n",
            "The loss function for the iteration 18370----->42.60498883294909 :)\n",
            "The loss function for the iteration 18380----->42.60488174047132 :)\n",
            "The loss function for the iteration 18390----->42.60477470778604 :)\n",
            "The loss function for the iteration 18400----->42.60466771730011 :)\n",
            "The loss function for the iteration 18410----->42.60456072663038 :)\n",
            "The loss function for the iteration 18420----->42.604453819230216 :)\n",
            "The loss function for the iteration 18430----->42.604346937413865 :)\n",
            "The loss function for the iteration 18440----->42.60424011174616 :)\n",
            "The loss function for the iteration 18450----->42.60413331260907 :)\n",
            "The loss function for the iteration 18460----->42.60402655733973 :)\n",
            "The loss function for the iteration 18470----->42.60391984244183 :)\n",
            "The loss function for the iteration 18480----->42.60381317091886 :)\n",
            "The loss function for the iteration 18490----->42.60370654289216 :)\n",
            "The loss function for the iteration 18500----->42.603599930299275 :)\n",
            "The loss function for the iteration 18510----->42.60349338566378 :)\n",
            "The loss function for the iteration 18520----->42.603386849112354 :)\n",
            "The loss function for the iteration 18530----->42.603280359868045 :)\n",
            "The loss function for the iteration 18540----->42.60317392298562 :)\n",
            "The loss function for the iteration 18550----->42.603067517846895 :)\n",
            "The loss function for the iteration 18560----->42.60296118131639 :)\n",
            "The loss function for the iteration 18570----->42.60285483650352 :)\n",
            "The loss function for the iteration 18580----->42.6027485380306 :)\n",
            "The loss function for the iteration 18590----->42.60264229819354 :)\n",
            "The loss function for the iteration 18600----->42.60253608846203 :)\n",
            "The loss function for the iteration 18610----->42.602429911539936 :)\n",
            "The loss function for the iteration 18620----->42.602323801455505 :)\n",
            "The loss function for the iteration 18630----->42.60221773567864 :)\n",
            "The loss function for the iteration 18640----->42.60211164622441 :)\n",
            "The loss function for the iteration 18650----->42.60200562687017 :)\n",
            "The loss function for the iteration 18660----->42.60189965066292 :)\n",
            "The loss function for the iteration 18670----->42.601793705134924 :)\n",
            "The loss function for the iteration 18680----->42.60168780332329 :)\n",
            "The loss function for the iteration 18690----->42.60158192382473 :)\n",
            "The loss function for the iteration 18700----->42.60147611096163 :)\n",
            "The loss function for the iteration 18710----->42.60137030701923 :)\n",
            "The loss function for the iteration 18720----->42.60126453486672 :)\n",
            "The loss function for the iteration 18730----->42.60115883646761 :)\n",
            "The loss function for the iteration 18740----->42.60105316781661 :)\n",
            "The loss function for the iteration 18750----->42.60094752035072 :)\n",
            "The loss function for the iteration 18760----->42.600841928727405 :)\n",
            "The loss function for the iteration 18770----->42.60073636407442 :)\n",
            "The loss function for the iteration 18780----->42.600630832972556 :)\n",
            "The loss function for the iteration 18790----->42.60052535769156 :)\n",
            "The loss function for the iteration 18800----->42.600419907951796 :)\n",
            "The loss function for the iteration 18810----->42.600314504568864 :)\n",
            "The loss function for the iteration 18820----->42.6002091379458 :)\n",
            "The loss function for the iteration 18830----->42.60010379818996 :)\n",
            "The loss function for the iteration 18840----->42.5999985074496 :)\n",
            "The loss function for the iteration 18850----->42.59989324447537 :)\n",
            "The loss function for the iteration 18860----->42.59978802691919 :)\n",
            "The loss function for the iteration 18870----->42.59968285530496 :)\n",
            "The loss function for the iteration 18880----->42.59957770679592 :)\n",
            "The loss function for the iteration 18890----->42.59947259945315 :)\n",
            "The loss function for the iteration 18900----->42.59936756366707 :)\n",
            "The loss function for the iteration 18910----->42.599262531986916 :)\n",
            "The loss function for the iteration 18920----->42.59915754364889 :)\n",
            "The loss function for the iteration 18930----->42.59905262669375 :)\n",
            "The loss function for the iteration 18940----->42.59894769496357 :)\n",
            "The loss function for the iteration 18950----->42.598842832941926 :)\n",
            "The loss function for the iteration 18960----->42.598737991529 :)\n",
            "The loss function for the iteration 18970----->42.598633178941725 :)\n",
            "The loss function for the iteration 18980----->42.59852845341678 :)\n",
            "The loss function for the iteration 18990----->42.59842371381857 :)\n",
            "The loss function for the iteration 19000----->42.59831904895914 :)\n",
            "The loss function for the iteration 19010----->42.5982144195691 :)\n",
            "The loss function for the iteration 19020----->42.598109801087844 :)\n",
            "The loss function for the iteration 19030----->42.59800524388799 :)\n",
            "The loss function for the iteration 19040----->42.59790069962119 :)\n",
            "The loss function for the iteration 19050----->42.59779620194367 :)\n",
            "The loss function for the iteration 19060----->42.59769176985071 :)\n",
            "The loss function for the iteration 19070----->42.59758735027771 :)\n",
            "The loss function for the iteration 19080----->42.59748297186054 :)\n",
            "The loss function for the iteration 19090----->42.59737865507487 :)\n",
            "The loss function for the iteration 19100----->42.5972743432017 :)\n",
            "The loss function for the iteration 19110----->42.597170060881524 :)\n",
            "The loss function for the iteration 19120----->42.59706583638715 :)\n",
            "The loss function for the iteration 19130----->42.59696164251399 :)\n",
            "The loss function for the iteration 19140----->42.596857507720244 :)\n",
            "The loss function for the iteration 19150----->42.5967533845311 :)\n",
            "The loss function for the iteration 19160----->42.59664932298134 :)\n",
            "The loss function for the iteration 19170----->42.59654530481229 :)\n",
            "The loss function for the iteration 19180----->42.59644128105991 :)\n",
            "The loss function for the iteration 19190----->42.59633730980057 :)\n",
            "The loss function for the iteration 19200----->42.59623339057774 :)\n",
            "The loss function for the iteration 19210----->42.59612951081271 :)\n",
            "The loss function for the iteration 19220----->42.59602563733673 :)\n",
            "The loss function for the iteration 19230----->42.59592182032882 :)\n",
            "The loss function for the iteration 19240----->42.59581805482437 :)\n",
            "The loss function for the iteration 19250----->42.595714298272355 :)\n",
            "The loss function for the iteration 19260----->42.59561059089427 :)\n",
            "The loss function for the iteration 19270----->42.595506923858686 :)\n",
            "The loss function for the iteration 19280----->42.59540330245465 :)\n",
            "The loss function for the iteration 19290----->42.59529968534338 :)\n",
            "The loss function for the iteration 19300----->42.595196121473734 :)\n",
            "The loss function for the iteration 19310----->42.595092615766 :)\n",
            "The loss function for the iteration 19320----->42.594989131752854 :)\n",
            "The loss function for the iteration 19330----->42.59488569361264 :)\n",
            "The loss function for the iteration 19340----->42.5947822893785 :)\n",
            "The loss function for the iteration 19350----->42.59467892658785 :)\n",
            "The loss function for the iteration 19360----->42.59457558039988 :)\n",
            "The loss function for the iteration 19370----->42.594472283972166 :)\n",
            "The loss function for the iteration 19380----->42.59436903483777 :)\n",
            "The loss function for the iteration 19390----->42.59426580162999 :)\n",
            "The loss function for the iteration 19400----->42.59416261910877 :)\n",
            "The loss function for the iteration 19410----->42.59405946180977 :)\n",
            "The loss function for the iteration 19420----->42.593956338770425 :)\n",
            "The loss function for the iteration 19430----->42.5938532557547 :)\n",
            "The loss function for the iteration 19440----->42.59375023076488 :)\n",
            "The loss function for the iteration 19450----->42.59364724535249 :)\n",
            "The loss function for the iteration 19460----->42.59354428144332 :)\n",
            "The loss function for the iteration 19470----->42.593441360008356 :)\n",
            "The loss function for the iteration 19480----->42.593338473399285 :)\n",
            "The loss function for the iteration 19490----->42.59323562028873 :)\n",
            "The loss function for the iteration 19500----->42.59313279309319 :)\n",
            "The loss function for the iteration 19510----->42.59303002264554 :)\n",
            "The loss function for the iteration 19520----->42.592927304510276 :)\n",
            "The loss function for the iteration 19530----->42.5928245763086 :)\n",
            "The loss function for the iteration 19540----->42.592721913167814 :)\n",
            "The loss function for the iteration 19550----->42.59261927495253 :)\n",
            "The loss function for the iteration 19560----->42.592516677022225 :)\n",
            "The loss function for the iteration 19570----->42.59241412180736 :)\n",
            "The loss function for the iteration 19580----->42.59231159235343 :)\n",
            "The loss function for the iteration 19590----->42.5922091286801 :)\n",
            "The loss function for the iteration 19600----->42.59210669553584 :)\n",
            "The loss function for the iteration 19610----->42.59200427816773 :)\n",
            "The loss function for the iteration 19620----->42.59190191920179 :)\n",
            "The loss function for the iteration 19630----->42.591799602540426 :)\n",
            "The loss function for the iteration 19640----->42.59169728712223 :)\n",
            "The loss function for the iteration 19650----->42.59159502328622 :)\n",
            "The loss function for the iteration 19660----->42.591492794093426 :)\n",
            "The loss function for the iteration 19670----->42.59139058331338 :)\n",
            "The loss function for the iteration 19680----->42.59128845097873 :)\n",
            "The loss function for the iteration 19690----->42.591186309211885 :)\n",
            "The loss function for the iteration 19700----->42.59108423814812 :)\n",
            "The loss function for the iteration 19710----->42.590982212602256 :)\n",
            "The loss function for the iteration 19720----->42.59088016798572 :)\n",
            "The loss function for the iteration 19730----->42.59077820298274 :)\n",
            "The loss function for the iteration 19740----->42.59067626357641 :)\n",
            "The loss function for the iteration 19750----->42.590574349678526 :)\n",
            "The loss function for the iteration 19760----->42.59047249593569 :)\n",
            "The loss function for the iteration 19770----->42.59037065847218 :)\n",
            "The loss function for the iteration 19780----->42.5902688601614 :)\n",
            "The loss function for the iteration 19790----->42.5901671289227 :)\n",
            "The loss function for the iteration 19800----->42.590065398553996 :)\n",
            "The loss function for the iteration 19810----->42.58996369852983 :)\n",
            "The loss function for the iteration 19820----->42.589862079120884 :)\n",
            "The loss function for the iteration 19830----->42.58976044160363 :)\n",
            "The loss function for the iteration 19840----->42.589658866681134 :)\n",
            "The loss function for the iteration 19850----->42.589557342163026 :)\n",
            "The loss function for the iteration 19860----->42.58945582737001 :)\n",
            "The loss function for the iteration 19870----->42.58935438173934 :)\n",
            "The loss function for the iteration 19880----->42.5892529330963 :)\n",
            "The loss function for the iteration 19890----->42.589151524103606 :)\n",
            "The loss function for the iteration 19900----->42.589050177288065 :)\n",
            "The loss function for the iteration 19910----->42.58894884867368 :)\n",
            "The loss function for the iteration 19920----->42.58884756368345 :)\n",
            "The loss function for the iteration 19930----->42.58874633442237 :)\n",
            "The loss function for the iteration 19940----->42.58864513058263 :)\n",
            "The loss function for the iteration 19950----->42.58854393562558 :)\n",
            "The loss function for the iteration 19960----->42.588442791083395 :)\n",
            "The loss function for the iteration 19970----->42.58834168090116 :)\n",
            "The loss function for the iteration 19980----->42.588240618125376 :)\n",
            "The loss function for the iteration 19990----->42.58813957750543 :)\n",
            "The loss function for the iteration 20000----->42.58803856384221 :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict"
      ],
      "metadata": {
        "id": "QUp_8WBfe1n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_lasso_custom_kaggle = lasso_regression_custom_kaggle.predict(X_test_kaggle)\n",
        "y_pred_ridge_custom_kaggle = ridge_regression_custom_kaggle.predict(X_test_kaggle)"
      ],
      "metadata": {
        "id": "jvy6gTNUe1Ob"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "xMp91vQMe2Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse_lasso_custom_kaggle = mean_squared_error(y_test_kaggle, y_pred_lasso_custom_kaggle)\n",
        "r2_lasso_custom_kaggle = r2_score(y_test_kaggle, y_pred_lasso_custom_kaggle)\n",
        "print(\"Custom Lasso:\" + \"\\n\" + \"MSE: \" + str(mse_lasso_custom_kaggle) + \"\\n\" + \"R2: \" + str(r2_lasso_custom_kaggle) + \"\\n\")\n",
        "\n",
        "mse_ridge_custom_kaggle = mean_squared_error(y_test_kaggle, y_pred_ridge_custom_kaggle)\n",
        "r2_ridge_custom_kaggle = r2_score(y_test_kaggle, y_pred_ridge_custom_kaggle)\n",
        "print(\"Custom Ridge:\" + \"\\n\" + \"MSE: \" + str(mse_ridge_custom_kaggle) + \"\\n\" + \"R2: \" + str(r2_ridge_custom_kaggle) + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkRvhMi5e0c8",
        "outputId": "6deb9409-f6a1-4413-9bb9-b2605940c090"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Lasso:\n",
            "MSE: 4771.258126635796\n",
            "R2: 0.0\n",
            "\n",
            "Custom Ridge:\n",
            "MSE: 4769.559535152782\n",
            "R2: 0.0\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}